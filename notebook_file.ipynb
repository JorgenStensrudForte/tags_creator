from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
 1/2: pip install pypdf
 1/3: pip install pypdf
 1/4:
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
pages = loader.load_and_split()
 1/5: pages[0]
 1/6:
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
 1/7:
forte_web_pages[0]
forte_some_pages[0]
 1/8:
from langchain.llms import OpenAI

llm = OpenAI(openai_api_key="2cb0b5e2e4e3480f87a53e350e40b7cd")
 1/9:
from langchain.llms import OpenAI
from langchain.chains.router import MultiRetrievalQAChain

llm = OpenAI(openai_api_key="2cb0b5e2e4e3480f87a53e350e40b7cd")
1/10:
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.vectorstores import FAISS

from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/11:
from langchain.llms import OpenAI
from langchain.chains.router import MultiRetrievalQAChain

llm = OpenAI(openai_api_key="2cb0b5e2e4e3480f87a53e350e40b7cd")
1/12:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/13:
import os
import getpass

os.environ['OPEN_API_KEY'] = getpass.getpass('OpenAI API Key:')
1/14:
import os
import getpass

os.environ['OPEN_API_KEY'] = getpass.getpass('OpenAI API Key:')
1/15:
import os
import getpass

os.environ['OPEN_API_KEY'] = getpass.getpass('OpenAI API Key:')
1/16:
import os
import getpass

os.environ['OPEN_API_KEY'] = getpass.getpass('OpenAI API Key:')
print(os.environ['OPEN_API_KEY'])
1/17:
import os
import getpass

os.environ['OPEN_API_KEY'] = getpass.getpass('OpenAI API Key:')
1/18:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/19:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/20:
import os
import getpass

os.environ['OPEN_API_KEY'] = getpass.getpass('OpenAI API Key:')
1/21:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/22:
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
1/23:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/24:
import os
import getpass

openai_api_key="2cb0b5e2e4e3480f87a53e350e40b7cd"
1/25:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/26:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/27:
import os
import getpass

os.environ['OPENAI_API_KEY'] ="2cb0b5e2e4e3480f87a53e350e40b7cd"
1/28:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/29:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
import openai
1/30:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "2cb0b5e2e4e3480f87a53e350e40b7cd"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(deployment="embedding-ada")
1/31:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "2cb0b5e2e4e3480f87a53e350e40b7cd"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(deployment="embedding-ada")
1/32:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, OpenAIEmbeddings()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, OpenAIEmbeddings()).as_retriever()
1/33:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings.embed_documents()).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings.embed_documents()).as_retriever()
1/34:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/35:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "2cb0b5e2e4e3480f87a53e350e40b7cd"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(deployment="embedding-ada",
                model="text-embedding-ada-002")
1/36:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "2cb0b5e2e4e3480f87a53e350e40b7cd"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(deployment="embedding-ada",
                model="text-embedding-ada-002")
1/37:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/38:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "2cb0b5e2e4e3480f87a53e350e40b7cd"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(deployment="embedding-ada",
                model="text-embedding-ada-002")
1/39:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "2cb0b5e2e4e3480f87a53e350e40b7cd"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(deployment="embedding-ada",
                model="text-embedding-ada-002")
1/40:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "2cb0b5e2e4e3480f87a53e350e40b7cd"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(deployment="embedding-ada",
                model="text-embedding-ada-002")
1/41:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "2cb0b5e2e4e3480f87a53e350e40b7cd"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(deployment="embedding-ada",
                model="text-embedding-ada-002")
1/42:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/43:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model="text-embedding-ada-002")

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/44:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/45:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model_kwargs="text-embedding-ada-002")

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/46:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model_kwargs="text-embedding-ada-002")

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/47:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model="text-embedding-ada-002")

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/48:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model="text-embedding-ada-002",openai_api_base="https://cog-ycyy4wyxwg7ck.openai.azure.com",
    openai_api_type="azure",)

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/49:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model="text-embedding-ada-002",openai_api_base="https://cog-ycyy4wyxwg7ck.openai.azure.com",
    openai_api_type="azure")

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/50:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model="text-embedding-ada-002")
text = "This is a test document."
query_result = embeddings.embed_query(text)
"""
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/51:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model="text-embedding-ada-002")
text = "This is a test document."
query_result = embeddings.embed_query(text)
"""
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/52:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",model="text-embedding-ada-002")
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/53:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/54:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "9d3c1988c8c24db79d391ba83d5f761e"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

from langchain.embeddings import OpenAIEmbeddings
1/55:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "9d3c1988c8c24db79d391ba83d5f761e"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

from langchain.embeddings import OpenAIEmbeddings
1/56:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/57:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/58: pip install pypdf
1/59:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
import openai
1/60:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "9d3c1988c8c24db79d391ba83d5f761e"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

from langchain.embeddings import OpenAIEmbeddings
1/61:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/62:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment="embedding-ada", client="azure", chunk_size=1) 
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/63:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
1/64:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
1/65:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "9d3c1988c8c24db79d391ba83d5f761e"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

from langchain.embeddings import OpenAIEmbeddings
1/66:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment="embedding-ada", client="azure", chunk_size=1) 
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/67:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_name="embedding-ada", client="azure", chunk_size=1) 
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/68:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", client="azure", chunk_size=1) 
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/69:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", client="azure", chunk_size=1) 
text = "This is a test document."
query_result = embeddings.embed_query(text)

tekst= """
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
"""
1/70:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", client="azure", chunk_size=1) 

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/71:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", client="azure", chunk_size=1) 

loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/72:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/73:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }
]
1/74: chain = MultiRetrievalQAChain.from_retrievers(AzureOpenAI(), retriever_infos, verbose=True)
1/75: chain = MultiRetrievalQAChain.from_retrievers(AzureOpenAI(), retriever_infos, verbose=True)
1/76:
from langchain.chains.router import MultiRetrievalQAChain
# from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
1/77: chain = MultiRetrievalQAChain.from_retrievers(AzureOpenAI(), retriever_infos, verbose=True)
1/78: chain = MultiRetrievalQAChain.from_retrievers(AzureOpenAI(), retriever_infos, verbose=True)
1/79:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
1/80:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }
]
1/81: chain = MultiRetrievalQAChain.from_retrievers(OpenAI(), retriever_infos, verbose=True)
1/82: print(chain.run("Write a linkedIn post about Forte Digital"))
1/83: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
1/84:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
1/85:
# openai.api_type = "azure"
# openai.api_version="2023-03-15-preview"
# openai.api_key = "9d3c1988c8c24db79d391ba83d5f761e"
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = "9d3c1988c8c24db79d391ba83d5f761e"
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"

from langchain.embeddings import OpenAIEmbeddings
1/86:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", client="azure", chunk_size=1)
1/87:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", client="azure", chunk_size=1)
1/88: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
1/89: llm = AzureOpenAI(deployment_name="chat", model_id="gpt-35-turbo")
1/90: llm = AzureOpenAI(deployment_id="chat", model="gpt-35-turbo")
1/91: llm = AzureOpenAI(deployment_id="chat", model="gpt-35-turbo")
1/92: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
1/93: print(chain.run("Write a linkedIn post about Forte Digital"))
1/94:
# print(chain.run("Write a linkedIn post about Forte Digital"))
print(llm("hva er hovedstaden i Oslo?"))
1/95:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }]
1/96: llm = AzureOpenAI(deployment_id="chat", model_name="gpt-35-turbo")
1/97:
# print(chain.run("Write a linkedIn post about Forte Digital"))
print(llm("hva er hovedstaden i Oslo?"))
1/98:
# print(chain.run("Write a linkedIn post about Forte Digital"))
print(llm('hva er hovedstaden i Oslo?'))
1/99: llm = AzureOpenAI(deployment_id="chat", model_name="gpt-35-turbo")
1/100: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
1/101: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
1/102:
# print(chain.run("Write a linkedIn post about Forte Digital"))
print(llm('hva er hovedstaden i Oslo?'))
1/103: print(chain.run("Write a linkedIn post about Forte Digital"))
1/104:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_name="embedding-ada", client="azure", chunk_size=1)
1/105:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_name="embedding-ada", client="azure", chunk_size=1)
1/106:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/107:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", client="azure", chunk_size=1)
1/108: print(embeddings)
1/109:
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1)
1/110: print(embeddings)
1/111:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1)
1/112: print(embeddings)
1/113:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/114:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/115:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
1/116: print(embeddings)
1/117:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/118:
openai.api_type = "azure"
openai.api_version="2023-03-15-preview"
openai.api_key = "9d3c1988c8c24db79d391ba83d5f761e"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = "9d3c1988c8c24db79d391ba83d5f761e"
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
1/119:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
1/120: print(embeddings)
1/121:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/122:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/123:
openai.api_type = "azure"
openai.api_key = "9d3c1988c8c24db79d391ba83d5f761e"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = "9d3c1988c8c24db79d391ba83d5f761e"
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
1/124:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
1/125: print(embeddings)
1/126:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/127:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
    openai_api_base = "azure",
    openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
    openai_api_key = "https://cog-ycyy4wyxwg7ck.openai.azure.com/",
    deployment='text-embedding-ada-002',
    
    chunk_size=1,
)
1/128:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
    openai_api_type = "azure",
    openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
    openai_api_key = "https://cog-ycyy4wyxwg7ck.openai.azure.com/",
    deployment='text-embedding-ada-002',
    
    chunk_size=1,
)
1/129: print(embeddings)
1/130:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/131:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
    openai_api_type = "azure",
    openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
    openai_api_key = "https://cog-ycyy4wyxwg7ck.openai.azure.com/",
    deployment='embedding-ada',
    
    chunk_size=1,
)
1/132: print(embeddings)
1/133:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/134:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
    openai_api_type = "azure",
    openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
    openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
    deployment='embedding-ada',
    
    chunk_size=1,
)
1/135:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
    openai_api_type = "azure",
    openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
    openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
    deployment='embedding-ada',
    
    chunk_size=1,
)
1/136:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/137:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
    openai_api_type = "azure",
    openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
    openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
    engine='embedding-ada',
    
    chunk_size=1,
)
1/138:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/139:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(engine = "embedding-ada")
1/140:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/141:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(engine = "text-embedding-ada-002")
1/142:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/143:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(model = "text-embedding-ada-002",chunk_size=1) 
# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
1/144:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/145:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "text-embedding-ada-002",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
1/146:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/147:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
1/148:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/149:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/150:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }]
1/151: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
1/152: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
1/153: print(chain.run("Write a linkedIn post about Forte Digital"))
1/154:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }
]
1/155:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }
]
1/156:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
1/157:
openai.api_type = "azure"
openai.api_key = "9d3c1988c8c24db79d391ba83d5f761e"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = "9d3c1988c8c24db79d391ba83d5f761e"
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
1/158:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
1/159: print(embeddings)
1/160:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/161:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }
]
1/162: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
1/163: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
1/164: print(chain.run("Write a linkedIn post about Forte Digital"))
1/165:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }
]
1/166: chain = MultiRetrievalQAChain.from_retrievers(llm=llm, retriever= retriever_infos, verbose=True)
1/167: chain = MultiRetrievalQAChain.from_retrievers(llm=llm, retriever= retriever_infos, verbose=True)
1/168: chain = MultiRetrievalQAChain.from_retrievers(llm=llm, retriever=retriever_infos, verbose=True)
1/169: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
1/170: print(chain.run("Write a linkedIn post about Forte Digital"))
1/171: print(chain.run("Write a linkedIn post about Forte Digital"))
1/172: print(chain.run("Write a linkedIn post about Forte Digital"))
1/173:
print(chain.run("""    
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian. """))
1/174:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_BASE')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = s.getenv('OPENAI_API_BASE')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
1/175:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_BASE')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = s.getenv('OPENAI_API_BASE')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
1/176:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_BASE')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_BASE')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
1/177:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
1/178:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_BASE')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_BASE')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
1/179:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
1/180: print(embeddings)
1/181:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/182:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
1/183:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
1/184: print(embeddings)
1/185:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/186:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print(os.getenv('OPENAI_API_KEY')
1/187:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print(os.getenv('OPENAI_API_KEY'))
1/188:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print(os.getenv('OPENAI_API_KEY'))
1/189:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print(os.getenv('OPEN_API_KEY'))
1/190:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print(os.getenv('OPEN_API_KEY'))
1/191:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPEN_API_KEY'))
1/192:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPENAI_API_KEY'))
1/193:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPENAI_API_KEY'))
1/194:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPENAI_API_KEY'))
1/195:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPENAI_API_KEY'))
1/196:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
OPENAI_API_KEY = "9d3c1988c8c24db79d391ba83d5f761e"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPENAI_API_KEY'))
1/197:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
OPENAI_API_KEY = "9d3c1988c8c24db79d391ba83d5f761e"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPENAI_API_KEY'))
1/198:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
OPENAI_API_KEY = "9d3c1988c8c24db79d391ba83d5f761e"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPENAI_API_KEY'))
1/199:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
OPENAI_API_KEY = "9d3c1988c8c24db79d391ba83d5f761e"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

print("kes", os.getenv('OPENAI_API_KEY'))
1/200:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print("kes", os.getenv('OPENAI_API_KEY'))
1/201:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print("kes", openai.api_key)
1/202:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print("kes", openai.api_key)
1/203:
openai.api_type = "azure"
openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(OPENAI_API_KEY)
1/204:
openai.api_type = "azure"
openai.api_key = OPENAI_API_KEY
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(OPENAI_API_KEY)
1/205:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
1/206:

openai.api_type = "azure"
openai.api_key = OPENAI_API_KEY
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(OPENAI_API_KEY)
1/207:
load_dotenv()
openai.api_type = "azure"
openai.api_key = OPENAI_API_KEY
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(OPENAI_API_KEY)
1/208:
load_dotenv()
openai.api_type = "azure"
openai.api_key = OPENAI_API_KEY
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(OPENAI_API_KEY)
1/209:
load_dotenv()
openai.api_type = "azure"
openai.api_key = OPENAI_API_KEY
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(OPENAI_API_KEY)
1/210:
load_dotenv()
openai.api_type = "azure"
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(os.getenv("OPENAI_API_KEY"))
1/211:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/212:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
1/213:
load_dotenv()
openai.api_type = "azure"
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(os.getenv("OPENAI_API_KEY"))
1/214:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
1/215: print(embeddings)
1/216:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/217:
retriever_infos = [
    {
        "name": "Web context from forte digital", 
        "description": "General information and knowledge about Forte digital from their website", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "General linkedin post from forte digital, good for writing social media post. ",
        "retriever": pg_retriever
    }
]
1/218: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
1/219: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
1/220:
print(chain.run("""    
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian. """))
1/221:
print(chain.run("""    
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian. """))
1/222:
print(chain.run("""    
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian. """))
1/223: print(chain.run("What is forte digital"))
1/224: print(chain.run("What is forte digital"))
1/225: print(chain.run("What is forte digital?"))
1/226:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
1/227:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
1/228:
load_dotenv()
openai.api_type = "azure"
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")

print(os.getenv("OPENAI_API_KEY"))
1/229:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
1/230: print(embeddings)
1/231:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")

forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
1/232:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
1/233: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
1/234: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
1/235: print(chain.run("What is forte digital?"))
1/236: print(chain.run("Tell me about the technology company Forte Digital"))
1/237: print(chain.run("Tell me about the company Forte Digital"))
1/238: print(chain.run("Tell me about the company Forte Digital"))
1/239:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
1/240:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
1/241:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
 2/1:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
 2/2:
load_dotenv()
openai.api_type = "azure"
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
 2/3:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
 2/4:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
 2/5:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
 2/6:
load_dotenv()
openai.api_type = "azure"
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
 2/7:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
 2/8:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
 2/9:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
2/10:
load_dotenv()
openai.api_type = "azure"
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
2/11:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
2/12:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
2/13:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
2/14: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
2/15:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
2/16:
load_dotenv()
openai.api_type = "azure"
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
openai_api_version ="2023-03-15-preview"


os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
2/17:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
2/18:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
2/19:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
2/20: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
2/21:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
2/22:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
2/23:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
2/24:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
2/25:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
2/26: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
2/27: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
2/28:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
2/29:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
2/30:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
2/31:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
2/32:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
2/33:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
2/34: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
2/35: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
2/36:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
2/37:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
2/38:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
2/39:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
2/40:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
2/41:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
2/42:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
2/43: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
2/44: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
2/45:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
2/46:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
2/47:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
2/48:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
2/49:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
2/50:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
2/51: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
2/52: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
2/53:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
2/54:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
2/55:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
2/56:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
2/57:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
2/58:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
2/59: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
2/60: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
2/61:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
 5/1:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
 5/2:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
 5/3:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
 5/4:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
 5/5:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
 5/6: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
 5/7: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
 5/8:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
 5/9:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/10:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/11:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/12:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/13:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/14: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/15: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/16:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
5/17:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
5/18:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/19:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/20:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/21:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/22:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/23: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/24: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/25:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
5/26:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/27:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/28:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/29:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/30:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/31: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/32: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/33:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
5/34:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/35:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/36:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/37:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/38:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/39: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/40: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/41:
print(chain.run("""
    You are system writing social media post for Forte Digital. 
    write as many words as the avrage linkedin post in the pdf.  
    Use Forte Digital's personal tone and language which is provided in the pdf.  
    If the pdf is using emojies, then you can use the emoji in the message. 
    Write about their upcoming conferance RELEVANS. 
    Use examples and a link to the conference.
    Write in norwegian."""))
5/42:
print(chain.run("""
    Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse. Du skal analysere tekster som er laget av et teknologiselskapet Forte Digital
    og konkludere hvordan selskapet fremstr med fire ord. Gi ordene en prosentvis vekting, for  gi argumentasjon bak rekkeflgen"""))
5/43:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/44:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/45:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/46:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/47:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/48: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/49: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/50:
print(chain.run("""
    Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse. Du skal analysere tekster som er laget av et teknologiselskapet Forte Digital
    og konkludere hvordan selskapet fremstr med fire ord. Gi ordene en prosentvis vekting, for  gi argumentasjon bak rekkeflgen"""))
5/51:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/52:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/53:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/54:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/55:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/56: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/57: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/58:
print(chain.run("""
    Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse. Du skal analysere tekster som er laget av et teknologiselskapet Forte Digital
    og konkludere hvordan selskapet fremstr med fire ord. Gi ordene en prosentvis vekting, for  gi argumentasjon bak rekkeflgen"""))
5/59:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/60:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/61:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/62:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/63:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/64: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/65: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/66:
print(chain.run("""
    Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse. Du skal analysere tekster som er laget av et teknologiselskapet Forte Digital
    og konkludere hvordan selskapet fremstr med fire ord. Gi ordene en prosentvis vekting, for  gi argumentasjon bak rekkeflgen"""))
5/67:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/68:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/69:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/70:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/71:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/72: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/73: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/74:
print(chain.run("""
    Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse. Du skal analysere tekster som er laget av et teknologiselskapet Forte Digital
    og konkludere hvordan selskapet fremstr med fire ord. Gi ordene en prosentvis vekting, for  gi argumentasjon bak rekkeflgen"""))
5/75:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/76:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/77:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/78:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/79:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/80: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/81: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/82:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/83:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/84:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/85:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/86:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/87:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/88: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/89: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/90:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/91:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/92:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/93:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/94:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/95:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/96:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/97:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/98:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/99:
retriever_infos = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/100: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/101: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, verbose=True)
5/102:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/103:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/104:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/105:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/106:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/107:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/108: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/109: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/110:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/111:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/112:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/113:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/114:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()
5/115:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }
]
5/116: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/117: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/118:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order. Write in norwegian and display the output in JSON format"""))
5/119:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/120:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/121:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/122:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/123:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/124:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/125: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/126: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/127:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order. Write in norwegian and display the output in JSON format"""))
5/128:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/129:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/130:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/131:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/132:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/133: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/134: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/135:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order. Write in norwegian and display the output in JSON format"""))
5/136:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/137:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/138:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/139:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/140:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/141: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/142: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/143:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order. Write in norwegian and display the output in JSON format"""))
5/144:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/145:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/146:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/147:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/148:
retriever_info = [
    // {
    //     "name": "Web context of forte digital", 
    //     "description": "General information from website of Forte", 
    //     "retriever": sou_retriever
    // },
    
    // {
    //     "name": "Forte digital's linkedin posts", 
    //     "description": "Good for writing social meadia posts",
    //     "retriever": pg_retriever
    // }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    // {
    //     "name": "How to communicate as a Forte Digital Consultant.", 
    //     "description": "Good for answering how a Forte Consultant should conduct.",
    //     "retriever": consult_retriever
    // },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/149:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/150:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/151:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/152:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/153:
retriever_info = [
    // {
    //     "name": "Web context of forte digital", 
    //     "description": "General information from website of Forte", 
    //     "retriever": sou_retriever
    // },
    
    // {
    //     "name": "Forte digital's linkedin posts", 
    //     "description": "Good for writing social meadia posts",
    //     "retriever": pg_retriever
    // }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    // {
    //     "name": "How to communicate as a Forte Digital Consultant.", 
    //     "description": "Good for answering how a Forte Consultant should conduct.",
    //     "retriever": consult_retriever
    // },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/154:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/155:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/156:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/157:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/158:
retriever_info = [
    // {
    //     "name": "Web context of forte digital", 
    //     "description": "General information from website of Forte", 
    //     "retriever": sou_retriever
    // },
    
    // {
    //     "name": "Forte digital's linkedin posts", 
    //     "description": "Good for writing social meadia posts",
    //     "retriever": pg_retriever
    // }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    // {
    //     "name": "How to communicate as a Forte Digital Consultant.", 
    //     "description": "Good for answering how a Forte Consultant should conduct.",
    //     "retriever": consult_retriever
    // },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/159:
retriever_info = [
    // {
    //     "name": "Web context of forte digital", 
    //     "description": "General information from website of Forte", 
    //     "retriever": sou_retriever
    // },
    
    // {
    //     "name": "Forte digital's linkedin posts", 
    //     "description": "Good for writing social meadia posts",
    //     "retriever": pg_retriever
    // }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    // {
    //     "name": "How to communicate as a Forte Digital Consultant.", 
    //     "description": "Good for answering how a Forte Consultant should conduct.",
    //     "retriever": consult_retriever
    // },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/160:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/161:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/162:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/163:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/164:
retriever_info = [
    // {
    //     "name": "Web context of forte digital", 
    //     "description": "General information from website of Forte", 
    //     "retriever": sou_retriever
    // },
    
    // {
    //     "name": "Forte digital's linkedin posts", 
    //     "description": "Good for writing social meadia posts",
    //     "retriever": pg_retriever
    // }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    // {
    //     "name": "How to communicate as a Forte Digital Consultant.", 
    //     "description": "Good for answering how a Forte Consultant should conduct.",
    //     "retriever": consult_retriever
    // },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/165:
retriever_info = [
    // {
    //     "name": "Web context of forte digital", 
    //     "description": "General information from website of Forte", 
    //     "retriever": sou_retriever
    // },
    
    // {
    //     "name": "Forte digital's linkedin posts", 
    //     "description": "Good for writing social meadia posts",
    //     "retriever": pg_retriever
    // }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    // {
    //     "name": "How to communicate as a Forte Digital Consultant.", 
    //     "description": "Good for answering how a Forte Consultant should conduct.",
    //     "retriever": consult_retriever
    // },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/166:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/167:
// retriever_info = [
//     {
//         "name": "Web context of forte digital", 
//         "description": "General information from website of Forte", 
//         "retriever": sou_retriever
//     },
    
//     {
//         "name": "Forte digital's linkedin posts", 
//         "description": "Good for writing social meadia posts",
//         "retriever": pg_retriever
//     }, 
//     {
//         "name": "Forte way of work", 
//         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
//         "retriever": wayofwork_retriever
//     },
//     {
//         "name": "How to communicate as a Forte Digital Consultant.", 
//         "description": "Good for answering how a Forte Consultant should conduct.",
//         "retriever": consult_retriever
//     },
//     {
//         "name": "Forte e-commerce strategy", 
//         "description": "About the Forte digital@s e-commerce strategy",
//         "retriever": strategy_retriever
//     }
// ]
5/168:
// retriever_info = [
//     {
//         "name": "Web context of forte digital", 
//         "description": "General information from website of Forte", 
//         "retriever": sou_retriever
//     },
    
//     {
//         "name": "Forte digital's linkedin posts", 
//         "description": "Good for writing social meadia posts",
//         "retriever": pg_retriever
//     }, 
//     {
//         "name": "Forte way of work", 
//         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
//         "retriever": wayofwork_retriever
//     },
//     {
//         "name": "How to communicate as a Forte Digital Consultant.", 
//         "description": "Good for answering how a Forte Consultant should conduct.",
//         "retriever": consult_retriever
//     },
//     {
//         "name": "Forte e-commerce strategy", 
//         "description": "About the Forte digital@s e-commerce strategy",
//         "retriever": strategy_retriever
//     }
// ]
5/169:
// retriever_info = [
//     {
//         "name": "Web context of forte digital", 
//         "description": "General information from website of Forte", 
//         "retriever": sou_retriever
//     },
    
//     {
//         "name": "Forte digital's linkedin posts", 
//         "description": "Good for writing social meadia posts",
//         "retriever": pg_retriever
//     }, 
//     {
//         "name": "Forte way of work", 
//         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
//         "retriever": wayofwork_retriever
//     },
//     {
//         "name": "How to communicate as a Forte Digital Consultant.", 
//         "description": "Good for answering how a Forte Consultant should conduct.",
//         "retriever": consult_retriever
//     },
//     {
//         "name": "Forte e-commerce strategy", 
//         "description": "About the Forte digital@s e-commerce strategy",
//         "retriever": strategy_retriever
//     }
// ]
5/170:
// retriever_info = [
//     {
//         "name": "Web context of forte digital", 
//         "description": "General information from website of Forte", 
//         "retriever": sou_retriever
//     },
    
//     {
//         "name": "Forte digital's linkedin posts", 
//         "description": "Good for writing social meadia posts",
//         "retriever": pg_retriever
//     }, 
//     {
//         "name": "Forte way of work", 
//         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
//         "retriever": wayofwork_retriever
//     },
//     {
//         "name": "How to communicate as a Forte Digital Consultant.", 
//         "description": "Good for answering how a Forte Consultant should conduct.",
//         "retriever": consult_retriever
//     },
//     {
//         "name": "Forte e-commerce strategy", 
//         "description": "About the Forte digital@s e-commerce strategy",
//         "retriever": strategy_retriever
//     }
// ]
5/171:
# retriever_info = [
#     {
#         "name": "Web context of forte digital", 
#         "description": "General information from website of Forte", 
#         "retriever": sou_retriever
#     },
    
#     {
#         "name": "Forte digital's linkedin posts", 
#         "description": "Good for writing social meadia posts",
#         "retriever": pg_retriever
#     }, 
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },
#     {
#         "name": "How to communicate as a Forte Digital Consultant.", 
#         "description": "Good for answering how a Forte Consultant should conduct.",
#         "retriever": consult_retriever
#     },
#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/172:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/173:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/174:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/175:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/176:
# retriever_info = [
#     {
#         "name": "Web context of forte digital", 
#         "description": "General information from website of Forte", 
#         "retriever": sou_retriever
#     },
    
#     {
#         "name": "Forte digital's linkedin posts", 
#         "description": "Good for writing social meadia posts",
#         "retriever": pg_retriever
#     }, 
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },
#     {
#         "name": "How to communicate as a Forte Digital Consultant.", 
#         "description": "Good for answering how a Forte Consultant should conduct.",
#         "retriever": consult_retriever
#     },
#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/177:
retriever_info = [
   
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },

    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/178: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/179: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/180:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order. Write in norwegian and display the output in JSON format"""))
5/181:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/182:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/183:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/184:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/185:
# retriever_info = [
#     {
#         "name": "Web context of forte digital", 
#         "description": "General information from website of Forte", 
#         "retriever": sou_retriever
#     },
    
#     {
#         "name": "Forte digital's linkedin posts", 
#         "description": "Good for writing social meadia posts",
#         "retriever": pg_retriever
#     }, 
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },
#     {
#         "name": "How to communicate as a Forte Digital Consultant.", 
#         "description": "Good for answering how a Forte Consultant should conduct.",
#         "retriever": consult_retriever
#     },
#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/186:
retriever_info = [
   
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },

    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/187: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/188: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/189:
print(chain.run("""You are a clever marketer with expertise in text analysis. You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order. Write in norwegian and display the output in JSON format"""))
5/190:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/191:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/192:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/193:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/194:
# retriever_info = [
#     {
#         "name": "Web context of forte digital", 
#         "description": "General information from website of Forte", 
#         "retriever": sou_retriever
#     },
    
#     {
#         "name": "Forte digital's linkedin posts", 
#         "description": "Good for writing social meadia posts",
#         "retriever": pg_retriever
#     }, 
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },
#     {
#         "name": "How to communicate as a Forte Digital Consultant.", 
#         "description": "Good for answering how a Forte Consultant should conduct.",
#         "retriever": consult_retriever
#     },
#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/195:
retriever_info = [
   
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },

    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/196: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/197: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/198:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/199:
retriever_info = [
   
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },

    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/200: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/201: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/202:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/203:
retriever_info = [
   
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },

    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/204: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/205: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/206:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/207:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/208:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/209:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/210:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/211:
# retriever_info = [
#     {
#         "name": "Web context of forte digital", 
#         "description": "General information from website of Forte", 
#         "retriever": sou_retriever
#     },
    
#     {
#         "name": "Forte digital's linkedin posts", 
#         "description": "Good for writing social meadia posts",
#         "retriever": pg_retriever
#     }, 
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },
#     {
#         "name": "How to communicate as a Forte Digital Consultant.", 
#         "description": "Good for answering how a Forte Consultant should conduct.",
#         "retriever": consult_retriever
#     },
#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/212:
retriever_info = [
   
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },

    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/213: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/214: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/215:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/216:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/217:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order."""))
5/218:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
                << OUTPUT (must include ```json at the start of the response) >>"""))
5/219:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/220:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/221:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/222:
# loader = PyPDFLoader("forte_data/forte_web.pdf")
# forte_web_pages = loader.load_and_split()
# sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
# forte_some_pages = loader.load_and_split()
# pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/223:
# retriever_info = [
#     {
#         "name": "Web context of forte digital", 
#         "description": "General information from website of Forte", 
#         "retriever": sou_retriever
#     },
    
#     {
#         "name": "Forte digital's linkedin posts", 
#         "description": "Good for writing social meadia posts",
#         "retriever": pg_retriever
#     }, 
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },
#     {
#         "name": "How to communicate as a Forte Digital Consultant.", 
#         "description": "Good for answering how a Forte Consultant should conduct.",
#         "retriever": consult_retriever
#     },
#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/224:
retriever_info = [
   
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },

    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/225: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/226: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/227:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
                << OUTPUT (must include ```json at the start of the response) >>"""))
5/228:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/229:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/230:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/231:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/232:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/233:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/234: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/235: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/236:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
                << OUTPUT (must include ```json at the start of the response) >>"""))
5/237:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/238:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/239:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/240:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/241:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/242:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/243: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/244: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/245:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
                << OUTPUT (must include ```json at the start of the response) >>"""))
5/246:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/247:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/248: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/249: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/250:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
                << OUTPUT (must include ```json at the start of the response) >>"""))
5/251:
print(chain.run("""Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
                << OUTPUT (must include ```json at the start of the response) >>"""))
5/252:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/253:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/254:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/255:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/256:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/257:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/258: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/259: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/260:
print(chain.run("""  << OUTPUT (must include ```json at the start of the response) >>. Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
  """))
5/261:
print(chain.run("""  << OUTPUT (must include ```json at the start of the response) >>. Write in norwegian and display the output in JSON format. You are a clever marketer with expertise in text analysis. 
   What are the company strategy towards e-commerce?
  """))
5/262:
print(chain.run("""  << OUTPUT (must include ```json at the start of the response) >>. Write in norwegian and display the output in JSON format. 
   What are the company strategy towards e-commerce?
  """))
5/263:
print(chain.run("""  << OUTPUT (must include ```json at the start of the response) >>. Write in norwegian and display the output in JSON format. 
   What are the company strategy towards e-commerce?
  """))
5/264:
print(chain.run("""  << OUTPUT (must include ```json at the start of the response) >>. Write in norwegian and display the output in JSON format. 
   What are the company strategy towards e-commerce?
  """))
5/265:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/266:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/267:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/268:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/269:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/270:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/271: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/272: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True,return_intermediate_steps=True)
5/273:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/274:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/275:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/276:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/277:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/278:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/279: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/280: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/281:
print(chain.run("""  << OUTPUT (must include ```json at the start of the response) >>. Write in norwegian and display the output in JSON format. 
   What are the company strategy towards e-commerce?
  """))
5/282:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/283:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/284:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/285:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/286:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/287:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/288: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/289: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/290:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian and display the output in JSON format. 
   What are the company strategy towards e-commerce?
  """))
5/291:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian and display the output in JSON format. 
   What are the company strategy towards e-commerce?
  """))
5/292:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian and display the output in JSON format. 
   What are the company strategy towards e-commerce?
  """))
5/293:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   What are the company strategy towards e-commerce?
  """))
5/294:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/295:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/296:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/297:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/298:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/299:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/300: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/301: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/302:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   What are the company strategy towards e-commerce?
  """))
5/303:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   What are the company strategy towards e-commerce?
  """))
5/304:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/305:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/306:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/307:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/308:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/309:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/310: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/311: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/312:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   What are the company strategy towards e-commerce?
  """))
5/313:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/314:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/315:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/316:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/317:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/318:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/319: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/320: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/321:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. 
    Du skal analysere strategidokumenter som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier, spesielt linkedin. 
    Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
    Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. 
    Disse konkurrentene skal nevnes med navn.
  """))
5/322:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/323:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/324:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/325:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/326:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/327:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/328: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/329: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/330:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. 
    Du skal analysere strategidokumenter som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier, spesielt linkedin. 
    Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
    Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. 
    Disse konkurrentene skal nevnes med navn.
  """))
5/331:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/332:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/333:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/334:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/335:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/336:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/337: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/338: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/339:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. 
    Du skal analysere strategidokumenter som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier, spesielt linkedin. 
    Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
    Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. 
    Disse konkurrentene skal nevnes med navn.
  """))
5/340:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/341:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/342:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/343:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/344:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/345:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/346: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/347: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/348:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. 
    Du skal analysere strategidokumenter som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier, spesielt linkedin. 
    Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
    Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. 
    Disse konkurrentene skal nevnes med navn.
  """))
5/349:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/350:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/351:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/352:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/353:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/354:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/355: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/356: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/357:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian. 
   Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. 
    Du skal analysere strategidokumenter som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier, spesielt linkedin. 
    Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
    Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. 
    Disse konkurrentene skal nevnes med navn.
  """))
5/358:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/359:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/360:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/361:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/362:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/363:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/364: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/365: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/366:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian and display the output in JSON format. 
    You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
  """))
5/367:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/368:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/369:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/370:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/371:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/372:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/373: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/374: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/375:
print(chain.run("""  << OUTPUT (must include ```json at the start and end of the response) >>. Write in norwegian and display the output in JSON format. 
    You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
  """))
5/376:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/377:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/378:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/379:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
forte_consult = loader.load_and_split()
consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
forte_wayofwork_pages = loader.load_and_split() 
wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/380:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte way of work", 
        "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
        "retriever": wayofwork_retriever
    },
    {
        "name": "How to communicate as a Forte Digital Consultant.", 
        "description": "Good for answering how a Forte Consultant should conduct.",
        "retriever": consult_retriever
    },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/381:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/382: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/383: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/384:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response). Write in norwegian and display the output in JSON format. 
    You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
  """))
5/385:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/386:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/387:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/388:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/389:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/390:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/391: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/392: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/393:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response). Write in norwegian and display the output in JSON format. 
    You are a clever marketer with expertise in text analysis. 
    You are going to analyze texts created by the technology company Forte Digital and conclude how the 
    company appears in four words. Assign a percentage weighting to the words to provide reasoning for the order.
  """))
5/394:
print(chain.run(""" Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. Disse konkurrentene skal nevnes med navn.
  """))
5/395: print(chain.run(""" You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name.  """))
5/396:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/397:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/398:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/399:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/400:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/401:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/402: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/403: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/404: print(chain.run(""" You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name.  """))
5/405:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/406:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/407:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/408:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/409:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/410:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/411: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/412: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/413:
print(chain.run("""   OUTPUT (must include ```json at the start of the response). Write in norwegian and display the output in JSON format. 
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name.
  """))
5/414:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/415:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/416:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/417:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/418:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/419:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/420: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/421: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/422:
print(chain.run("""   OUTPUT (must include ```json at the start of the response). Write in norwegian and display the output in JSON format. 
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name.
  """))
5/423:
print(chain.run("""   OUTPUT (must include ```json at the start of the response and be in norwegian).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name.
  """))
5/424:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/425:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/426:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/427:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/428:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/429:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/430: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/431: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/432:
print(chain.run("""   OUTPUT (must include ```json at the start of the response and be in norwegian).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name.
  """))
5/433:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/434:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/435:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/436:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/437:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/438:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/439: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/440: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/441:
print(chain.run("""   OUTPUT (must include ```json at the start of the response and be in norwegian).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name.
  """))
5/442:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/443:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/444:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/445:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/446:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/447:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/448:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/449:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/450:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/451:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/452:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/453:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/454:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/455:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/456:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/457:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/458: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/459: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/460:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/461:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/462:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/463:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/464:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/465:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/466:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/467: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/468: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/469:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/470:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/471:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/472:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/473:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/474:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/475:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/476: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/477: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/478:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/479:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/480:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/481:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/482:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/483:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/484:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/485: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/486: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/487:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/488:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/489:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/490:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/491:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/492:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/493:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/494: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/495: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/496:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian. Skriv svaret p norsk
  """))
5/497:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/498:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/499:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/500:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/501:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/502:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/503: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/504: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/505:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format.
                Skriv svaret p norsk
  """))
5/506:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/507:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/508:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/509:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/510:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/511:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/512: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/513: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/514:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format.
                Skriv svaret p norsk
  """))
5/515:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/516:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/517:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/518:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/519:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/520:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/521: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/522: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/523:
print(chain.run("""   OUTPUT (must include ```json at the start of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format.
                Skriv svaret p norsk
  """))
5/524:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format.
                Skriv svaret p norsk
  """))
5/525:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/526:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/527:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/528:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/529:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/530:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/531: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/532: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/533:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response).
 You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, 
                which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. 
                Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
                You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
                and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format.
                Skriv svaret p norsk
  """))
5/534:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing {content internal}, which consists of internal documents created by a technology company, external web content {content web}, and social media content {content some}. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk
  """))
5/535:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/536:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/537:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/538:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/539:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/540:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/541: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/542: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/543:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk
  """))
5/544:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/545:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/546:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/547:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/548:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/549:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/550: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/551: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/552:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk
  """))
5/553:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/554:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/555:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/556:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/557:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/558:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/559:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/560:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/561:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/562: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/563: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/564:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk
  """))
5/565:
print(chain.run("""   OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk
  """))
5/566:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/567:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/568:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/569:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/570:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/571:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/572: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/573: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/574: print(chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk"))
5/575:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/576:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/577:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/578:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/579:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/580:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/581: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/582: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/583: print(chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk"))
5/584: print(chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk"))
5/585:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/586:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/587:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/588:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/589:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/590:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/591:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/592:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/593:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/594:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/595: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/596: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/597: print(chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format. Skriv svaret p norsk"))
5/598: english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))
5/599: print(llm(english_chain))
5/600:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/601:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/602:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/603:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/604:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/605:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/606: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/607: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/608:
english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/609: print(llm(english_chain))
5/610:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/611:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/612:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/613:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/614:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/615:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/616: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/617: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/618:
english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/619:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/620:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/621:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/622:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/623:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/624:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/625: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/626: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/627:
english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/628: print(llm(english_chain))
5/629:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/630:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/631:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/632:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/633:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/634:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/635: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/636: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=False)
5/637:
english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/638:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/639:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/640:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/641:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/642:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/643:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/644: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/645: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=False)
5/646:
english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/647:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/648:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/649:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/650:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/651:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/652:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/653: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/654: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=False)
5/655:
english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/656:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/657:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/658:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/659:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/660:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/661:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/662: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/663: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=False)
5/664:
english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/665:
import langchainq
langchain.debug = True

english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/666:
import langchain
langchain.debug = True

english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/667:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/668:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/669:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/670:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/671:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/672:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/673: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/674: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/675:
import langchain
langchain.debug = True

english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/676:
from langchain.chains.router import MultiRetrievalQAChain
from langchain.llms import OpenAI
from langchain.llms import AzureOpenAI
import openai
import os
from dotenv import load_dotenv
5/677:
load_dotenv()
# openai.api_type = "azure"
# openai.api_key = os.getenv("OPENAI_API_KEY")
# openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com/"
# openai_api_version = "2023-03-15-preview"

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
5/678:
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader

# embeddings = OpenAIEmbeddings(deployment="embedding-ada",engine="text-embedding-ada-002")
# embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, deployment_id="embedding-ada", model_name = "text-embedding-ada-002", client="azure", chunk_size=1) 
# embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 

# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(
#     openai_api_type = "azure",
#     openai_api_base = "9d3c1988c8c24db79d391ba83d5f761e",
#     openai_api_key = f"https://cog-ycyy4wyxwg7ck.openai.azure.com/",
#     engine='embedding-ada',
    
#     chunk_size=1,
# )

embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 

# openai.Embedding.create(input=x, engine='embedding-ada')['data'][0]['embedding'])
5/679:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte WoW communication and psychological safety.pdf")
# forte_consult = loader.load_and_split()
# consult_retriever = FAISS.from_documents(forte_consult, embeddings).as_retriever()

# loader = PyPDFLoader("forte_data/Forte Way of Work nyansatte august 2022.pdf")
# forte_wayofwork_pages = loader.load_and_split() 
# wayofwork_retriever = FAISS.from_documents(forte_wayofwork_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
5/680:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    # {
    #     "name": "Forte way of work", 
    #     "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
    #     "retriever": wayofwork_retriever
    # },
    # {
    #     "name": "How to communicate as a Forte Digital Consultant.", 
    #     "description": "Good for answering how a Forte Consultant should conduct.",
    #     "retriever": consult_retriever
    # },
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
5/681:
# retriever_info = [
   
#     {
#         "name": "Forte way of work", 
#         "description": "Information about Forte ideologi and working ethics and how they work in teams.", 
#         "retriever": wayofwork_retriever
#     },

#     {
#         "name": "Forte e-commerce strategy", 
#         "description": "About the Forte digital@s e-commerce strategy",
#         "retriever": strategy_retriever
#     }
# ]
5/682: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
5/683: chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
5/684:
import langchain
langchain.debug = True

english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/685:
import langchain
langchain.debug = True

english_chain = (chain.run("OUTPUT (must include ```json at the start and end of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian and provide the output in JSON format."))

print(english_chain)
5/686:
import langchain
langchain.debug = True

english_chain = (chain.run("OUTPUT (must include ```json at the start of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."))

print(english_chain)
 6/1:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoaderv
import os
from langchain import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
 6/2:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoaderv
import os
from langchain import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
 6/3:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoaderv
import os
from langchain import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
 6/4:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoaderv
import os
from langchain import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
 6/5:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
 6/6:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
 6/7:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
 6/8:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
 6/9:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
6/10:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/11:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/12: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/13: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/14:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings, collection_name="Web context of forte digital").as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = Chroma.from_documents(forte_some_pages, embeddings,collection_name="Forte digital's linkedin posts").as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = Chroma.from_documents(forte_strategy_pages, embeddings,collection_name="Forte e-commerce strategy").as_retriever()
6/15:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
6/16:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/17:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/18:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings, collection_name="Web context of forte digital").as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = Chroma.from_documents(forte_some_pages, embeddings,collection_name="Forte digital's linkedin posts").as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = Chroma.from_documents(forte_strategy_pages, embeddings,collection_name="Forte e-commerce strategy").as_retriever()
6/19:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
6/20:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/21:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/22: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/23: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/24:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings, collection_name="Web context of forte digital").as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = Chroma.from_documents(forte_some_pages, embeddings,collection_name="Forte digital's linkedin posts").as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = Chroma.from_documents(forte_strategy_pages, embeddings,collection_name="Forte e-commerce strategy").as_retriever()
6/25:
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)

vectorstore_info = VectorStoreInfo(
    name="state_of_union_address",
    description="the most recent state of the Union adress",
    vectorstore=state_of_union_store,
)
ruff_vectorstore_info = VectorStoreInfo(
    name="ruff",
    description="Information about the Ruff python linting library",
    vectorstore=ruff_store,
)
6/26:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings, collection_name="Web context of forte digital").as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = Chroma.from_documents(forte_some_pages, embeddings,collection_name="Forte digital's linkedin posts").as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = Chroma.from_documents(forte_strategy_pages, embeddings,collection_name="Forte e-commerce strategy").as_retriever()
6/27:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
6/28:
loader = PyPDFLoader("forte_data/forte_web.pdf")
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
forte_web_pages = text_splitter.split_documents(loader)
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings, collection_name="Web context of forte digital").as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = Chroma.from_documents(forte_some_pages, embeddings,collection_name="Forte digital's linkedin posts").as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = Chroma.from_documents(forte_strategy_pages, embeddings,collection_name="Forte e-commerce strategy").as_retriever()
6/29:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
6/30:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)


loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = text_splitter.split_documents(loader)
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings).as_retriever()
6/31:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)


loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = text_splitter.split_documents(loader)
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings).as_retriever()
6/32:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)


loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = text_splitter.split_documents(loader.load())
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings).as_retriever()
6/33:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)


loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = text_splitter.split_documents(loader.load())
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings).as_retriever()
6/34:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)


loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = text_splitter.split_documents(loader.load())
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings).as_retriever()
6/35:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)


loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = text_splitter.split_documents(loader.load())
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings).as_retriever()
6/36:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)


loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = text_splitter.split_documents(loader.load())
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings).as_retriever()
6/37:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
import chromadb.config
6/38:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
import chromadb.config
6/39:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
import chromadb.config
6/40:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/41:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/42: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/43: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/44:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
6/45:
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)


loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = text_splitter.split_documents(loader.load())
sou_retriever = Chroma.from_documents(forte_web_pages, embeddings).as_retriever()
6/46:
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)

vectorstore_info = VectorStoreInfo(
    name="state_of_union_address",
    description="the most recent state of the Union adress",
    vectorstore=state_of_union_store,
)
ruff_vectorstore_info = VectorStoreInfo(
    name="ruff",
    description="Information about the Ruff python linting library",
    vectorstore=ruff_store,
)
6/47:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/48:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
6/49:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/50:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
6/51:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/52:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/53:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/54:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="RandomWord",
        func=chain.run,
        description="call this to get a random word.",
    ),
]
6/55:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish
6/56:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish
6/57:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/58:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/59: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/60: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/61:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
6/62:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
6/63:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/64:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=chain.run,
        description="useful for when you need to answer questions about Forte Digital",
    ),
]
6/65:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
6/66: agent = FakeAgent()
6/67:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
6/68: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian.")
6/69:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish
6/70:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/71:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/72: llm = AzureOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/73: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/74:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
6/75:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
6/76:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/77:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=chain.run,
        description="useful for when you need to answer questions about Forte Digital",
    ),
]
6/78:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
6/79: agent = FakeAgent()
6/80:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
6/81: agent_executor.run("OUTPUT (must include ```json at the start of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
6/82: agent_executor.run("OUTPUT (must include ```json at the start of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
6/83: agent_executor.run("OUTPUT (must include ```json at the start of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
6/84:
loader = PyPDFLoader("forte_data/forte_web.pdf")

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
documents = text_splitter.split_documents(loader.load())
6/85:
loader = PyPDFLoader("forte_data/forte_web.pdf")

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
documents = text_splitter.split_documents(loader.load())

vectordb = Chroma.from_documents(documents, embedding=embeddings(), persist_directory="./data")
vectordb.persist()
6/86:
loader = PyPDFLoader("forte_data/forte_web.pdf")

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
documents = text_splitter.split_documents(loader.load())

vectordb = Chroma.from_documents(documents, embedding=embeddings, persist_directory="./data")
vectordb.persist()
6/87:
from langchain.chains import RetrievalQA

loader = PyPDFLoader("forte_data/forte_web.pdf")

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
documents = text_splitter.split_documents(loader.load())

vectordb = Chroma.from_documents(documents, embedding=embeddings, persist_directory="./data")
vectordb.persist()

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(search_kwargs={'k': 7}),
    return_source_documents=True
)

result = qa_chain({'query': 'Who is the CV about?'})
print(result['result'])
6/88:
from langchain.chains import RetrievalQA

loader = PyPDFLoader("forte_data/forte_web.pdf")

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
documents = text_splitter.split_documents(loader.load())

vectordb = Chroma.from_documents(documents, embedding=embeddings, persist_directory="./data")
vectordb.persist()

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(search_kwargs={'k': 7}),
    return_source_documents=True
)

result = qa_chain({'query': 'Who is the CV about?'})
print(result['result'])
6/89:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish
6/90:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/91:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/92: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/93: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/94:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
6/95:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
6/96:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/97:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=chain.run,
        description="useful for when you need to answer questions about Forte Digital",
    ),
]
6/98:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
6/99: agent = FakeAgent()
6/100:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
6/101: agent_executor.run("OUTPUT (must include ```json at the start of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
6/102: agent_executor.run("OUTPUT (must include ```json at the start of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
6/103: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
6/104:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish
6/105:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/106:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/107: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/108: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/109:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
6/110:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
6/111:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/112:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=chain.run,
        description="useful for when you need to answer questions about Forte Digital",
    ),
]
6/113:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
6/114: agent = FakeAgent()
6/115:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
6/116: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
6/117:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish
6/118:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/119:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/120: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/121: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/122:
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
sou_retriever = FAISS.from_documents(forte_web_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/some_forte_relevanse.pdf")
forte_some_pages = loader.load_and_split()
pg_retriever = FAISS.from_documents(forte_some_pages, embeddings).as_retriever()

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
strategy_retriever = FAISS.from_documents(forte_strategy_pages, embeddings).as_retriever()
6/123:
retriever_info = [
    {
        "name": "Web context of forte digital", 
        "description": "General information from website of Forte", 
        "retriever": sou_retriever
    },
    {
        "name": "Forte digital's linkedin posts", 
        "description": "Good for writing social meadia posts",
        "retriever": pg_retriever
    }, 
    {
        "name": "Forte e-commerce strategy", 
        "description": "About the Forte digital@s e-commerce strategy",
        "retriever": strategy_retriever
    }
]
6/124:
chain = MultiRetrievalQAChain.from_retrievers(llm, retriever_info, verbose=True)
search = SerpAPIWrapper()
6/125:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=chain.run,
        description="useful for when you need to answer questions about Forte Digital",
    ),
]
6/126:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
6/127: agent = FakeAgent()
6/128:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
6/129: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
6/130: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
 7/1:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)


forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="IGeneral information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
 7/2:


agent_executor.run(
    "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian."
)
 7/3:


agent_executor.run(
    "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian."
)
 7/4:


agent_executor.run(
    "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian."
)
 7/5:


agent_executor.run(
    "Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. Disse konkurrentene skal nevnes med navn."
)
 7/6:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
 7/7:


agent_executor.run(
    "Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. Disse konkurrentene skal nevnes med navn."
)
 7/8:


agent_executor.run(
    "Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. Disse konkurrentene skal nevnes med navn."
)
 7/9:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. Disse konkurrentene skal nevnes med navn."
)
7/10:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
7/11:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent og hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter. Disse konkurrentene skal nevnes med navn."
)
6/131:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
6/132:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
6/133:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
6/134: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
6/135: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
6/136:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
6/137:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=chain.run,
        description="useful for when you need to answer questions about Forte Digital",
    ),
]
6/138:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
6/139: agent = FakeAgent()
6/140:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
6/141: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
 8/1:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
 8/2:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
 8/3:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
 8/4: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
 8/5: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
 8/6:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
 8/7:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
 8/8:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
 8/9:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/10:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/11: agent = FakeAgent()
8/12:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/13: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
8/14:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/15:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/16:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/17: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/18: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/19:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/20:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events and information not in the files",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/21:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/22: agent = FakeAgent()
8/23:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/24: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
8/25: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
8/26: agent_executor.run("You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. Write everything in norwegian.")
8/27: agent_executor.run("Write everything in norwegian. You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. ")
8/28:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors, and to triangulate the data",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/29:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors, and to triangulate the data",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/30:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/31: agent = FakeAgent()
8/32:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/33: agent_executor.run("Write everything in norwegian. You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. ")
8/34:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/35:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/36:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/37: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/38: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/39:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/40:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors, and to triangulate the data",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/41:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/42: agent = FakeAgent()
8/43:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/44: agent_executor.run("Write everything in norwegian. You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name and you may search the internett. ")
8/45: print(agent_executor)
 9/1:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
 9/2:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
 9/3:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
8/46:
agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
    )
 9/4:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
 9/5:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
8/47:
agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
    )
8/48:
agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
    )
8/49: search.run("Obama's first name?")
8/50:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/51:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/52:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/53: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/54: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/55:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/56:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/57:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/58: agent = FakeAgent()
8/59:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/60:
agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
    )
8/61:
agent_executor.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/62:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/63:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/64:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/65: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/66: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/67:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/68:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/69:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/70: agent = FakeAgent()
8/71:
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/72:
agent_executor.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/73:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/74:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/75:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/76: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/77: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/78:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/79:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/80:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/81: agent = FakeAgent()
8/82:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/83:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/84:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/85:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/86:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/87: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/88: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/89:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/90:
search = SerpAPIWrapper()
tools = [
    # Tool(
    #     name="Search",
    #     func=search.run,
    #     description="useful for when you need to answer questions about competitors and market",
    # ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/91:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/92: agent = FakeAgent()
8/93:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/94:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/95:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/96:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/97:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/98: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/99: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/100:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/101:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/102:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/103: agent = FakeAgent()
8/104:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/105:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/106:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/107:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/108: agent = FakeAgent()
8/109:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/110:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/111:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/112: agent = FakeAgent()
8/113:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/114:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/115:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/116:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/117:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/118:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/119:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/120: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/121: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/122:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/123:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/124:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/125: agent = FakeAgent()
8/126:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/127:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/128:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/129:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
 9/6:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
 9/7:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
5/687:
import langchain
langchain.debug = True

english_chain = (chain.run("OUTPUT (must include ```json at the start of the response). You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."))

print(english_chain)
 9/8:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
 9/9:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
9/10:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
8/130:
agent_executor_web.run(
   "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokument, which consists of internal strategy documents created by a technology company named Forte Digital, web content from document, and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
    )
8/131:
agent_executor_web.run(
   "WHat is obamas fully name"
    )
8/132:
agent_executor_web.run(
   "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
    )
8/133:
agent_executor_web.run(
   "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
    )
8/134:
agent_executor_web.run(
   "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
    )
8/135:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.OPENAI_MULTI_FUNCTIONS, verbose=True
)
8/136:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.OPENAI_MULTI_FUNCTIONS, verbose=True
)
8/137:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run("What is the weather in LA and SF?")
8/138:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/139:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/140:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/141: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/142: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/143:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/144:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/145:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/146: agent = FakeAgent()
8/147:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/148:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/149:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.OPENAI_MULTI_FUNCTIONS, verbose=True
)
8/150:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run("What is the weather in LA and SF?")
8/151:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/152:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/153:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/154: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/155: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/156:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/157:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/158:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/159: agent = FakeAgent()
8/160:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/161:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/162:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.OPENAI_MULTI_FUNCTIONS, verbose=True
)
8/163:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run("What is the weather in LA and SF?")
8/164:
searchs =   Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
8/165:
mrkl = initialize_agent(
    searchs, llm, agent=AgentType.OPENAI_MULTI_FUNCTIONS, verbose=True
)
8/166:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run("What is the weather in LA and SF?")
8/167:
mrkl = initialize_agent(
    searchs, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/168:
mrkl = initialize_agent(
    searchs, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/169:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run("What is the weather in LA and SF?")
8/170:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
         )
8/171:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/172:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/173:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/174:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/175:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
         )
8/176:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
         )
8/177:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/178:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/179:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/180: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/181: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/182:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/183:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/184:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/185: agent = FakeAgent()
8/186:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/187:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/188:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/189:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
         )
8/190:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/191:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/192:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/193: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/194: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/195:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/196:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
         )
8/197:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
         )
9/11:


# agent_executor.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
# )
agent_executor.run(
"You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
)
8/198:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
#          )
mrkl.run(
"You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
)
8/199:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/200:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/201:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/202: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
8/203: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/204:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/205:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/206:



class FakeAgent(BaseMultiActionAgent):
    """Fake Custom Agent."""

    @property
    def input_keys(self):
        return ["input"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[List[AgentAction], AgentFinish]:
        """Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations
            **kwargs: User inputs.

        Returns:
            Action specifying what tool to use.
        """
        if len(intermediate_steps) == 0:
            return [
                AgentAction(tool="Search", tool_input=kwargs["input"], log=""),
                # AgentAction(tool="file_search", tool_input=kwargs["input"], log=""),
            ]
        else:
            return AgentFinish(return_values={"output": "bar"}, log="")
8/207: agent = FakeAgent()
8/208:
agent_executor_web = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True
)
8/209:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/210:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/211:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
#          )
mrkl.run(
"You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
)
8/212:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
#          )
mrkl.run(
"""You are a savvy marketer with expertise in text analysis and company positioning.
 You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
 and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
 You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
 and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."""
)
8/213:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
#          )
mrkl.run(
"""You are a savvy marketer with expertise in text analysis and company positioning.
 You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
 and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
 You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
 and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."""
)
8/214: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/215:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/216:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/217:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/218: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/219: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/220:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/221:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/222:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/223:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/224:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
#          )
mrkl.run(
"""You are a savvy marketer with expertise in text analysis and company positioning.
 You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
 and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
 You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
 and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
 Write everything in norwegian."""
)
8/225:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"
         )
# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/226:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
     Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
     Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
     Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
     Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
    )
# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/227:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

mrkl.run(
    """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
     Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
     Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
     Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
     Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
    )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    vennligst gi klare retningslinjer for  analysere et teknologiselskaps interne dokumenter, eksternt webinnhold og sosiale medier. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Svar p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/228:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    vennligst gi klare retningslinjer for  analysere et teknologiselskaps interne dokumenter, eksternt webinnhold og sosiale medier. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Svar p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/229:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/230:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/231:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/232: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/233: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/234:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/235:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/236:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/237:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/238:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyseret teknologiselskaps interne dokumenter, eksternt webinnhold og sosiale medier. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Svar p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/239:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/240:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/241:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/242: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/243: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/244:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/245:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/246:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/247:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/248:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne dokumenter, eksternt webinnhold og sosiale medier. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Svar p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/249:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne dokumenter, eksternt webinnhold og sosiale medier. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/250:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/251:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/252:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/253: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/254: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/255:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/256:
search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    )# ),
    # Tool(
    #     name="file_search",
    #     func=agent_executor.run,
    #     description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    # ),
]
8/257:
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/258:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/259:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/260:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/261:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/262:
from langchain.agents import load_tools
google_search = load_tools(["serpapi"])

tools = [
    Tool(
        name="Search",
        func=google_search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/263:
from langchain.agents import load_tools
tools = load_tools(["serpapi"])
print(tools)
8/264:

tools = [
    Tool(
        name="Search",
        func= tools,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/265:
from langchain.agents import load_tools
google_search = load_tools(["serpapi"])
8/266:

tools = [
    Tool(
        name="Search",
        func= google_search,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/267:

tools = [
    Tool(
        name="Search",
        func= google_search.search,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/268:
from langchain.agents import load_tools
google_search = load_tools(["serpapi"])
google_search("hva er datoen i dag")
8/269:
from langchain.agents import load_tools
google_search = load_tools(["serpapi"])
google_search.run("hva er datoen i dag")
8/270:
from langchain.agents import load_tools
google_search = load_tools(["serpapi"])
8/271:
from langchain.agents import load_tools
google_search = load_tools(["serpapi"])
8/272:
from langchain.agents import load_tools
google_search = load_tools(["serpapi"])
print(google_search)
8/273: search = SerpAPIWrapper()
8/274:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/275:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/276:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/277:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/278: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/279: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/280:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/281: search = SerpAPIWrapper()
8/282:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/283:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.REACT_DOCSTORE, verbose=True
)
8/284:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/285:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/286:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/287:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/288:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse ved  definere kategorier og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk, 
    samtidig som du angir suksessgraden i prosent for hver kategori. Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/289:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/290:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/291:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/292:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True
)
8/293:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/294:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/295:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/296:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/297: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/298: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/299:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/300: search = SerpAPIWrapper()
8/301:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/302:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True
)
8/303:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/304:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/305:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/306:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/307: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/308: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/309:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/310: search = SerpAPIWrapper()
8/311:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market and need to ask with search",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/312:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True
)
8/313:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/314:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/315:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/316:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/317:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/318:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/319: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/320: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/321:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/322: search = SerpAPIWrapper()
8/323:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market and need to ask with search",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/324:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/325:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
8/326:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/327:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/328:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/329:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/330:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/331:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/332:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/333:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/334:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/335:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/336:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/337:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/338:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/339:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/340: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/341: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/342:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/343: search = SerpAPIWrapper()
8/344:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/345:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/346:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/347:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/348:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/349:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/350:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/351:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/352:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)
PlayWrightBrowserToolkit.update_forward_refs()
# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/353:
from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit
from langchain.tools.playwright.utils import (
    create_async_playwright_browser,
    create_sync_playwright_browser, # A synchronous browser is available, though it isn't compatible with jupyter.
)

# This import is required only for jupyter notebooks, since they have their own eventloop
import nest_asyncio
nest_asyncio.apply()
8/354:
async_browser = create_async_playwright_browser()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
PlayWrightBrowserToolkit.update_forward_refs()
tools = browser_toolkit.get_tools()
8/355:
async_browser = create_async_playwright_browser()
PlayWrightBrowserToolkit.update_forward_refs()
browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)
tools = browser_toolkit.get_tools()
8/356:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
8/357:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
8/358:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
8/359: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
8/360: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
8/361:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
8/362: search = SerpAPIWrapper()
8/363:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market and need to ask with search",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
8/364:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
8/365:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
12/1:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
12/2:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
# agent_executor.run(
# "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
# )
14/1:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
14/2:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
14/3:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
14/4: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0)
14/5: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
14/6:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
14/7: search = SerpAPIWrapper()
14/8:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market and need to ask with search",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
14/9:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
14/10:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
14/11:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
14/12:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
14/13:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
14/14: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0.4)
14/15: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
14/16:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
14/17: search = SerpAPIWrapper()
14/18:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market and need to ask with search",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
14/19:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
14/20:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
14/21:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
14/22:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
14/23:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
14/24: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0.7)
14/25: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
14/26:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
14/27: search = SerpAPIWrapper()
14/28:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market and need to ask with search",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
14/29:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
14/30:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
12/3:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
12/4:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
# agent_executor.run(
# "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
# )
12/5:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
# agent_executor.run(
# "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
# )
12/6:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
12/7:


agent_executor.run(
    "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
)
# agent_executor.run(
# "You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
# )
12/8:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
12/9:


# agent_executor.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
# )
agent_executor.run(
"You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
)
12/10:
import os
import getpass
from langchain.llms import AzureOpenAI


os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

from langchain.chat_models import AzureChatOpenAI

from langchain.document_loaders import PyPDFLoader

from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo")
embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1) 


# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
12/11:


# agent_executor.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
# )
agent_executor.run(
"You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
)
12/12:


# agent_executor.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
# )
agent_executor.run(
"You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
)
12/13:


# agent_executor.run(
#     "Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du skal analysere {content internal} som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web {content web} og sosiale medier {content some}. Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. Du skal ogs definere kategorisert p hvordan selskapet kommuniserer, i hvilken grad de lykkes med dette i prosent. Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn."
# )
agent_executor.run(
"You are a savvy marketer with expertise in text analysis and company positioning. You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. You should also categorize how the company communicates, assess the degree of their success in percentage terms, and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. Write everything in norwegian."
)
14/31:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
14/32:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
14/33:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
14/34:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
14/35: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0.7)
14/36: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
14/37:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
14/38: search = SerpAPIWrapper()
14/39:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market and need to ask with search",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
14/40:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
14/41:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
15/1:
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import TextLoader
import os
from langchain.llms import AzureOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain import SerpAPIWrapper
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.router import MultiRetrievalQAChain
from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent
from typing import List, Tuple, Any, Union
from langchain.schema import AgentAction, AgentFinish

from langchain.agents.agent_toolkits import (
    create_vectorstore_router_agent,
    VectorStoreRouterToolkit,
    VectorStoreInfo,
)
15/2:
load_dotenv()
os.environ["SERPAPI_API_KEY"] = os.getenv(("SERPAPI_API_KEY"))
15/3:
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
15/4: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo",temperature=0.7)
15/5: embeddings = OpenAIEmbeddings(deployment = "embedding-ada",chunk_size=1)
15/6:

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
loader = PyPDFLoader("forte_data/forte_web.pdf")
forte_web_pages = loader.load_and_split()
db_forte_web = Chroma.from_documents(forte_web_pages, embeddings)

loader = PyPDFLoader("forte_data/posts.pdf")
linkedin_posts = loader.load_and_split()
db_linkedin= Chroma.from_documents(linkedin_posts, embeddings)

loader = PyPDFLoader("forte_data/Forte-Technology-Ecom-Strategy.pdf")
forte_strategy_pages = loader.load_and_split()
db_forte_strategy = Chroma.from_documents(forte_strategy_pages, embeddings)

forte_web_vectorstore = VectorStoreInfo(
    name="Forte web",
    description="General information from website of Forte",
    vectorstore=db_forte_web,
)

forte_linkedin_vectorstore = VectorStoreInfo(
    name="LinkedIn posts",
    description="Social media post generated by Forte Digital",
    vectorstore=db_linkedin,
)
forte_strategy_vectorstore = VectorStoreInfo(
    name="internal strategy",
    description="Internal strategy document",
    vectorstore=db_forte_strategy,
)


router_toolkit = VectorStoreRouterToolkit(
    vectorstores=[forte_linkedin_vectorstore, forte_web_vectorstore,forte_strategy_vectorstore], llm=llm
)
agent_executor = create_vectorstore_router_agent(
    llm=llm, toolkit=router_toolkit, verbose=True
)
15/7: search = SerpAPIWrapper()
15/8:

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about competitors and market and need to ask with search",
    ),
    Tool(
        name="file_search",
        func=agent_executor.run,
        description="gathers local files, useful for when you need to answer questions about Forte Digital ",
    ),
]
15/9:
mrkl = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
15/10:
# Do this so we can see exactly what's going on under the hood
import langchain
langchain.debug = True

# mrkl.run(
#     """Svar p norsk. Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
#      Du skal analysere som er interne dokumenter laget av et teknologiselskap, eksternt innhold fra web og sosiale medier. 
#      Du skal konkludere i et kort avsnitt om hvordan fremstr, hva de leverer og hvordan de fremstr. 
#      Vennligst gi en tydelig instruksjon for  vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
#      Hvilken anbefaling du ville gitt om de skal skille seg bedre til sine konkurrenter, basert p web sk. Disse konkurrentene skal nevnes med navn"""
#     )

mrkl.run(
    """Som en erfaren markedsfrer med spesialkompetanse innen tekstanalyse og selskapsposisjonering, 
    analyser teknologiselskapet Forte digitals interne strategi dokumenter {internal strategy}, eksternt webinnhold {forte web} og sosiale medier {linkedin posts}. 
    Formuler ogs en kort konklusjon om selskapets profil, tjenesteleveranse og generelle image. 
    Inkluder en anbefaling for hvordan de kan differensiere seg fra konkurrentene, og nevn disse konkurrentene ved navn basert p web-sk.
    Vurdere selskapets kommunikasjonsytelse og angi suksessgraden i prosent.
    Output svaret skal alltid vre p norsk."""
)

# mrkl.run(
# """You are a savvy marketer with expertise in text analysis and company positioning.
#  You are tasked with analyzing internal strategies dokuments, which consists of internal documents created by a technology company named Forte Digital, external web content , 
#  and social media content from linkedin. Your objective is to conclude in a brief paragraph how they appear, what they deliver, and how they come across. 
#  You should also categorize how the company communicates, assess the degree of their success in percentage terms, 
#  and provide recommendations on how they can differentiate themselves better from their competitors. These competitors should be mentioned by name. 
#  Write everything in norwegian."""
# )
16/1:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = set()
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.add(cleaned_url)
    
    return all_urls
16/2:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
for url in urls:
    print(url)
16/3:
df = pd.DataFrame({'URL': urls})
print(df)
16/4:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/5:
df = pd.DataFrame({'URL': urls})
print(df)
16/6:
for url in urls:
    print(url)
16/7:
df = pd.DataFrame(columns="url","analyse")
for url in urls:
    print(url)
16/8:
df = pd.DataFrame(columns=["url","analyse"])
for url in urls:
    print(url)
16/9:
df = pd.DataFrame(columns=["url","analyse"])
print(df)
16/10:
df = pd.DataFrame(columns=["url","analyse"])
for i in urls:
    df.append(urls=i,analyse="test")
print(df)
16/11:
df = pd.DataFrame(columns=["url","analyse"])
print(urls[0])
16/12:
df = pd.DataFrame(columns=["url","analyse"])
print(urls(0))
16/13:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    # Generate a response from GPT-3
    response = openai.Completion.create(
        engine="chat",  # Use the appropriate engine
        prompt=prompt,
        max_tokens=100,  # Adjust the max tokens as needed
        n = 1,  # The number of responses to generate
    )

    # Extract the GPT-3 generated analysis
    analysis = response.choices[0].text

    return analysis
16/14:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    # Generate a response from GPT-3
    response = openai.Completion.create(
        engine="chat",  # Use the appropriate engine
        prompt=prompt,
        max_tokens=100,  # Adjust the max tokens as needed
        n = 1,  # The number of responses to generate
    )

    # Extract the GPT-3 generated analysis
    analysis = response.choices[0].text

    return analysis
16/15:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.add(cleaned_url)
    
    return all_urls
16/16:
df = pd.DataFrame(columns=["url","analyse"])
get_all_urls()
16/17:
df = pd.DataFrame(columns=["url","analyse"])
get_all_urls("Sats.no")
16/18:
df = pd.DataFrame(columns=["url","analyse"])
get_all_urls("https://Sats.no")
16/19:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/20:
df = pd.DataFrame(columns=["url","analyse"])
get_all_urls("https://Sats.no")
16/21:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

analyze_url(urls[0])
16/22:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    # Generate a response from GPT-3
    response = openai.Completion.create(
        engine="gpt-35-turbo",  # Use the appropriate engine
        prompt=prompt,
        max_tokens=100,  # Adjust the max tokens as needed
        n = 1,  # The number of responses to generate
    )

    # Extract the GPT-3 generated analysis
    analysis = response.choices[0].text

    return analysis
16/23:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

analyze_url(urls[0])
16/24:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    # Generate a response from GPT-3
    response = openai.Completion.create(
        engine="chat",  # Use the appropriate engine
        prompt=prompt,
        max_tokens=100,  # Adjust the max tokens as needed
        n = 1,  # The number of responses to generate
    )

    # Extract the GPT-3 generated analysis
    analysis = response.choices[0].text

    return analysis
16/25:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

analyze_url(urls[0])
16/26:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    # Generate a response from GPT-3
    response = openai.Completion.create(
        engine="chat",  # Use the appropriate engine
        prompt=prompt,
        max_tokens=100,  # Adjust the max tokens as needed
        n = 1,  # The number of responses to generate
    )

    # Extract the GPT-3 generated analysis
    analysis = response.choices[0].text

    return analysis
16/27:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")



prompt = f"From the {urls}, generate a sentiment analysis of the language used and how the languange is percived."
16/28:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")



prompt = f"From the {urls[0]}, generate a sentiment analysis of the language used and how the languange is percived."
llm(prompt)
16/29:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/30:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/31:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/32:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
16/33: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/34:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    # Generate a response from GPT-3
    response = openai.Completion.create(
        engine="chat",  # Use the appropriate engine
        prompt=prompt,
        max_tokens=100,  # Adjust the max tokens as needed
        n = 1,  # The number of responses to generate
    )

    # Extract the GPT-3 generated analysis
    analysis = response.choices[0].text

    return analysis
16/35:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")



prompt = f"From the {urls[0]}, generate a sentiment analysis of the language used and how the languange is percived."
llm(prompt)
16/36:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = f"From the {urls[0]}, generate a sentiment analysis of the language used and how the languange is percived."
llm(prompt)
16/37:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
llm(prompt)
16/38:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
llm("prompt")
16/39:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    completion = openai.ChatCompletion.create(
        engine="gpt35turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    analysis = completion.choices[0].text

    return analysis
16/40:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url("prompt")
16/41:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url(urls[0])
16/42:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/43:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/44:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/45:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
16/46: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/47:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    completion = openai.ChatCompletion.create(
        engine="gpt35turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    analysis = completion.choices[0].text

    return analysis
16/48:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url(urls[0])
16/49:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/50:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/51:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/52:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/53:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/54: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/55:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    completion = openai.ChatCompletion.create(
        engine="gpt35turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    analysis = completion.choices[0].text

    return analysis
16/56:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url(urls[0])
16/57:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    analysis = completion.choices[0].text

    return analysis
16/58:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/59:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/60:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/61:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/62: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/63:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    analysis = completion.choices[0].text

    return analysis
16/64:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url(urls[0])
16/65:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/66:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/67:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/68:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/69: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/70:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived."

  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/71:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url(urls[0])
16/72:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/73:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/74:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/75:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/76: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/77:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived. The sentiment analysis should contain 4 words of how the language is percived and why they are included"

  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/78:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url(urls[0])
16/79:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/80:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/81:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/82:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/83: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/84:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived. The sentiment analysis should contain 4 words of how the language is percived and why they are included"

  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/85:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url(urls[0])
16/86:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/87:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/88:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/89:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/90: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/91:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt = f"From the {url}, generate a sentiment analysis of the language used and how the languange is percived. The sentiment analysis should contain 4 words of how the language is percived and why they are included"

    """You are a social scientist, Your task is to analyze the
sentiment of a single page on a website. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/92:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

prompt = "From the {}, generate a sentiment analysis of the language used and how the languange is percived.".format(urls[0])
analyze_url(urls[0])
16/93:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/94:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/95:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/96:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/97: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/98:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}  single page on a website. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/99:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

analyze_url(urls[0])
16/100:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/101:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/102:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/103:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/104: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/105:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/106:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

analyze_url(urls[0])
16/107:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/108:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/109:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/110:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/111: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/112:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/113:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

print(urls[0])

analyze_url(urls[0])
16/114:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/115:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/116:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/117:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/118: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/119:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/120:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
16/121: from langchain.document_loaders import UnstructuredURLLoader
16/122:
loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
16/123:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/124:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/125:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/126:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/127: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/128:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/129:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
16/130: from langchain.document_loaders import UnstructuredURLLoader
16/131:
loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
16/132: print(len(data))
16/133:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/134:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/135:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/136:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/137: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/138:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/139:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
16/140: from langchain.document_loaders import UnstructuredURLLoader
16/141:
loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
16/142:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/143:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/144:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/145:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/146: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/147:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/148:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
16/149: from langchain.document_loaders import UnstructuredURLLoader
16/150:
loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
16/151:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
16/152:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
16/153:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
16/154:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
16/155: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
16/156:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
16/157:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
16/158: from langchain.document_loaders import UnstructuredURLLoader
16/159:
loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
20/1:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/2:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/3:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/4:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/5: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/6:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/7:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/8: from langchain.document_loaders import UnstructuredURLLoader
20/9:
loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
20/10:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/11:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/12:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/13:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/14: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/15:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/16:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/17: from langchain.document_loaders import UnstructuredURLLoader
20/18:
loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
20/19: print(len(data))
20/20:
loader = UnstructuredURLLoader("sats.no")
data = loader.load()
20/21:
loader = UnstructuredURLLoader("https://sats.no")
data = loader.load()
20/22:
loader = UnstructuredURLLoader(["https://sats.no"])
data = loader.load()
20/23:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/24:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/25:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/26:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/27: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/28:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/29:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/30:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/31:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(urls=urls)
data = loader.load()
20/32:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/33: print(len(data))
20/34: print((data))
20/35:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/36:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/37:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/38:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/39: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/40:
def analyze_url(url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/41:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/42:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/43:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/44:
print((data))

analyze_url(data)
20/45:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/46:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/47:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/48:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/49: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/50:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this text: {text} from this url: {url}. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/51:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/52:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/53:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/54:
print((data))

analyze_url(data,"https://sats.no")
20/55:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/56:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/57:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/58:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/59: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/60:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this text: {text} from this url: {url}. Give me 4 words that decibe the sentiment in the text. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/61:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/62:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/63:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/64:
print((data))

analyze_url(data,"https://sats.no")
20/65:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this text: {text} from this url: {url}. Give me 4 words that decibe how the language is percived. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/66:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/67:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/68:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/69:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/70: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/71:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt =  f"""You are a social scientist, Your task is to analyze the
sentiment of this text: {text} from this url: {url}. Give me 4 words that decibe how the language is percived. In situations where the sentiment is difficult to
definitively analyse, write "no information" """
  

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/72:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/73:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/74:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/75:
print((data))

analyze_url(data,"https://sats.no")
20/76:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt =  f"""      
                You are a social scientist, Your task is to analyze the
                sentiment of this text: {text} from this url: {url}. Give me 4 words that decibe how the language is percived. In situations where the sentiment is difficult to
                definitively analyse, write "no information" 
                """
  
    """"""

    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/77:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt =  f"""      
                As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
                Your task is to identify and describe the prevailing sentiment using four descriptive words. 
                In cases where determining sentiment is challenging, please indicate "no information."
                """


    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/78:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/79:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/80:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/81:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/82: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/83:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt =  f"""      
                As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
                Your task is to identify and describe the prevailing sentiment using four descriptive words. 
                In cases where determining sentiment is challenging, please indicate "no information."
                """


    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/84:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/85:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/86:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/87:
print((data))

analyze_url(data,"https://sats.no")
20/88:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/89:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/90:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/91:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/92: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/93:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    prompt = f"""
    "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    Describe the sentiment using four words and provide a brief description for each word. 
    If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    Word 1  Word 2  Word 3  Word 4
    Description:    Description:    Description:    Description:



    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/94:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/95:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/96:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/97:
print((data))

analyze_url(data,"https://sats.no")
20/98:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/99:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/100:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/101:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/102: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/103:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    prompt = f"""
    "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    Describe the sentiment using four words and provide a brief description for each word. 
    If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    Word 1  Word 2  Word 3  Word 4
    Description:    Description:    Description:    Description:



    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/104:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/105:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/106:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/107:
print((data))

analyze_url(data,"https://sats.no")
20/108:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/109:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/110:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/111:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/112: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/113:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    prompt = f"""
    "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    Describe the sentiment using four words and provide a brief description for each word. 
    If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    Word 1  Word 2  Word 3  Word 4
    Description:    Description:    Description:    Description:



    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/114:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://Sats.no")

# print(urls[0])

# analyze_url(urls[0])
20/115:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/116:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/117:
print((data))

analyze_url(data,"https://sats.no")
20/118:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/119:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/120:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/121:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/122: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/123:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    prompt = f"""
    "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    Describe the sentiment using four words and provide a brief description for each word. 
    If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    Word 1  Word 2  Word 3  Word 4
    Description:    Description:    Description:    Description:



    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/124:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/125:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/126:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/127:
print((data))

analyze_url(data,"https://sats.no")
20/128:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/129:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/130:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/131:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/132: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/133:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    prompt = f"""
    "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    Describe the sentiment using four words and provide a brief description for each word. 
    If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    Word 1  Word 2  Word 3  Word 4
    Description:    Description:    Description:    Description:



    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/134:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/135:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/136:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/137:
print((data))

analyze_url(data,"https://sats.no")
20/138:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/139:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/140:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/141:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/142: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/143:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    prompt = f"""
    "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    Describe the sentiment using four words and provide a brief description for each word. 
    If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    Word 1  Word 2  Word 3  Word 4
    Description:    Description:    Description:    Description:



    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/144:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/145:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/146:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/147:
print((data))

analyze_url(data,"https://sats.no")
20/148:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/149:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/150:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/151:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/152: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/153:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    prompt = f"""
    "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    Describe the sentiment using four words and provide a brief description for each word. 
    If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    Word 1  Word 2  Word 3  Word 4
    Description:    Description:    Description:    Description:



    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/154:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/155:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/156:
# loader = UnstructuredURLLoader(["https://sats.no"])
loader = SeleniumURLLoader(["https://sats.no"])
data = loader.load()
20/157:
print((data))

analyze_url(data,"https://sats.no")
20/158:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/159:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/160:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/161:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/162: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/163:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    prompt = f"""
    "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    Describe the sentiment using four words and provide a brief description for each word. 
    If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    Word 1  Word 2  Word 3  Word 4
    Description:    Description:    Description:    Description:



    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/164:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/165:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/166:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/167:
print((data))

analyze_url(data,url)
20/168:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/169:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/170:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/171:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/172: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/173:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt =  f"""      
                As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
                Your task is to identify and describe the prevailing sentiment using four descriptive words. 
                In cases where determining sentiment is challenging, please indicate "no information."
                """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/174:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/175:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/176:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/177:
print((data))

analyze_url(data,url)
20/178:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/179:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/180:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/181:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/182: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/183:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}.
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/184:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/185:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/186:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/187:
print((data))

analyze_url(data,url)
20/188:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/189:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/190:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/191:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/192: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/193:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words. 
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/194:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/195:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/196:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/197:
print((data))

analyze_url(data,url)
20/198:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/199:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/200:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/201:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/202: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/203:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/204:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/205:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/206:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/207:
print((data))

analyze_url(data,url)
20/208:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/209:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/210:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/211:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/212: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/213:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/214:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/215:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/216:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/217:
print((data))

analyse_text_out = analyze_url(data,url) 
print(analyse_text_out)
20/218:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/219:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/220:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/221:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/222: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/223:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1,Description
            Word 2,Description
            Word 3,Description
            Word 4,Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/224:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/225:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/226:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/227:
# print((data))

analyse_text_out = analyze_url(data,url) 
print(analyse_text_out)
20/228:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/229:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/230:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/231:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/232: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/233:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/234:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/235:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/236:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/237:
# print((data))

analyse_text_out = analyze_url(data,url) 
print(analyse_text_out)
20/238:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/239:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/240:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/241:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/242: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/243:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/244:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/245:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/246:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/247:
# print((data))

analyse_text_out = analyze_url(data,url) 
print(analyse_text_out)
20/248:
sentiment_output = """
Positive: The document highlights the success and achievements of Forte Digital in various projects and partnerships, indicating a positive sentiment towards the company and its capabilities.

Innovative: The document emphasizes the use of new digital technology and automation in various industries, indicating an innovative sentiment towards the potential of these technologies.

Professional: The document presents Forte Digital as a highly competent and professional company, with a focus on data analysis and strategic planning.

Collaborative: The document mentions several partnerships and collaborations with other companies, indicating a collaborative sentiment towards working with others to achieve success.
"""

# Split the sentiment output into lines
sentiment_lines = analyse_text_out.strip().split('\n')

# Initialize an empty list to store sentiment data
sentiment_data = []

# Iterate through the lines and split them into words and descriptions
for line in sentiment_lines:
    parts = line.split(':')
    if len(parts) == 2:
        word = parts[0].strip()
        description = parts[1].strip()
        sentiment_data.append({"Word": word, "Description": description})

# Create a DataFrame from the sentiment data
df = pd.DataFrame(sentiment_data)

# Save the DataFrame to a CSV file
df.to_csv('sentiment_analysis.csv', index=False)
20/249:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/250:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/251:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/252:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/253: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/254:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/255:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/256:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/257:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/258:
# print((data))

analyse_text_out = analyze_url(data,url) 
print(analyse_text_out)
20/259:
sentiment_output = """
Positive: The document highlights the success and achievements of Forte Digital in various projects and partnerships, indicating a positive sentiment towards the company and its capabilities.

Innovative: The document emphasizes the use of new digital technology and automation in various industries, indicating an innovative sentiment towards the potential of these technologies.

Professional: The document presents Forte Digital as a highly competent and professional company, with a focus on data analysis and strategic planning.

Collaborative: The document mentions several partnerships and collaborations with other companies, indicating a collaborative sentiment towards working with others to achieve success.
"""

# Split the sentiment output into lines
sentiment_lines = analyse_text_out.strip().split('\n')

# Initialize an empty list to store sentiment data
sentiment_data = []

# Iterate through the lines and split them into words and descriptions
for line in sentiment_lines:
    parts = line.split(':')
    if len(parts) == 2:
        word = parts[0].strip()
        description = parts[1].strip()
        sentiment_data.append({"Word": word, "Description": description})

# Create a DataFrame from the sentiment data
df = pd.DataFrame(sentiment_data)

print(df)
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/260:
df = pd.DataFrame("Word", "Description")
print(df)

def add_to_pandas(df,analysed_text_out):
      df = pd.DataFrame(sentiment_data)
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
  

    print(df)
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/262:
df = pd.DataFrame("Word", "Description")
print(df)

def add_to_pandas(df,analysed_text_out):
    df = pd.DataFrame(sentiment_data)
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
  

    print(df)
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/263:
df = pd.DataFrame(columns= ["Word", "Description"])
print(df)

def add_to_pandas(df,analysed_text_out):
    df = pd.DataFrame(sentiment_data)
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
  

    print(df)
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/264:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/265:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/266:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/267:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/268: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/269:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/270:
def add_to_pandas(df,analysed_text_out):
    df = pd.DataFrame(sentiment_data)
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df.append(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/271:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/272:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/273:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/274:
# print((data))

analyse_text_out = analyze_url(data,url) 
print(analyse_text_out)

df = pd.DataFrame(columns= ["Word", "Description"])
df = add_to_pandas(df,analyse_text_out)
print(df)
20/275:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/276:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/277:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/278:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/279: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/280:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/281:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df.append(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/282:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/283:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/284:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/285:
# print((data))

analyse_text_out = analyze_url(data,url) 
print(analyse_text_out)

df = pd.DataFrame(columns= ["Word", "Description"])
df = add_to_pandas(df,analyse_text_out)
print(df)
20/286:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/287:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/288:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/289:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/290: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/291:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/292:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df = pd.DataFrame.from_dict(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/293:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/294:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/295:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/296:
# print((data))

analyse_text_out = analyze_url(data,url) 
print(analyse_text_out)

df = pd.DataFrame(columns= ["Word", "Description"])
df = add_to_pandas(df,analyse_text_out)
print(df)
20/297: for url in urls:
20/298:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/299:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/300:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/301:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/302: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/303:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/304:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df = pd.DataFrame.from_dict(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/305:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/306:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/307:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/308:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/309: df = pd.DataFrame(columns= ["Word", "Description"])
20/310:

for url in urls:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/311:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/312:


# Replace 'https://example.com' with the URL of the website you want to scrape
base_url = "https://Sats.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
20/313:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/314:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/315: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/316:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/317:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df = pd.DataFrame.from_dict(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/318:
df = pd.DataFrame(columns=["url","analyse"])
urls = get_all_urls("https://fortedigital.no")

# print(urls[0])

# analyze_url(urls[0])
20/319:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/320:
# loader = UnstructuredURLLoader(["https://sats.no"])
url = "https://fortedigital.no"
loader = SeleniumURLLoader([url])
data = loader.load()
20/321:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/322: df = pd.DataFrame(columns= ["Word", "Description"])
20/323:

for url in urls:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/324:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/325:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/326:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/327: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/328:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/329:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df = pd.DataFrame.from_dict(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/330:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/331:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/332:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/333: df = pd.DataFrame(columns= ["Word", "Description"])
20/334:

for url in urls:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/335:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/336:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/337:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/338: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/339:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/340:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df = pd.DataFrame.from_dict(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/341:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/342:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/343:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/344: df = pd.DataFrame(columns= ["Word", "Description"])
20/345:

for url in urls[:,10]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/346:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/347:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/348:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/349: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/350:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/351:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df = pd.DataFrame.from_dict(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/352:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/353:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/354:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/355: df = pd.DataFrame(columns= ["Word", "Description"])
20/356:

for url in urls[:10]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/357: print(df)
20/358:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/359:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/360:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/361: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/362:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/363:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df = pd.DataFrame.from_dict(sentiment_data)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/364:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/365:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/366:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/367: df = pd.DataFrame(columns= ["Word", "Description"])
20/368:

for url in urls[:1]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/369: print(df)
20/370:

for url in urls[:1]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/371:

for url in urls[:1]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/372: print(df)
20/373: print(df)
20/374: print(df)
20/375:

for url in urls[:1]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/376: print(df)
20/377:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/378:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/379:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/380: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/381:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/382:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    df = pd.DataFrame.from_dict(sentiment_data)
    df = pd.DataFrame(sentiment_data).append(df, sort=False)
    

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/383:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/384:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/385:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/386: df = pd.DataFrame(columns= ["Word", "Description"])
20/387:

for url in urls[:1]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/388:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/389:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/390:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/391: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/392:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/393:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    # df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    

    return pd.concat(sentiment_data)
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/394:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/395:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/396:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/397: df = pd.DataFrame(columns= ["Word", "Description"])
20/398:

for url in urls[:]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/399:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/400:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/401:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/402: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/403:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/404:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    # df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    

    return pd.concat(sentiment_data)
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/405:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/406:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/407:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/408: df = pd.DataFrame(columns= ["Word", "Description"])
20/409:

for url in urls[:2]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/410:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/411:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/412:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/413: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/414:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/415:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = df.concat(new_df)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/416:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/417:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/418:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/419: df = pd.DataFrame(columns= ["Word", "Description"])
20/420:

for url in urls[:2]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/421:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/422:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/423:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/424: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/425:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/426:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat(df, new_df)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/427:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/428:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/429:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/430: df = pd.DataFrame(columns= ["Word", "Description"])
20/431:

for url in urls[:2]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/432:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/433:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/434:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/435: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/436:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/437:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    df = pd.concat(new_df)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/438:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/439:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/440:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/441: df = pd.DataFrame(columns= ["Word", "Description"])
20/442:

for url in urls[:2]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/443:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/444:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/445:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/446: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/447:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/448:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    new_df = pd.concat(df)

    return df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/449:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/450:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/451:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/452: df = pd.DataFrame(columns= ["Word", "Description"])
20/453:

for url in urls[:2]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/454:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/455:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/456:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/457: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/458:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/459:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df])

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/460:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/461:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/462:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/463: df = pd.DataFrame(columns= ["Word", "Description"])
20/464:

for url in urls[:2]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/465: print(df)
20/466:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/467:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/468:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/469: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/470:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/471:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/472:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/473:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/474:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/475: df = pd.DataFrame(columns= ["Word", "Description"])
20/476:

for url in urls[:2]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/477: print(df)
20/478:

for url in urls[:10]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/479: print(df)
20/480: print(df)
20/481: print(df)
20/482: print(df)
20/483: print(df)
20/484: print(df)
20/485:
word_counts = df['Word'].value_counts()

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'index': 'Word', 'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=False)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/486:
word_counts = df['Word'].value_counts()

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'index': 'Word', 'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/487:
word_counts = df['Word'].value_counts()
# print(word_counts)

# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'index': 'Word', 'Word': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
20/488:
word_counts = df['Word'].value_counts()
print(word_counts)

# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'index': 'Word', 'Word': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
20/489:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'index': 'Word', 'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/490:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/491:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/492:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/493:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/494:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/495:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/496:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='Occurrences', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/497:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/498:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/499:
word_counts = df['Word'].value_counts()
print(word_counts)

# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/500:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/501:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/502:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/503: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/504:
def analyze_url(text,url):

    # Create a prompt for GPT-3
    prompt = f"""
             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
             Please format your response in CSV format as follows:

            Word 1:Description
            Word 2:Description
            Word 3:Description
            Word 4:Description"
            """
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/505:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/506:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/507:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/508:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/509: df = pd.DataFrame(columns= ["Word", "Description"])
20/510:

for url in urls[:10]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/511: print(df)
20/512:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/513:
df.to_csv()
print(df)
20/514:
df.to_csv("sentiment_words")
print(df)
20/515:
df.to_csv("sentiment_words.csv")
print(df)
20/516: df = pd.read_csv("sentiment_words.csv")
20/517:
df.to_csv("sentiment_words.csv")
print(df)
20/518:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/519:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/520:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/521: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/522:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    
    prompt = f"""
             Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord.  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse
            Ord 2:Beskrivelse
            Ord 3:Beskrivelse
            Ord 4:Beskrivelse
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/523:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/524:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/525:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/526:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/527: df = pd.read_csv("sentiment_words.csv")
20/528:

for url in urls[:10]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/529:
df.to_csv("sentiment_words.csv")
print(df)
20/530:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/531:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/532:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/533:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/534: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/535:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    
    prompt = f"""
             Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord.  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse
            Ord 2:Beskrivelse
            Ord 3:Beskrivelse
            Ord 4:Beskrivelse
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/536:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/537:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/538:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/539:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/540: df = pd.read_csv("sentiment_words.csv")
20/541:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url) 
    df = add_to_pandas(df,analyse_text_out)
20/542:
df.to_csv("sentiment_words.csv")
print(df)
20/543:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/544:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/545:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/546:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/547: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/548:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    
    prompt = f"""
             Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord.  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse
            Ord 2:Beskrivelse
            Ord 3:Beskrivelse
            Ord 4:Beskrivelse
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/549:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/550:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/551:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/552:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/553: df = pd.read_csv("sentiment_words.csv")
20/554:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/555:
df.to_csv("sentiment_words.csv")
print(df)
20/556:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/557:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/558:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/559:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/560: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/561:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    
    prompt = f"""
             Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord.  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/562:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Word": word, "Description": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/563:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/564:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/565:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/566: df = pd.read_csv("sentiment_words.csv")
20/567:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/568:
df.to_csv("sentiment_words.csv")
print(df)
20/569:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/570:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/571:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/572:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/573: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/574:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    
    prompt = f"""
             Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord.  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/575:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/576:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/577:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/578:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/579: df = pd.read_csv("sentiment_words.csv")
20/580:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/581:
df.to_csv("sentiment_words.csv")
print(df)
20/582:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/583:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/584:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/585:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/586: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/587:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    
    prompt = f"""
             Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord.  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/588:
# def add_to_pandas(df,analysed_text_out):
#           # Split the sentiment output into lines
#     sentiment_lines = analysed_text_out.strip().split('\n')

#     # Initialize an empty list to store sentiment data
#     sentiment_data = []

#     # Iterate through the lines and split them into words and descriptions
#     for line in sentiment_lines:
#         parts = line.split(':')
#         if len(parts) == 2:
#             word = parts[0].strip()
#             description = parts[1].strip()
#             sentiment_data.append({"Ord": word, "Beskrivelse": description})

#     # Create a DataFrame from the sentiment data
#     new_df = pd.DataFrame.from_dict(sentiment_data)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)

#     return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/589:
import re
def add_to_pandas(df,analysed_text_out):
    pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

    # Use re.findall to extract matches
    matches = re.findall(pattern, analysed_text_out, re.DOTALL)

    # Create a list of dictionaries to store the extracted data
    data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

    # Create a pandas DataFrame
    new_df = pd.DataFrame.from_dict(data_list)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
20/590:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/591:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/592:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/593: df = pd.read_csv("sentiment_words.csv")
20/594:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/595:
df.to_csv("sentiment_words.csv")
print(df)
20/596:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/597:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/598:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/599:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/600: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/601:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    
    prompt = f"""
             Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord.  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/602:
# def add_to_pandas(df,analysed_text_out):
#           # Split the sentiment output into lines
#     sentiment_lines = analysed_text_out.strip().split('\n')

#     # Initialize an empty list to store sentiment data
#     sentiment_data = []

#     # Iterate through the lines and split them into words and descriptions
#     for line in sentiment_lines:
#         parts = line.split(':')
#         if len(parts) == 2:
#             word = parts[0].strip()
#             description = parts[1].strip()
#             sentiment_data.append({"Ord": word, "Beskrivelse": description})

#     # Create a DataFrame from the sentiment data
#     new_df = pd.DataFrame.from_dict(sentiment_data)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)

#     return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/603:
import re
def add_to_pandas(df,analysed_text_out):
    pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

    # Use re.findall to extract matches
    matches = re.findall(pattern, analysed_text_out, re.DOTALL)

    # Create a list of dictionaries to store the extracted data
    data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

    # Create a pandas DataFrame
    new_df = pd.DataFrame.from_dict(data_list)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
20/604:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/605:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/606:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/607: df = pd.read_csv("sentiment_words.csv")
20/608:

for url in urls[:10]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/609:
df.to_csv("sentiment_words.csv")
print(df)
20/610:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/611:

for url in urls[:20]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/612:
df.to_csv("sentiment_words.csv")
print(df)
20/613:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/614:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/615:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/616:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/617: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/618:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Informerende
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/619:
# def add_to_pandas(df,analysed_text_out):
#           # Split the sentiment output into lines
#     sentiment_lines = analysed_text_out.strip().split('\n')

#     # Initialize an empty list to store sentiment data
#     sentiment_data = []

#     # Iterate through the lines and split them into words and descriptions
#     for line in sentiment_lines:
#         parts = line.split(':')
#         if len(parts) == 2:
#             word = parts[0].strip()
#             description = parts[1].strip()
#             sentiment_data.append({"Ord": word, "Beskrivelse": description})

#     # Create a DataFrame from the sentiment data
#     new_df = pd.DataFrame.from_dict(sentiment_data)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)

#     return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/620:
import re
def add_to_pandas(df,analysed_text_out):
    pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

    # Use re.findall to extract matches
    matches = re.findall(pattern, analysed_text_out, re.DOTALL)

    # Create a list of dictionaries to store the extracted data
    data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

    # Create a pandas DataFrame
    new_df = pd.DataFrame.from_dict(data_list)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
20/621:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/622:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/623:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/624: df = pd.read_csv("sentiment_words.csv")
20/625:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/626:
df.to_csv("sentiment_words.csv")
print(df)
20/627:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/628:
df.to_csv("sentiment_words.csv")
print(df)
20/629:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/630:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/631:
df.to_csv("sentiment_words.csv")
print(df)
20/632:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/633:
df.to_csv("sentiment_words.csv")
print(df)
20/634:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/635:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/636:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/637: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/638:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/639:
# def add_to_pandas(df,analysed_text_out):
#           # Split the sentiment output into lines
#     sentiment_lines = analysed_text_out.strip().split('\n')

#     # Initialize an empty list to store sentiment data
#     sentiment_data = []

#     # Iterate through the lines and split them into words and descriptions
#     for line in sentiment_lines:
#         parts = line.split(':')
#         if len(parts) == 2:
#             word = parts[0].strip()
#             description = parts[1].strip()
#             sentiment_data.append({"Ord": word, "Beskrivelse": description})

#     # Create a DataFrame from the sentiment data
#     new_df = pd.DataFrame.from_dict(sentiment_data)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)

#     return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/640:
import re
def add_to_pandas(df,analysed_text_out):
    pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

    # Use re.findall to extract matches
    matches = re.findall(pattern, analysed_text_out, re.DOTALL)

    # Create a list of dictionaries to store the extracted data
    data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

    # Create a pandas DataFrame
    new_df = pd.DataFrame(data_list)
    # new_df = pd.DataFrame.from_dict(data_list)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
20/641:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/642:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/643:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/644: df = pd.read_csv("sentiment_words.csv")
20/645:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/646:
df.to_csv("sentiment_words.csv")
print(df)
20/647:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/648:
df.to_csv("sentiment_words.csv")
print(df)
20/649:
df.to_csv("sentiment_words.csv")
print(df)
20/650:
df.to_csv("sentiment_words.csv")
print(df)
20/651:
df.to_csv("sentiment_words.csv")
print(df)
20/652:
df.to_csv("sentiment_words.csv")
print(df)
20/653:
df.to_csv("sentiment_words.csv")
print(df)
20/654:
df.to_csv("sentiment_words.csv")
print(df)
20/655:
df.to_csv("sentiment_words.csv")
print(df)
20/656:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/657:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
    print(df)
20/658:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/659:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/660:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/661: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/662:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/663:
# def add_to_pandas(df,analysed_text_out):
#           # Split the sentiment output into lines
#     sentiment_lines = analysed_text_out.strip().split('\n')

#     # Initialize an empty list to store sentiment data
#     sentiment_data = []

#     # Iterate through the lines and split them into words and descriptions
#     for line in sentiment_lines:
#         parts = line.split(':')
#         if len(parts) == 2:
#             word = parts[0].strip()
#             description = parts[1].strip()
#             sentiment_data.append({"Ord": word, "Beskrivelse": description})

#     # Create a DataFrame from the sentiment data
#     new_df = pd.DataFrame.from_dict(sentiment_data)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)

#     return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/664:
import re
def add_to_pandas(df,analysed_text_out):
    pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

    # Use re.findall to extract matches
    matches = re.findall(pattern, analysed_text_out, re.DOTALL)

    # Create a list of dictionaries to store the extracted data
    data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

    # Create a pandas DataFrame
    new_df = pd.DataFrame(data_list)
    # new_df = pd.DataFrame.from_dict(data_list)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
20/665:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/666:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/667:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/668: df = pd.read_csv("sentiment_words.csv")
20/669:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
    print(df)
20/670:
df.to_csv("sentiment_words.csv")
print(df)
20/671:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/672:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/673:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/674:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/675: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/676:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/677:
# def add_to_pandas(df,analysed_text_out):
#           # Split the sentiment output into lines
#     sentiment_lines = analysed_text_out.strip().split('\n')

#     # Initialize an empty list to store sentiment data
#     sentiment_data = []

#     # Iterate through the lines and split them into words and descriptions
#     for line in sentiment_lines:
#         parts = line.split(':')
#         if len(parts) == 2:
#             word = parts[0].strip()
#             description = parts[1].strip()
#             sentiment_data.append({"Ord": word, "Beskrivelse": description})

#     # Create a DataFrame from the sentiment data
#     new_df = pd.DataFrame.from_dict(sentiment_data)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)

#     return final_df
# Save the DataFrame to a CSV file
# df.to_csv('sentiment_analysis.csv', index=False)
20/678:
import re
def add_to_pandas(df,analysed_text_out):
    pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

    # Use re.findall to extract matches
    matches = re.findall(pattern, analysed_text_out, re.DOTALL)

    # Create a list of dictionaries to store the extracted data
    data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

    # Create a pandas DataFrame
    new_df = pd.DataFrame(data_list)
    print(new_df)
    # new_df = pd.DataFrame.from_dict(data_list)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)
    print(final_df)
    return final_df
20/679:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/680:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/681:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/682: df = pd.read_csv("sentiment_words.csv")
20/683:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/684:
df.to_csv("sentiment_words.csv")
print(df)
20/685:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/686:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/687:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/688:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/689:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/690: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/691:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/692:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
# Save the DataFrame to a CSV file
df.to_csv('sentiment_analysis.csv', index=False)
20/693:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
20/694:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/695:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/696:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/697: df = pd.read_csv("sentiment_words.csv")
20/698:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/699:
df.to_csv("sentiment_words.csv")
print(df)
20/700:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/701:
word_counts = df['Word'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Word': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/702:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/703:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/704:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/705: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/706:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/707:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
20/708:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
20/709:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/710:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/711:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/712: df = pd.read_csv("sentiment_words.csv")
20/713:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/714:
df.to_csv("sentiment_words.csv")
print(df)
20/715:
word_counts = df['Ord'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/716:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/717:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/718:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/719: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/720:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/721:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
20/722:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
20/723:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/724:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/725:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/726: df = pd.read_csv("sentiment_words.csv",index_col="index")
20/727: df = pd.read_csv("sentiment_words.csv",index_col="index")
20/728: df = pd.read_csv("sentiment_words.csv",index="index")
20/729: df = pd.read_csv("sentiment_words.csv")
20/730:
df.to_csv("sentiment_words.csv")
print(df)
20/731:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
20/732:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
20/733:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
20/734: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
20/735:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
20/736:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
20/737:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
20/738:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
20/739:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
20/740:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
20/741: df = pd.read_csv("sentiment_words.csv")
20/742:

for url in urls[:25]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
20/743:
df.to_csv("sentiment_words.csv")
print(df)
20/744:
word_counts = df['Ord'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/745:
df.to_csv("sentiment_words.csv")
print(df)
20/746:
word_counts = df['Ord'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/747:
word_counts = df['Ord'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
20/748:
df.to_csv("sentiment_words.csv")
print(df)
20/749:
df.to_csv("sentiment_words.csv")
print(df)
20/750: new_df = df.groupby('Ord')
20/751:
new_df = df.groupby('Ord')
print(new_df)
20/752:
new_df = df.groupby('Ord')
print(new_df.describe())
20/753:
df.to_csv("sentiment_words.csv")
print(df)
20/754:
df.to_csv("sentiment_words.csv")
print(df)
20/755:
new_df = df.groupby('Ord')
print(new_df.describe())
20/756:
new_df = df.groupby('Ord')
print(new_df.describe())
20/757:
new_df = df.groupby('Ord')
print(new_df.describe())
20/758:
# new_df = df.groupby('Ord')
# print(new_df.describe())
df.groupby('Ord').apply(print)
20/759:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

for key, value in df.groupby('Ord'):
    print(key, value)
20/760:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

for key, value in df.groupby('Ord'):
    print(key, value)
21/1:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
21/2:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
21/3:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
21/4:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
21/5: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
21/6:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
21/7:
def add_to_pandas(df,analysed_text_out):
          # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
21/8:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
21/9:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
21/10:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
21/11:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
21/12: df = pd.read_csv("sentiment_words.csv")
21/13:

for url in urls[:25]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
21/14:
df.to_csv("sentiment_words.csv")
print(df)
21/15:
word_counts = df['Ord'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
21/16:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
print(df.groupby('Ord').count())
21/17:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

print(df.groupby('Ord').count())
21/18:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 

print(df.groupby('Ord').count())
21/19:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 

print(df.groupby('Ord').count())
21/20:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 

print(df.groupby('Ord').count())
21/21:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# sort the df by occurrences (highest to lowest)
df = df.sort_values(by='Ord', ascending=False,ignore_index=True)

print(df.groupby('Ord').count())
21/22:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# sort the df by occurrences (highest to lowest)
df = df.sort_values(by='Ord', ascending=False,ignore_index=True)

print(df.groupby('Ord').count())
21/23:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# sort the df by occurrences (highest to lowest)
df = df.sort_values(by='Beskrivelse', ascending=False,ignore_index=True)

print(df.groupby('Ord').count())
21/24:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# sort the df by occurrences in beskrivelse (highest to lowest)
df = df.sort_values(by='Beskrivelse', ascending=False,ignore_index=True)

print(df.groupby('Ord').count())
21/25:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# sort the df by occurrences in beskrivelse (highest to lowest)
df.sort_values(by='Beskrivelse', ascending=False,ignore_index=True)

print(df.groupby('Ord').count())
21/26:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# sort the df by occurrences in count column in descending order
df.sort_values(by=['count'], inplace=True, ascending=False)
print(df.groupby('Ord').count())
21/27:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# sort the df by occurrences in count column in descending order
df.sort_values(by=['Ord'], inplace=True, ascending=False)
print(df.groupby('Ord').count())
21/28:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False)


# print(df.groupby('Ord').count())
21/29:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False)


# print(df.groupby('Ord').count())
21/30:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False))


# print(df.groupby('Ord').count())
21/31:
word_counts = df['Ord'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
21/32:
word_counts = df['Ord'].value_counts()
# Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
sorted_words = word_counts.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
21/33:
word_counts = df['Ord'].value_counts()
# Sort words based on occurrences (highest to lowest)
sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

print("Words sorted by highest occurrences:")
print(sorted_words)
21/34:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()
21/35:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

print(word_counts)
21/36:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()
print(df)

print(word_counts)
21/37:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
print(df)
21/38:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

# print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
print(df)
21/39:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

# print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
# pretty print the summary
print(df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index())
21/40:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

# print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
# pretty print the summary and sort the words by highest occurrences
print(df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index())
21/41:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

# print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
# pretty print the summary and sort the words by highest occurrences
print(df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index().sort_values(by='Ord', ascending=False,ignore_index=True))
21/42:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

# print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
# pretty print the whole table and sort the words by highest occurrences
print(df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index().sort_values(by='Ord', ascending=False,ignore_index=True))
21/43:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

# print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
# pretty print the whole table and sort the words by highest occurrences
print(df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index().sort_values(by='Ord', ascending=False,ignore_index=True))

df.to_csv("sentiment_words_group.csv")
22/1:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count
print df.groupby('ord').count().sort_values(['count'],ascending=False)

# print df by group ord and count and sort by count


# print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False))


# print(df.groupby('Ord').count())
22/2:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count



# Group the data by the "Ord" column and count the number of rows in each group
df.groupby('Ord').count()

# Sort the result by the "Beskrivelse" column, in descending order
df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False)

# print(df.groupby('Ord').count())
22/3:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count



def my_function(df, column='Ord'):
    try:
        return df.groupby(column).count().sort_values(by='Beskrivelse', ascending=False)
    except:
        return None


# print(df.groupby('Ord').count())
22/4:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/5:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/6:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/7: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/8:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/9:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/10:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/11:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/12:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/13:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
22/14: df = pd.read_csv("sentiment_words.csv")
22/15:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/16:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/17:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/18:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/19: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/20:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/21:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/22:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/23:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/24:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/25:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
22/26: df = pd.read_csv("sentiment_words.csv")
22/27:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/28:
df.to_csv("sentiment_words.csv")
print(df)
22/29:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

# print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
# pretty print the whole table and sort the words by highest occurrences
print(df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index().sort_values(by='Ord', ascending=False,ignore_index=True))

df.to_csv("sentiment_words_group.csv")
22/30:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count


print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False))


# print(df.groupby('Ord').count())
22/31:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/32:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/33:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/34: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/35:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/36:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/37:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/38:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/39:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/40:
# print((data))

# analyse_text_out = analyze_url(data,url) 
# print(analyse_text_out)

# df = pd.DataFrame(columns= ["Word", "Description"])
# df = add_to_pandas(df,analyse_text_out)
# print(df)
22/41: df = pd.read_csv("sentiment_words.csv")
22/42:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/43:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
print(df)
22/44:
# word_counts = df['Ord'].value_counts()
# # Sort words based on occurrences (highest to lowest)
# sorted_words = word_counts.reset_index().rename(columns={'Ord': 'Occurrences'})
# sorted_words = sorted_words.sort_values(by='count', ascending=False,ignore_index=True)

# print("Words sorted by highest occurrences:")
# print(sorted_words)
# group and count the words
word_counts = df['Ord'].value_counts()

# print(word_counts)

# for the same words, make a summary of all the descriptions in the "beskrivelse" column
df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index()
# pretty print the whole table and sort the words by highest occurrences
print(df.groupby('Ord')['Beskrivelse'].apply(lambda x: ','.join(x)).reset_index().sort_values(by='Ord', ascending=False,ignore_index=True))

df.to_csv("sentiment_words_group.csv")
22/45:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count


print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False))


# print(df.groupby('Ord').count())
22/46:
# new_df = df.groupby('Ord')
# print(new_df.describe())
# df.groupby('Ord').apply(print)

# for key, value in df.groupby('Ord'):
#     print(key, value)

# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

# print df by group ord and count and sort by count


print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False))


# print(df.groupby('Ord').count())
22/47:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False))
22/48:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# drop all the unnamed columns 
df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False))
22/49:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))
22/50:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))
22/51:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Beskrivelse', ascending=False))
22/52:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))
22/53:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with description column only
print(df['Beskrivelse'])
22/54:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
print(df.groupby('Ord')['Beskrivelse'].apply(list))
22/55:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
print(df.groupby('Ord')['Beskrivelse'].apply(list))

# save the df to a csv file
df.to_csv("sentiment_words_test.csv")
22/56:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
print(df.groupby('Ord')['Beskrivelse'].apply(list))

# save the df grouped description column only to a csv file
df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/57:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
print(df.groupby('Ord')['Beskrivelse'].apply(list))

# save the df grouped description column only to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/58: #use chat model to generate a summary of each of the descriptions in the grouped df
22/59:
# Use the llm to generate a summary of the description column in the grouped df
summary = llm.summarize(df.groupby('Ord')['Beskrivelse'].apply(list))
print(summary)
22/60:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column
completion = openai.ChatCompletion.create(
    engine="chat",
22/61: # Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
22/62:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    summary = summary_of_description(description)
    print(summary)
    df.loc[index, 'Summary'] = summary
22/63:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in grouped.iterrows():
    description = row['Beskrivelse']
    summary = summary_of_description(description)
    print(summary)
    df.loc[index, 'Summary'] = summary
22/64:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    summary = summary_of_description(description)
    print(summary)
    df.loc[index, 'Summary'] = summary
22/65:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    print(description)
    summary = summary_of_description(description)
    print(summary)
    df.loc[index, 'Summary'] = summary
22/66:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    print(description.type)
    summary = summary_of_description(description)
    print(summary)
    df.loc[index, 'Summary'] = summary
22/67:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    print(type(description))
    summary = summary_of_description(description)
    print(summary)
    df.loc[index, 'Summary'] = summary
22/68:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    print(type(list(description)))
    summary = summary_of_description(description)
    print(summary)
    df.loc[index, 'Summary'] = summary
22/69:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    print(type(list(description)))
    summary = summary_of_description(list(description))
    print(summary)
    df.loc[index, 'Summary'] = summary
22/70:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    print(type(list(description)))
    summary = summary_of_description(int(description))
    print(summary)
    df.loc[index, 'Summary'] = summary
22/71:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    print(type(list(description)))
    summary = summary_of_description()
    print(summary)
    df.loc[index, 'Summary'] = summary
22/72:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    print(type(list(description)))
    summary = summary_of_description(description)
    print(summary)
    df.loc[index, 'Summary'] = summary
22/73:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")
for index, row in df.iterrows():
    description = row['Beskrivelse']
    summary = summary_of_description(description)
    df.loc[index, 'Summary'] = summary
22/74:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")

for index, row in df.iterrows():
    description = row['Beskrivelse']
    summary = summary_of_description(description)
    print("Summary of description: " + description + " is " + summary)
    df.loc[index, 'Summary'] = summary
22/75:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column
df = pd.read_csv("sentiment_words_test.csv")

for index, row in df.iterrows():
    description = row['Beskrivelse']
    print("Processing description: " + description)
    summary = summary_of_description(description)
    print("Summary of description: " + description + " is " + summary)
    df.loc[index, 'Summary'] = summary
22/76:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column

print(grouped)
22/77:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
print(df.groupby('Ord')['Beskrivelse'].apply(list))

# save the df grouped description column only to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/78:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column

print(grouped)
22/79:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print grouped
print(df.groupby('Ord')['Beskrivelse'].apply(list))
22/80:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file
22/81:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)
22/82:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)
22/83:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets in the description column
df = df.str[1:-1]
print(df)
22/84:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)
22/85:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)
22/86:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)
22/87:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

#access the list of descriptions in the description column
df = df.str.split(',')
print(df)
22/88:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)
22/89:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)
22/90:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)


print(type(df))
22/91:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to string
df = df.astype(str)
22/92:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to string
df = df.astype(str)
print(df)
22/93:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# loop through the list in the description column
for description in df:
    print(description)
22/94:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split(',')
print(df)
22/95:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split(',')
print(df)

# loop through the description column 
for description in df:
    print(description)
22/96:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split(',')
# print(df)

# loop through the description column 
for description in df:
    print(description)
22/97:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split(',')
print(df)
22/98:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split(',')
print(df)

print(df[0])
22/99:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split('.')
print(df)

print(df[0])
22/100:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split('.')
print(df)

print(df[1])
22/101:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split('.')
print(df)

print(df[2])
22/102:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split('.')
print(df)

print(df[0])
22/103:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split('.')
print(df)

#clean the list of the description column
df = [x for x in df if x]
print(df)
22/104:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split('.')
print(df)

#clean the list of the description column by removing empty strings 
df = list(filter(None, df))
print(df)
22/105:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split('.')
print(df)

#clean the list of the description column by removing empty strings 
df = list(filter(None, df))
print(df)
22/106:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split('','')
print(df)

#clean the list of the description column by removing empty strings 
df = list(filter(None, df))
print(df)
22/107:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split("', '")
print(df)

#clean the list of the description column by removing empty strings 
df = list(filter(None, df))
print(df)
22/108:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# only access the description column
df = df['Beskrivelse']
print(df)

#remove the outer list brackets and the "" in the description column
df = df.str[1:-1]
print(df)

#remove the "" in the description column
df = df.str.replace('"', '')
print(df)

# access the first row of the description column
df = df.iloc[0]
print(df)

# change the type of the description column to list
df = df.split("', '")
print(df)

for description in df:
    print(description)
22/109:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse']})
    
print(sentiment_data)



# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/110:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse']})
    
print(sentiment_data)

for description in sentiment_data:
    print(description)

# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/111:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse']})
    
# print(sentiment_data)

for description in sentiment_data:
    print(description)

# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/112:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse']})
    
# print(sentiment_data)

for description in sentiment_data:
    print(description)

# access the values of the sentiment_data list
for description in sentiment_data:
    print(description.values())
    

# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/113:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse']})
    
# print(sentiment_data)

for description in sentiment_data:
    print(description)

# access the values of the sentiment_data list
for description in sentiment_data:
    print(type(description.values()))


# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/114:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by '. '
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('. ')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description)




# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/115:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split(', /')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description)




# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/116:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split(', /')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description)




# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/117:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split(', ')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description)




# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/118:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description)




# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/119:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description.values())




# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/120:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description.values())

print(sentiment_data[0]['Beskrivelse'][0])


# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/121:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description.values())

print(sentiment_data[0]['Beskrivelse'][0][0])


# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/122:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description.values())

print(sentiment_data[0]['Beskrivelse'][0][0])


# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/123:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(description):
    prompt = f"""
    Write a summary of the list of descriptions of why a word is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# print(sentiment_data)

for description in sentiment_data:
    print(description.values())

print(sentiment_data[0]['Beskrivelse'][0])


# # only access the description column
# df = df['Beskrivelse']
# print(df)

# #remove the outer list brackets and the "" in the description column
# df = df.str[1:-1]
# print(df)

# #remove the "" in the description column
# df = df.str.replace('"', '')
# print(df)

# # access the first row of the description column
# df = df.iloc[0]
# print(df)

# # change the type of the description column to list
# df = df.split("', '")
# print(df)

# for description in df:
#     print(description)
22/124:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(word, description):
    prompt = f"""
    Write a summary of the list of descriptions: {description} of why the word: {word} is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# for each row in the sentiment_data list, loop through the list of descriptions and generate a summary of the description 
for row in sentiment_data:
    for description in row['Beskrivelse']:
        summary = summary_of_description(row['Ord'], description)
        print(summary)
        # append the summary to the sentiment_data list
        row['Beskrivelse'].append(summary)

# print the sentiment_data list
print(sentiment_data)
22/125:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(word, description):
    prompt = f"""
    Write a summary of the list of descriptions: {description} of why the word: {word} is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# for each row in the sentiment_data list, loop through the list of descriptions and generate a summary of the description 
for row in sentiment_data:
    for description in row['Beskrivelse']:
        print(description)
        summary = summary_of_description(row['Ord'], description)
        print(summary)
        # append the summary to the sentiment_data list
        row['Beskrivelse'].append(summary)

# print the sentiment_data list
print(sentiment_data)
22/126:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(word, description):
    prompt = f"""
    Write a summary of the list of descriptions: {description} of why the word: {word} is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
# print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# for each row in the sentiment_data list, loop through the list of descriptions and generate a summary of the description 
for row in sentiment_data:
    for description in row['Beskrivelse']:
        print(description)
        summary = summary_of_description(row['Ord'], description)
        print(summary)
        # append the summary to the sentiment_data list
        row['Beskrivelse'].append(summary)

# print the sentiment_data list
print(sentiment_data)
22/127:
# Use the openai.ChatCompletion.create() method to generate a summary of the description column of the sentiment_words_test.csv file
def summary_of_description(word, description):
    prompt = f"""
    Write a summary of the list of descriptions: {description} of why the word: {word} is used to describe the sentiment of the text.
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        prompt=prompt,
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    analysis= completion.choices[0].message.content

    return analysis

# loop through the sentiment_words_test.csv file and generate a summary of the description column


#print sentiment_words_test.csv file        
df = pd.read_csv("sentiment_words_test.csv")
# print(df)

# for each row in the dataframe, make a dictionary with Ord and Beskrivelse as keys and values respectively and append to a list called sentiment_data, make the values as a list splitted by "", /"" 
sentiment_data = []
for index, row in df.iterrows():
    sentiment_data.append({"Ord": row['Ord'], "Beskrivelse": row['Beskrivelse'].split('", ')})

    
# for each row in the sentiment_data list, loop through the list of descriptions and generate a summary of the description 
for row in sentiment_data:
    for description in row['Beskrivelse']:
        print(description)
        description = description.replace('"', '')
        summary = summary_of_description(row['Ord'], description)
        print(summary)
        # append the summary to the sentiment_data list
        row['Beskrivelse'].append(summary)

# print the sentiment_data list
print(sentiment_data)
22/128:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/129:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/130:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/131: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/132:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/133:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/134:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/135:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/136:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/137: df = pd.read_csv("sentiment_words.csv")
22/138:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/139:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/140:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
print(df.groupby('Ord')['Beskrivelse'].apply(list))

# save the df grouped description column only to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/141: print(grouped)
22/142:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/143:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/144:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/145: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/146:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/147:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/148:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/149:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/150:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/151: df = pd.read_csv("sentiment_words.csv")
22/152:

for url in urls[:5]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/153:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/154:
print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
print(df.groupby('Ord')['Beskrivelse'].apply(list))

# save the df grouped description column only to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/155: print(grouped)
22/156:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df)
22/157:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df[0])
22/158:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df[0:5])
22/159:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df[0][0])
22/160:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df)
22/161:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df.index[0])
22/162:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df.index[1])
22/163:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df("index")[1])
22/164:
grouped_df = pd.read_csv("sentiment_words_test.csv")
print(grouped_df("index"))
22/165:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
print(grouped_df.iloc[0,1])
22/166:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])
print(type(test))
22/167:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

# loop through the grouped df and save it as strings
22/168:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

# loop through the grouped df and save it as strings
for i in range(len(grouped_df)):
    test = (grouped_df.iloc[i,1])
    test = str(test)
    print(test)
    # pr
22/169:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

# loop through the grouped df and save it as strings
for i in range(len(grouped_df)):
    test = (grouped_df.iloc[i,1])
    test = str(test)
    print(type(test))
    # pr
22/170:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

# loop through the grouped df and save it as strings
for i in range(len(grouped_df)):
    test = (grouped_df.iloc[i,1])
    test = str(test)
    print(test)
    # pr
22/171:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Use the following words to describe the sentiment of the text: {word} \n {list} \n
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/172:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

print(grouped_df.iloc[1,1])
22/173:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

print(grouped_df.iloc[1,0])
22/174:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

print(grouped_df.iloc[1,0])
print(test)
22/175:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[1,1])

print(grouped_df.iloc[1,0])
print(test)
22/176:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

print(grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(grouped_df.iloc[1,0],test)
22/177:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])

print(grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(grouped_df.iloc[1,0],test)
print(analyse)
22/178:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
test = (grouped_df.iloc[0,1])
print(test)
print(grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(grouped_df.iloc[1,0],test)
print(analyse)
22/179:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Use the following list of sentences: {list} to describe why the word {word} is a good description. 
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/180:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(analyse)
22/181:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {liste} for  lage en begrunnelse for ordet {ord}. 
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/182:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(analyse)
22/183:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for ordet {word}. 
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/184:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(analyse)
22/185:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(word_of_sentiment, " ", analyse)
22/186:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. 
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/187:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(word_of_sentiment, " ", analyse)
22/188:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. 
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/189:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(word_of_sentiment, " ", analyse)
22/190:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. 
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=50,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/191:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(word_of_sentiment, " ", analyse)
22/192:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. 
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=100,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/193:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(word_of_sentiment, " ", analyse)
22/194:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. Bruk 100 tokens. 
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/195:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(word_of_sentiment, " ", analyse)
22/196:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/197:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/198:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(word_of_sentiment, " ", analyse)
22/199:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# print one element from the grouped df
list_of_sentiment = (grouped_df.iloc[0,1])

word_of_sentiment = (grouped_df.iloc[1,0])

analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/200:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/201:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/202:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/203: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/204:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/205:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/206:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/207:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/208:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/209: df = pd.read_csv("sentiment_words.csv")
22/210:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/211:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/212:
# print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
# print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
# print(df.groupby('Ord')['Beskrivelse'].apply(list))

# save the df grouped description column only to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/213:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/214:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/215:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/216:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/217:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/218:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/219: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/220:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/221:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/222:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/223:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/224:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/225: df = pd.read_csv("sentiment_words.csv")
22/226:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/227:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/228:
# print(df)
# group the df by the "Ord" column and count the number of occurrences
df.groupby('Ord').count()
# print df by group ord and count and sort by count
# print(df.groupby('Ord').count().sort_values(by='Ord', ascending=False))

# print df with the grouped description column only
# print(df.groupby('Ord')['Beskrivelse'].apply(list))

# save the df grouped description column only to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/229:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/230:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/231:
# drop description column from the grouped df
grouped_df = grouped_df.drop(['Beskrivelse'], axis=1)
grouped_df.to_csv("sentiment_words_analyse.csv")
22/232:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

print(df)

# save the df grouped description column only to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/233:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)

# save the df grouped description column only to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/234:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)

# save the df grouped description column only to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/235:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# save the df grouped description column only to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/236:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/237:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/238:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/239:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
print(grouped)
22/240:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/241:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/242:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/243:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/244: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/245:
def analyze_url(text,url):

    # # Create a prompt for GPT-3
    # prompt = f"""
    #          As a social scientist, your objective is to conduct a sentiment analysis of the provided {text}. You should identify and describe the prevailing sentiment using four descriptive words, wit a explaination per word.
    #          Please format your response in CSV format as follows:

    #         Word 1:Description
    #         Word 2:Description
    #         Word 3:Description
    #         Word 4:Description
    #         """
    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
    # prompt =  f"""      
    #             As a social scientist, your objective is to conduct a sentiment analysis of the provided {text} from the specified {url}. 
    #             Your task is to identify and describe the prevailing sentiment using four descriptive words. 
    #             In cases where determining sentiment is challenging, please indicate "no information."
    #             """

    # prompt = f"""
    # "As a social scientist, your task is to analyze the sentiment of the {text} from the specified {url}. 
    # Describe the sentiment using four words and provide a brief description for each word. 
    # If the sentiment is difficult to definitively analyze, write 'no information' in the description column.

    # Word 1    Word 2  Word 3  Word 4
    # Description:  Description:    Description:    Description:
    # """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/246:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/247:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/248:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/249:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/250: df = pd.read_csv("sentiment_words.csv")
22/251:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/252:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/253:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list

# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/254:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/255:
grouped_df = pd.read_csv("sentiment_words_test.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/256:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list

# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/257:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list

# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")

print(grouped)
22/258:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list

# save the df grouped description column and sort it by occurences to a csv file
grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")

print(grouped)
22/259:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list


# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")

print(grouped)
22/260:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")

print(grouped)
22/261:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/262:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/263:
grouped_df = pd.read_csv("sentiment_words_grouped.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/264:
# drop description column from the grouped df
grouped_df = grouped_df.drop(['Beskrivelse'], axis=1)
grouped_df.to_csv("sentiment_words_analyse.csv")
22/265:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/266:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/267:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/268: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/269:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. 
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/270:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/271:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/272:

base_url = "https://fortedigital.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/273:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/274: df = pd.read_csv("sentiment_words.csv")
22/275:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/276:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/277:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/278:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/279:
grouped_df = pd.read_csv("sentiment_words_grouped.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/280:
# drop description column from the grouped df
grouped_df = grouped_df.drop(['Beskrivelse'], axis=1)
grouped_df.to_csv("sentiment_words_analyse.csv")
22/281:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/282:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/283:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/284: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/285:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
    Lett
    heavy teknisk
    produktrettet
    brukerrettet
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/286:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/287:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/288:

base_url = "https://tek.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/289:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/290: df = pd.read_csv("sentiment_words.csv")
22/291:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/292:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/293:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/294:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/295:
grouped_df = pd.read_csv("sentiment_words_grouped.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/296:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/297:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/298:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/299: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/300:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
    Lett
    heavy teknisk
    produktrettet
    brukerrettet
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/301:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/302:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/303:

base_url = "https://tek.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/304:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/305: df = pd.read_csv("sentiment_words.csv")
22/306:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/307:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/308:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/309:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/310:
grouped_df = pd.read_csv("sentiment_words_grouped.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/311:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/312:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/313:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/314: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/315:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Analytisk
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Saklig
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
    Lett
    heavy teknisk
    produktrettet
    brukerrettet
    Forretningsorientert
    Fremtidsrettet
    Selvironisk
    Hjertelig
    Fornyd
    Fordmmende
    Realistisk
    Konkret
    Uklar
    Karismatisk
    Optimistisk
    Pessimistisk
    Diplomatisk
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/316:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/317:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/318:

base_url = "https://tek.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/319:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/320: df = pd.read_csv("sentiment_words.csv")
22/321:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/322:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/323:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/324:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/325:
grouped_df = pd.read_csv("sentiment_words_grouped.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/326:
# drop description column from the grouped df
grouped_df = grouped_df.drop(['Beskrivelse'], axis=1)
grouped_df.to_csv("sentiment_words_analyse.csv")
22/327:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/328:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/329:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/330: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/331:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
    Lett
    heavy teknisk
    produktrettet
    brukerrettet
    Forretningsorientert
    Fremtidsrettet
    Selvironisk
    Hjertelig
    Fornyd
    Fordmmende
    Realistisk
    Konkret
    Uklar
    Karismatisk
    Optimistisk
    Pessimistisk
    Diplomatisk
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/332:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/333:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/334:

base_url = "https://tek.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/335:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/336: df = pd.read_csv("sentiment_words.csv")
22/337:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/338:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/339:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/340:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/341:
grouped_df = pd.read_csv("sentiment_words_grouped.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/342:
# drop description column from the grouped df
grouped_df = grouped_df.drop(['Beskrivelse'], axis=1)
grouped_df.to_csv("sentiment_words_analyse.csv")
22/343:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/344:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/345:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/346: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/347:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
    Lett
    heavy teknisk
    produktrettet
    brukerrettet
    Forretningsorientert
    Fremtidsrettet
    Selvironisk
    Hjertelig
    Fornyd
    Fordmmende
    Realistisk
    Konkret
    Uklar
    Karismatisk
    Optimistisk
    Pessimistisk
    Diplomatisk
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/348:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/349:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/350:

base_url = "https://tek.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/351:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/352: df = pd.read_csv("sentiment_words.csv")
22/353:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/354:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/355:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/356:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/357:
grouped_df = pd.read_csv("sentiment_words_grouped.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/358:
# drop description column from the grouped df
grouped_df = grouped_df.drop(['Beskrivelse'], axis=1)
grouped_df.to_csv("sentiment_words_analyse.csv")
22/359:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/360:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/361:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/362: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/363:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
    Lett
    heavy teknisk
    produktrettet
    brukerrettet
    Forretningsorientert
    Fremtidsrettet
    Selvironisk
    Hjertelig
    Fornyd
    Fordmmende
    Realistisk
    Konkret
    Uklar
    Karismatisk
    Optimistisk
    Pessimistisk
    Diplomatisk
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/364:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/365:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/366:

base_url = "https://tek.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/367:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/368: df = pd.read_csv("sentiment_words.csv")
26/1:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
26/2:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
26/3:
documents = []

pdf_path = './forte_data/' + 'posts.pdf'
loader = PyPDFLoader(pdf_path)
documents.extend(loader.load_and_split())

pdf_path = './forte_data/' + 'forte_web (2).pdf'
loader = PyPDFLoader(pdf_path)
documents.extend(loader.load_and_split())
26/4:
text = ''
for doc in documents:
    text += doc.page_content

len(text)
26/5: #text = 'I dagens stadig mer komplekse og globaliserte verden er det avgjrende  forst og hndtere en rekke utfordringer og sprsml p en reflektert og ansvarlig mte. Samfunnet str overfor en rekke komplekse problemstillinger, inkludert klimaendringer, konomiske ulikheter, helsekriser og geopolitiske spenninger. For  takle disse utfordringene, er det viktig  utvikle en dypere forstelse av problemenes rtter og finne brekraftige lsninger. En grundig analyse og vitenskapelig tilnrming er ndvendig for  forst og hndtere problemene knyttet til klimaendringer. Klimavitenskapere har enstemmig advart om konsekvensene av kende globale temperaturer, og det er avgjrende  ta drastiske tiltak for  redusere utslipp av klimagasser og begrense temperaturstigningen. Dette krever internasjonalt samarbeid og politiske beslutninger som er basert p vitenskapelig kunnskap og bevis. konomiske ulikheter er et annet alvorlig problem som pvirker samfunn over hele verden. Det er ndvendig med en rettferdig fordeling av ressurser og muligheter for  sikre en mer inkluderende konomi. Dette krever politiske tiltak som fremmer konomisk rettferdighet og sosial mobilitet. I tillegg er helsekriser, som pandemier, en pminnelse om viktigheten av global helseberedskap og samarbeid. Effektive helsevesen og beredskapsplaner er avgjrende for  hndtere slike kriser og beskytte folkehelsen. Geopolitiske spenninger krever ogs en nye tilnrming og diplomati. Fredelig konfliktlsning og dialog er ofte den beste mten  hndtere internasjonale konflikter p. Samlet sett er det avgjrende at samfunnet tar ansvar for  adressere disse og andre alvorlige utfordringer. Dette krever innsats fra enkeltpersoner, samfunnet, myndigheter og internasjonale organisasjoner. Ved  arbeide sammen og ta beslutninger basert p vitenskapelig kunnskap og ansvarlig ledelse, kan vi hpe p en mer stabil og brekraftig fremtid.'
26/6:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=3000,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False
)
26/7: texts = text_splitter.create_documents([text])
26/8: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0)
26/9:
map_custom_prompt='''
Oppsummer flgende tekst p en klar og kortfattet mte:
TEKST: {text}
Kort oppsummering:
'''
26/10:
map_prompt_template = PromptTemplate (
    input_variables=['text'],
    template=map_custom_prompt
)
26/11:
combine_custom_prompt='''
Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
Vennligst analyser flgende tekst og identifiser de 3 ordene som best beskriver skrivestilen og sprket som brukes. 
Gi ogs en kort beskrivelse p hvert av disse ordene og hvorfor du velger at disse karakteriserer teksten.

Gi svaret p formatet:

Ord 1 | Beskrivelse for ord 1
Ord 2 | Beskrivelse for ord 2
Ord 3 | Beskrivelse for ord 3

Tekst:`{text}`
'''

combine_prompt_template = PromptTemplate(
    template=combine_custom_prompt, 
    input_variables=['text']
)
26/12: chain = load_summarize_chain(llm, chain_type="map_reduce", verbose=True, map_prompt=map_prompt_template, combine_prompt=combine_prompt_template)
26/13:
from time import time

start = time()
res_aa = await chain.arun(texts)
print(res_aa)
print(f"aapply time taken: {time() - start:.2f} seconds")
26/14:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
26/15:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
26/16:
documents = []

pdf_path = './forte_data/' + 'posts.pdf'
loader = PyPDFLoader(pdf_path)
documents.extend(loader.load_and_split())

pdf_path = './forte_data/' + 'forte_web (2).pdf'
loader = PyPDFLoader(pdf_path)
documents.extend(loader.load_and_split())
26/17:
text = ''
for doc in documents:
    text += doc.page_content

len(text)
26/18: #text = 'I dagens stadig mer komplekse og globaliserte verden er det avgjrende  forst og hndtere en rekke utfordringer og sprsml p en reflektert og ansvarlig mte. Samfunnet str overfor en rekke komplekse problemstillinger, inkludert klimaendringer, konomiske ulikheter, helsekriser og geopolitiske spenninger. For  takle disse utfordringene, er det viktig  utvikle en dypere forstelse av problemenes rtter og finne brekraftige lsninger. En grundig analyse og vitenskapelig tilnrming er ndvendig for  forst og hndtere problemene knyttet til klimaendringer. Klimavitenskapere har enstemmig advart om konsekvensene av kende globale temperaturer, og det er avgjrende  ta drastiske tiltak for  redusere utslipp av klimagasser og begrense temperaturstigningen. Dette krever internasjonalt samarbeid og politiske beslutninger som er basert p vitenskapelig kunnskap og bevis. konomiske ulikheter er et annet alvorlig problem som pvirker samfunn over hele verden. Det er ndvendig med en rettferdig fordeling av ressurser og muligheter for  sikre en mer inkluderende konomi. Dette krever politiske tiltak som fremmer konomisk rettferdighet og sosial mobilitet. I tillegg er helsekriser, som pandemier, en pminnelse om viktigheten av global helseberedskap og samarbeid. Effektive helsevesen og beredskapsplaner er avgjrende for  hndtere slike kriser og beskytte folkehelsen. Geopolitiske spenninger krever ogs en nye tilnrming og diplomati. Fredelig konfliktlsning og dialog er ofte den beste mten  hndtere internasjonale konflikter p. Samlet sett er det avgjrende at samfunnet tar ansvar for  adressere disse og andre alvorlige utfordringer. Dette krever innsats fra enkeltpersoner, samfunnet, myndigheter og internasjonale organisasjoner. Ved  arbeide sammen og ta beslutninger basert p vitenskapelig kunnskap og ansvarlig ledelse, kan vi hpe p en mer stabil og brekraftig fremtid.'
26/19:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=3000,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False
)
26/20: texts = text_splitter.create_documents([text])
26/21: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0)
26/22:
map_custom_prompt='''
Oppsummer flgende tekst p en klar og kortfattet mte:
TEKST: {text}
Kort oppsummering:
'''
26/23:
map_prompt_template = PromptTemplate (
    input_variables=['text'],
    template=map_custom_prompt
)
26/24:
combine_custom_prompt='''
Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
Vennligst analyser flgende tekst og identifiser de 3 ordene som best beskriver skrivestilen og sprket som brukes.
Gi ogs en kort beskrivelse p hvert av disse ordene og hvorfor du velger at disse karakteriserer teksten.
Gi svaret p formatet der "Ord #" skal bli erstattet med ordet og "Beskrivelse for ord #" skal erstattes av beskrivelsen:
Ord 1 | Beskrivelse for ord 1
Ord 2 | Beskrivelse for ord 2
Ord 3 | Beskrivelse for ord 3
Du kan bruke disse eksemplene p ord som utgangspunkt i vurderingen din:
Selvironisk
Hjertelig
Fornyd
Fordmmende
Realistisk
Dvelende
Pvirkende
Besettende
Konkret
Uklar
Dyptgende
Skremmende
pen
Utpreget
Vennlig
Kynisk
Sorgfull
Srgmodig
Uforutsigbar
Troverdig
Karismatisk
Spenningsskapende
Ufravikelig
Overveldende
Lett
Forvirrende
Hylytt
Kjrlig
Mrk
Opplftende
Formell
Informativ
Humoristisk
Flelsesladet
Formanende
Inspirerende
Sjarmerende
Saklig
Muntlig
Autoritr
Nytral
Poetisk
Kritisk
Ironisk
Emosjonell
Diplomatisk
Enthusiastisk
Konsis
Deskriptiv
Leken
Rrende
Satirisk
Optimistisk
Pessimistisk
Spennende
Interessant
Dyster
Uformell
Kreativ
Underskende
Tekst:`{text}`
'''

combine_prompt_template = PromptTemplate(
    template=combine_custom_prompt, 
    input_variables=['text']
)
26/25: chain = load_summarize_chain(llm, chain_type="map_reduce", verbose=True, map_prompt=map_prompt_template, combine_prompt=combine_prompt_template)
26/26:
from time import time

start = time()
res_aa = await chain.arun(texts)
print(res_aa)
print(f"aapply time taken: {time() - start:.2f} seconds")
27/1:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
27/2:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
27/3:
documents = []

pdf_path = './forte_data/' + 'posts.pdf'
loader = PyPDFLoader(pdf_path)
documents.extend(loader.load_and_split())

pdf_path = './forte_data/' + 'forte_web (2).pdf'
loader = PyPDFLoader(pdf_path)
documents.extend(loader.load_and_split())
27/4:
text = ''
for doc in documents:
    text += doc.page_content

len(text)
27/5: #text = 'I dagens stadig mer komplekse og globaliserte verden er det avgjrende  forst og hndtere en rekke utfordringer og sprsml p en reflektert og ansvarlig mte. Samfunnet str overfor en rekke komplekse problemstillinger, inkludert klimaendringer, konomiske ulikheter, helsekriser og geopolitiske spenninger. For  takle disse utfordringene, er det viktig  utvikle en dypere forstelse av problemenes rtter og finne brekraftige lsninger. En grundig analyse og vitenskapelig tilnrming er ndvendig for  forst og hndtere problemene knyttet til klimaendringer. Klimavitenskapere har enstemmig advart om konsekvensene av kende globale temperaturer, og det er avgjrende  ta drastiske tiltak for  redusere utslipp av klimagasser og begrense temperaturstigningen. Dette krever internasjonalt samarbeid og politiske beslutninger som er basert p vitenskapelig kunnskap og bevis. konomiske ulikheter er et annet alvorlig problem som pvirker samfunn over hele verden. Det er ndvendig med en rettferdig fordeling av ressurser og muligheter for  sikre en mer inkluderende konomi. Dette krever politiske tiltak som fremmer konomisk rettferdighet og sosial mobilitet. I tillegg er helsekriser, som pandemier, en pminnelse om viktigheten av global helseberedskap og samarbeid. Effektive helsevesen og beredskapsplaner er avgjrende for  hndtere slike kriser og beskytte folkehelsen. Geopolitiske spenninger krever ogs en nye tilnrming og diplomati. Fredelig konfliktlsning og dialog er ofte den beste mten  hndtere internasjonale konflikter p. Samlet sett er det avgjrende at samfunnet tar ansvar for  adressere disse og andre alvorlige utfordringer. Dette krever innsats fra enkeltpersoner, samfunnet, myndigheter og internasjonale organisasjoner. Ved  arbeide sammen og ta beslutninger basert p vitenskapelig kunnskap og ansvarlig ledelse, kan vi hpe p en mer stabil og brekraftig fremtid.'
27/6:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=3000,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False
)
27/7: texts = text_splitter.create_documents([text])
27/8: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0)
27/9:
map_custom_prompt='''
Oppsummer flgende tekst p en klar og kortfattet mte:
TEKST: {text}
Kort oppsummering:
'''
27/10:
map_prompt_template = PromptTemplate (
    input_variables=['text'],
    template=map_custom_prompt
)
27/11:
combine_custom_prompt='''
Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
Vennligst analyser flgende tekst og identifiser de 3 ordene som best beskriver skrivestilen og sprket som brukes.
Gi ogs en kort beskrivelse p hvert av disse ordene og hvorfor du velger at disse karakteriserer teksten.
Gi svaret p formatet der "Ord #" skal bli erstattet med ordet og "Beskrivelse for ord #" skal erstattes av beskrivelsen:
Ord 1 | Beskrivelse for ord 1
Ord 2 | Beskrivelse for ord 2
Ord 3 | Beskrivelse for ord 3
Du kan bruke disse eksemplene p ord som utgangspunkt i vurderingen din:
Selvironisk
Hjertelig
Fornyd
Fordmmende
Realistisk
Dvelende
Pvirkende
Besettende
Konkret
Uklar
Dyptgende
Skremmende
pen
Utpreget
Vennlig
Kynisk
Sorgfull
Srgmodig
Uforutsigbar
Troverdig
Karismatisk
Spenningsskapende
Ufravikelig
Overveldende
Lett
Forvirrende
Hylytt
Kjrlig
Mrk
Opplftende
Formell
Informativ
Humoristisk
Flelsesladet
Formanende
Inspirerende
Sjarmerende
Saklig
Muntlig
Autoritr
Nytral
Poetisk
Kritisk
Ironisk
Emosjonell
Diplomatisk
Enthusiastisk
Konsis
Deskriptiv
Leken
Rrende
Satirisk
Optimistisk
Pessimistisk
Spennende
Interessant
Dyster
Uformell
Kreativ
Underskende
Tekst:`{text}`
'''

combine_prompt_template = PromptTemplate(
    template=combine_custom_prompt, 
    input_variables=['text']
)
27/12: chain = load_summarize_chain(llm, chain_type="map_reduce", verbose=True, map_prompt=map_prompt_template, combine_prompt=combine_prompt_template)
27/13:


res_aa = await chain.arun(texts)
22/369:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/370:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/371:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/372: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/373:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
    Lett
    heavy teknisk
    produktrettet
    brukerrettet
    Forretningsorientert
    Fremtidsrettet
    Selvironisk
    Hjertelig
    Fornyd
    Fordmmende
    Realistisk
    Konkret
    Uklar
    Karismatisk
    Optimistisk
    Pessimistisk
    Diplomatisk
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/374:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/375:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/376:

base_url = "https://tek.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/377:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/378: df = pd.read_csv("sentiment_words.csv")
22/379:
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# Function to get all the URLs on a webpage
def get_all_urls(base_url):
    # Create an empty set to store unique URLs
    all_urls = []
    
    # Make a GET request to the base URL
    response = requests.get(base_url)
    
    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all anchor (a) tags in the HTML
        for anchor in soup.find_all('a'):
            # Extract the href attribute
            href = anchor.get('href')
            if href:
                # Combine the base URL and the href to get the absolute URL
                absolute_url = urljoin(base_url, href)
                
                # Parse the absolute URL to remove any fragments or query parameters
                parsed_url = urlparse(absolute_url)
                cleaned_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path
                
                # Add the cleaned URL to the set
                all_urls.append(cleaned_url)
    
    return all_urls
22/380:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
import pandas as pd
22/381:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = "azure"
openai.api_base = "https://cog-ycyy4wyxwg7ck.openai.azure.com"
openai.api_version = "2023-03-15-preview"
22/382: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0.5)
22/383:
def analyze_url(text,url):

    beskrivende_ord = """
    Seris
    Leken
    Emosjonell
    Formell
    Uformell
    Kreativ
    Teknisk
    Flelsesladet
    Profesjonell
    Ironisk
    Srgelig
    Humoristisk
    Instruktiv
    Inspirerende
    Kritisk
    Fortellende
    Engasjerende
    Rrende
    Poetisk
    Konkret
    Abstrakt
    Reflekterende
    Poetisk
    Skarp
    Informerende
    Sprklig utfordrende
    Dyptgende
    Morsom
    Lett
    heavy teknisk
    produktrettet
    brukerrettet
    Forretningsorientert
    Fremtidsrettet
    Selvironisk
    Hjertelig
    Fornyd
    Fordmmende
    Realistisk
    Konkret
    Uklar
    Karismatisk
    Optimistisk
    Pessimistisk
    Diplomatisk
                        """
    prompt = f"""
            #  Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper. Du gjennomfre en sentiment analyse p flgeende tekst: {text}. Svar med 4 ord som godt beskriver sprket i teksten og en forklaring per ord. Bruk ord fra listen{beskrivende_ord}  
            Vennligst formater svaret ditt i CSV-format som flger:
            Ord 1:Beskrivelse \n
            Ord 2:Beskrivelse \n
            Ord 3:Beskrivelse \n
            Ord 4:Beskrivelse \n
            """
    
   
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=350,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )

    # Extract the GPT-3 generated analysis
    # analysis = completion.choices[0].text
    analysis= completion.choices[0].message.content

    return analysis
22/384:
def add_to_pandas(df,analysed_text_out):
    
    # Split the sentiment output into lines
    sentiment_lines = analysed_text_out.strip().split('\n')

    # Initialize an empty list to store sentiment data
    sentiment_data = []

    # Iterate through the lines and split them into words and descriptions
    for line in sentiment_lines:
        parts = line.split(':')
        #if len(parts) == 2:
        if len(parts) == 2:
            word = parts[0].strip()
            description = parts[1].strip()
            sentiment_data.append({"Ord": word, "Beskrivelse": description})

    # Create a DataFrame from the sentiment data
    new_df = pd.DataFrame.from_dict(sentiment_data)
    # df = pd.DataFrame(sentiment_data).append(df, sort=False)
    final_df = pd.concat([df,new_df],ignore_index=True)

    return final_df
22/385:
# import re
# def add_to_pandas(df,analysed_text_out):
#     pattern = r'Ord (\d+): (.+?)\nBeskrivelse: (.+?)\n'

#     # Use re.findall to extract matches
#     matches = re.findall(pattern, analysed_text_out, re.DOTALL)

#     # Create a list of dictionaries to store the extracted data
#     data_list = [{'Ord': match[0], 'Word': match[1], 'Description': match[2]} for match in matches]

#     # Create a pandas DataFrame
#     new_df = pd.DataFrame(data_list)
#     print(new_df)
#     # new_df = pd.DataFrame.from_dict(data_list)
#     # df = pd.DataFrame(sentiment_data).append(df, sort=False)
#     final_df = pd.concat([df,new_df],ignore_index=True)
#     print(final_df)
#     return final_df
22/386:

base_url = "https://tek.no"

# Get all the URLs on the website
urls = get_all_urls(base_url)

# Print the unique URLs
print(len(urls))
22/387:
from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import SeleniumURLLoader
22/388: df = pd.read_csv("sentiment_words.csv")
22/389:

for url in urls[:15]:
    loader = SeleniumURLLoader([url])
    data = loader.load()
    analyse_text_out = analyze_url(data,url)
    # print(analyse_text_out) 
    df = add_to_pandas(df,analyse_text_out)
22/390:
#remove columns with unamed in the name 
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df.to_csv("sentiment_words.csv")
22/391:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
22/392:
def gpt_analyse_grouped(word, list):
    # use gpt chat.completion to analyse the grouped description column
    prompt = f"""Bruk flgende liste med setninger: {list} for  lage en begrunnelse for hvorfor ordet {word} er brukt for  beskrive en tekst. skriv 50 ord
    """
    completion = openai.ChatCompletion.create(
        engine="chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
        )
    return completion.choices[0].message.content
22/393:
grouped_df = pd.read_csv("sentiment_words_grouped.csv")

# # print one element from the grouped df
# list_of_sentiment = (grouped_df.iloc[0,1])

# word_of_sentiment = (grouped_df.iloc[1,0])

# analyse = gpt_analyse_grouped(word_of_sentiment,list_of_sentiment)
# print(word_of_sentiment, " ", analyse)

# add the analysed text to the grouped df as a new column
grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

# save the grouped df with the analysed text as a new column to a csv file
grouped_df.to_csv("sentiment_words_test.csv")
22/394:
# drop description column from the grouped df
grouped_df = grouped_df.drop(['Beskrivelse'], axis=1)
grouped_df.to_csv("sentiment_words_analyse.csv")
22/395:
def analyse_av_tekst(data_to_analyse, prompt):
    """
    write a function thats take inn a text as a dokument and a prompt and returns an analysed text as a csv.
    """
    if data_to_analyse == "url":
        base_url = "https://tek.no"

        # Get all the URLs on the website
        urls = get_all_urls(base_url)

        for url in urls:
            loader = SeleniumURLLoader([url])
            data = loader.load()
            analyse_text_out = analyze_url(data,url)
            df = add_to_pandas(df,analyse_text_out)
        
        #remove columns with unamed in the name
        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
        df.to_csv("sentiment_words.csv")

        # group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
        df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

        # sort the df by the "occurences" column in descending order
        df = df.sort_values(by=['occurences'], ascending=False)
        # print(df)
        # group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
        grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

        # # save the grouped df to a csv file
        # grouped.to_csv("sentiment_words_grouped.csv")
        # grouped_df = pd.read_csv("sentiment_words_grouped.csv")

        grouped_df = grouped

        # add the analysed text to the grouped df as a new column
        grouped_df['analyse'] = grouped_df.apply(lambda row: gpt_analyse_grouped(row['Ord'], row['Beskrivelse']), axis=1)

        # save the grouped df with the analysed text as a new column to a csv file
        grouped_df.to_csv("sentiment_words_test.csv")

        # drop description column from the grouped df
        grouped_df = grouped_df.drop(['Beskrivelse'], axis=1)
        grouped_df.to_csv("sentiment_words_analyse.csv")

    elif data_to_analyse == "pdf":
        loader = PyPDFLoader(pdf_path)
        documents.extend(loader.load_and_split())
27/14:
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import AzureChatOpenAI
from langchain import PromptTemplate
import openai
import os
from dotenv import load_dotenv
27/15:
load_dotenv()

os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_TYPE"] = "azure"

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://cog-ycyy4wyxwg7ck.openai.azure.com"

openai.api_key = os.getenv("OPENAI_API_KEY")
27/16:
documents = []

pdf_path = './forte_data/' + 'posts.pdf'
loader = PyPDFLoader(pdf_path)
documents.extend(loader.load_and_split())

pdf_path = './forte_data/' + 'forte_web (2).pdf'
loader = PyPDFLoader(pdf_path)
documents.extend(loader.load_and_split())
27/17:
text = ''
for doc in documents:
    text += doc.page_content

len(text)
27/18: #text = 'I dagens stadig mer komplekse og globaliserte verden er det avgjrende  forst og hndtere en rekke utfordringer og sprsml p en reflektert og ansvarlig mte. Samfunnet str overfor en rekke komplekse problemstillinger, inkludert klimaendringer, konomiske ulikheter, helsekriser og geopolitiske spenninger. For  takle disse utfordringene, er det viktig  utvikle en dypere forstelse av problemenes rtter og finne brekraftige lsninger. En grundig analyse og vitenskapelig tilnrming er ndvendig for  forst og hndtere problemene knyttet til klimaendringer. Klimavitenskapere har enstemmig advart om konsekvensene av kende globale temperaturer, og det er avgjrende  ta drastiske tiltak for  redusere utslipp av klimagasser og begrense temperaturstigningen. Dette krever internasjonalt samarbeid og politiske beslutninger som er basert p vitenskapelig kunnskap og bevis. konomiske ulikheter er et annet alvorlig problem som pvirker samfunn over hele verden. Det er ndvendig med en rettferdig fordeling av ressurser og muligheter for  sikre en mer inkluderende konomi. Dette krever politiske tiltak som fremmer konomisk rettferdighet og sosial mobilitet. I tillegg er helsekriser, som pandemier, en pminnelse om viktigheten av global helseberedskap og samarbeid. Effektive helsevesen og beredskapsplaner er avgjrende for  hndtere slike kriser og beskytte folkehelsen. Geopolitiske spenninger krever ogs en nye tilnrming og diplomati. Fredelig konfliktlsning og dialog er ofte den beste mten  hndtere internasjonale konflikter p. Samlet sett er det avgjrende at samfunnet tar ansvar for  adressere disse og andre alvorlige utfordringer. Dette krever innsats fra enkeltpersoner, samfunnet, myndigheter og internasjonale organisasjoner. Ved  arbeide sammen og ta beslutninger basert p vitenskapelig kunnskap og ansvarlig ledelse, kan vi hpe p en mer stabil og brekraftig fremtid.'
27/19:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=3000,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False
)
27/20: texts = text_splitter.create_documents([text])
27/21: llm = AzureChatOpenAI(deployment_name="chat", model_name="gpt-35-turbo", temperature=0)
27/22:
map_custom_prompt='''
Oppsummer flgende tekst p en klar og kortfattet mte:
TEKST: {text}
Kort oppsummering:
'''
27/23:
map_prompt_template = PromptTemplate (
    input_variables=['text'],
    template=map_custom_prompt
)
27/24:
combine_custom_prompt='''
Du er en smart markedsfrer som har spisskompetanse innen tekstanalyse og posisjonering av selskaper.
Vennligst analyser flgende tekst og identifiser de 3 ordene som best beskriver skrivestilen og sprket som brukes.
Gi ogs en kort beskrivelse p hvert av disse ordene og hvorfor du velger at disse karakteriserer teksten.
Gi svaret p formatet der "Ord #" skal bli erstattet med ordet og "Beskrivelse for ord #" skal erstattes av beskrivelsen:
Ord 1 | Beskrivelse for ord 1
Ord 2 | Beskrivelse for ord 2
Ord 3 | Beskrivelse for ord 3
Du kan bruke disse eksemplene p ord som utgangspunkt i vurderingen din:
Selvironisk
Hjertelig
Fornyd
Fordmmende
Realistisk
Dvelende
Pvirkende
Besettende
Konkret
Uklar
Dyptgende
Skremmende
pen
Utpreget
Vennlig
Kynisk
Sorgfull
Srgmodig
Uforutsigbar
Troverdig
Karismatisk
Spenningsskapende
Ufravikelig
Overveldende
Lett
Forvirrende
Hylytt
Kjrlig
Mrk
Opplftende
Formell
Informativ
Humoristisk
Flelsesladet
Formanende
Inspirerende
Sjarmerende
Saklig
Muntlig
Autoritr
Nytral
Poetisk
Kritisk
Ironisk
Emosjonell
Diplomatisk
Enthusiastisk
Konsis
Deskriptiv
Leken
Rrende
Satirisk
Optimistisk
Pessimistisk
Spennende
Interessant
Dyster
Uformell
Kreativ
Underskende
Tekst:`{text}`
'''

combine_prompt_template = PromptTemplate(
    template=combine_custom_prompt, 
    input_variables=['text']
)
27/25: chain = load_summarize_chain(llm, chain_type="map_reduce", verbose=True, map_prompt=map_prompt_template, combine_prompt=combine_prompt_template)
27/26:


res_aa = await chain.arun(texts)
22/396:

# group the df by the "Ord" column and count the number of occurrences and add it as a new column occurences
df['occurences'] = df.groupby('Ord')['Ord'].transform('count')

# sort the df by the "occurences" column in descending order
df = df.sort_values(by=['occurences'], ascending=False)
# print(df)
# group the df by "Ord" and aggregate the count of occurences, and save the descriptions as a list
grouped = df.groupby('Ord').agg({'occurences': 'first', 'Beskrivelse': lambda x: list(x)}).sort_values(by=['occurences'], ascending=False)

print(grouped)

# save the grouped df to a csv file
grouped.to_csv("sentiment_words_grouped.csv")

# save the df grouped description column and sort it by occurences to a csv file
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).sort_values(ascending=False).to_csv("sentiment_words_test.csv")
# grouped = df.groupby('Ord')['Beskrivelse'].apply(list).to_csv("sentiment_words_test.csv")
39/1:


import pandas as pd

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv')

data.head(10)
39/2:


import pandas as pd

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', error_bad_lines=False)


data.head(10)
39/3:


import pandas as pd

# Read the data
columns_to_read = ["Name", "Category paths", "Description"]
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv')


data.head(10)
39/4:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', usecols=columns_to_read, error_bad_lines=False)

data.head(10)
39/5:
import pandas as pd

columns_to_read = ["Name", "Category paths", "Description"]
data = []

with open('data/PIM_obsolete_UI_181023.csv', 'r') as f:
    for line in f.readlines()[:100]:  # read first 100 lines
        try:
            row = pd.read_csv(pd.StringIO(line), usecols=columns_to_read)
            data.append(row)
        except pd.errors.ParserError:
            print(f"Skipping line due to parsing error: {line}")
            continue

data = pd.concat(data, ignore_index=True)
data.head(10)
39/6:
import pandas as pd
import StringIO

columns_to_read = ["Name", "Category paths", "Description"]
data = []

with open('data/PIM_obsolete_UI_181023.csv', 'r') as f:
    for line in f.readlines()[:100]:  # read first 100 lines
        try:
            row = pd.read_csv(pd.StringIO(line), usecols=columns_to_read)
            data.append(row)
        except pd.errors.ParserError:
            print(f"Skipping line due to parsing error: {line}")
            continue

data = pd.concat(data, ignore_index=True)
data.head(10)
39/7:
import pandas as pd

columns_to_read = ["Name", "Category paths", "Description"]
data = []

with open('data/PIM_obsolete_UI_181023.csv', 'r') as f:
    for line in f.readlines()[:100]:  # read first 100 lines
        try:
            row = pd.read_csv(pd.StringIO(line), usecols=columns_to_read)
            data.append(row)
        except pd.errors.ParserError:
            print(f"Skipping line due to parsing error: {line}")
            continue

data = pd.concat(data, ignore_index=True)
data.head(10)
39/8:
import pandas as pd

columns_to_read = ["Name", "Category paths", "Description"]
data = []

with open('data/PIM_obsolete_UI_181023.csv', 'r') as f:
    for line in f.readlines()[:100]:  # read first 100 lines
        try:
            row = pd.read_csv(pd.StringIO(line), usecols=columns_to_read)
            data.append(row)
        except pd.errors.ParserError:
            print(f"Skipping line due to parsing error: {line}")
            continue

data = pd.concat(data, ignore_index=True)
data.head(10)
39/9:
import pandas as pd

columns_to_read = ["Name", "Category paths", "Description"]
data = []

with open('data/PIM_obsolete_UI_181023.csv', 'r') as f:
    for line in f.readlines()[:100]:  # read first 100 lines
        try:
            row = pd.read_csv((line), usecols=columns_to_read)
            data.append(row)
        except pd.errors.ParserError:
            print(f"Skipping line due to parsing error: {line}")
            continue

data = pd.concat(data, ignore_index=True)
data.head(10)
39/10:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', usecols=columns_to_read)

data.head(10)
39/11:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', usecols=columns_to_read)

data.head(10)
39/12:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";" usecols=columns_to_read)

data.head(10)
39/13:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";", usecols=columns_to_read)

data.head(10)
39/14:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";", usecols=columns_to_read)

data.head(50)
39/15:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";", usecols=columns_to_read)

# only use the rows that do not have "Reseptvarer" in the "Category paths" column
data = data[~data["Category paths"].str.contains("Reseptvarer")]


data.head(50)
39/16:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";", usecols=columns_to_read)

# only use the rows that do not have "Reseptvarer" in the "Category paths" column
data = data[~data["Category paths"].str.contains("Reseptvarer")]


data.head(200)
39/17:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";", usecols=columns_to_read)

# only use the rows that do not have "Reseptvarer" in the "Category paths" column
data = data[~data["Category paths"].str.contains("Reseptvarer")]


data.head(500)

# save to csv 
data.to_csv('data/PIM_500.csv', sep=';', encoding='utf-8', index=False)
39/18:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";", usecols=columns_to_read)

# only use the rows that do not have "Reseptvarer" in the "Category paths" column
data = data[~data["Category paths"].str.contains("Reseptvarer")]

# only use rows with values in all the columns
data = data.dropna()


data.head(500)

# save to csv 
# data.to_csv('data/PIM_500.csv', sep=';', encoding='utf-8', index=False)
39/19:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";", usecols=columns_to_read)

# only use the rows that do not have "Reseptvarer" in the "Category paths" column
data = data[~data["Category paths"].str.contains("Reseptvarer")]

# only use rows with values in all the columns
data = data.dropna()


data.head(500)

# save to csv 
data.to_csv('data/PIM_500.csv', sep=';', encoding='utf-8', index=False)
39/20:
import pandas as pd

# Specify the columns to read
columns_to_read = ["Name", "Category paths", "Description"]

# Read the data
data = pd.read_csv('data/PIM_obsolete_UI_181023.csv', delimiter=";", usecols=columns_to_read)

# only use the rows that do not have "Reseptvarer" in the "Category paths" column
data = data[~data["Category paths"].str.contains("Reseptvarer")]

# only use rows with values in all the columns
data = data.dropna()


data.head(100)

# save to csv 
data.to_csv('data/PIM_100.csv', sep=';', encoding='utf-8', index=False)
40/1:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category_paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)

# remove na
df = df.dropna()

df = df.head(400)

df.to_csv("data/PIM400.csv", sep=";", index=False)
# Use columns name, description and category_paths
40/2:
import pandas as pd
import requests
# pd.set_option('display.max_columns', 100)
40/3:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category_paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)

# remove na
df = df.dropna()

df = df.head(400)

df.to_csv("data/PIM400.csv", sep=";", index=False)
# Use columns name, description and category_paths
40/4:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)

# remove na
df = df.dropna()

df = df.head(400)

df.to_csv("data/PIM400.csv", sep=";", index=False)
# Use columns name, description and category_paths
40/5:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove na
df = df.dropna()

df = df.head(400)

df.to_csv("data/PIM400.csv", sep=";", index=False)
# Use columns name, description and category_paths
40/6:
filepath = "data/PIM_obsolete_UI_181023.csv"
df = pd.read_csv(filepath, sep=";",error_bad_lines=False)

# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")


# Konverterer gitte kolonner til int
integer_colnames = ["Product_SKU","Farmalogg_number", "Farmalogg_number_(Variant)", "Alternative_farmalogg_number"]
for col_name in integer_colnames:
    df[col_name] = df[col_name].fillna(0).astype(int)

# Filtrerer bort reseptvarer og lagrer i ny dataframe
df_otc_tg = df[~df["Category_paths"].str.startswith("Reseptvarer")]
40/7:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove na
df = df.dropna()

df = df.head(400)

# df.to_csv("data/PIM400.csv", sep=";", index=False)
df.head(100)

# Use columns name, description and category_paths
40/8:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove na
df = df.dropna()

df = df.head(400)

# df.to_csv("data/PIM400.csv", sep=";", index=False)
df.head(10)

# Use columns name, description and category_paths
40/9:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove na
df = df.dropna()

df = df.head(400)

# df.to_csv("data/PIM400.csv", sep=";", index=False)
df.head(10)

# Use columns name, description and category_paths
40/10:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove na
df = df.dropna()

df = df.head(400)

df.to_csv("data/PIM400.csv", sep=";", index=False)


# Use columns name, description and category_paths
40/11:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove na
df = df.dropna()



df.head(100).to_csv("data/PIM400.csv", sep=";", index=False)


# Use columns name, description and category_paths
40/12:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove na
df = df.dropna()



df.head(100).to_csv("data/PIM400.csv", sep=";", index=False)


# Use columns name, description and category_paths
40/13:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# remove na
df = df.dropna()

#only save 50 rows to csv
df = df.head(50)

# save to csv
df.to_csv('data/50_rows.csv', index=False)


# Use columns name, description and category_paths
40/14:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# remove na
df = df.dropna()

#only save 50 rows to csv
df = df.head(400)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
40/15:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na
df = df.dropna()

#only save 50 rows to csv
df = df.head(400)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
40/16:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na
df = df.dropna()

#only save 50 rows to csv
df = df.head(400)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
40/17:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
df = df[~df["Description"].str.contains("resept", case=False)]

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na
df = df.dropna()

#only save 50 rows to csv
df = df.head(400)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
40/18:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
df = df[~df["Description"].str.contains("Resept", case=False)]

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na
df = df.dropna()

#only save 50 rows to csv
df = df.head(400)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
40/19:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
df = df[~df["Description"].str.contains("Resept", case=False)]

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na
df = df.dropna()

#only save 50 rows to csv
df = df.head(400)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
40/20:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
df = df.dropna()
df = df[~df["Description"].str.contains("Resept", case=False)]

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(400)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/1:
import pandas as pd
import requests
# pd.set_option('display.max_columns', 100)
41/2:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
df = df.dropna()
df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
df = df.dropna(subset=['Classification'])

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(2000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/3:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification","Primary category"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
df = df.dropna()
# df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
# df = df.dropna(subset=['Classification'])

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(2000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/4:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification","Primary category"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
df = df.dropna()
# df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
# df = df.dropna(subset=['Classification'])

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(2000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/5:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification","Primary category"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
df = df.dropna()
# df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
# df = df.dropna(subset=['Classification'])

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(6000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/6:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification","Primary category"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
# df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
# df = df.dropna(subset=['Classification'])

# df = df[~df["Category paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(6000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/7:
import pandas as pd
import requests
# pd.set_option('display.max_columns', 100)
41/8:
filepath = "data/PIM_obsolete_UI_181023.csv"
df = pd.read_csv(filepath, sep=";")

# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")


# Konverterer gitte kolonner til int
integer_colnames = ["Product_SKU","Farmalogg_number", "Farmalogg_number_(Variant)", "Alternative_farmalogg_number"]
for col_name in integer_colnames:
    df[col_name] = df[col_name].fillna(0).astype(int)

# Filtrerer bort reseptvarer og lagrer i ny dataframe
df_otc_tg = df[~df["Category_paths"].str.startswith("Reseptvarer")]
41/9:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

print(data.head())
41/10:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns
print(data.columns)
41/11:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns
print(data.columns)

columns = ["Name", "Description", "Category paths","Classification","Primary category",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]
print(data.head(2))
41/12:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns
print(data.columns)

columns = ["Name", "Description", "Category paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]
print(data.head(2))
41/13:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns
print(data.columns)

columns = ["Name", "Description", "Category paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]
print(data.head(2))
41/14:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns
print(data.columns)

columns = ["Name", "Description", "Category paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]
print(data.head(2))
41/15:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns
print(data.columns)

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]
print(data.head(2))
41/16:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]
print(data.head(2))
41/17:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/18:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/19:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
df = df[~df["Description"].str.contains("Resept", case=False)]

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/20:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
df = df[~df["Description"].str.contains("Resept", case=False)]

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/21:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
df = df[~df["Category_paths"].str.contains("Resept", case=False)]

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/22:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/23:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification","Primary category"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
# df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
# df = df.dropna(subset=['Classification'])

df = df[~df["Category_paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(6000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/24:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/25:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/26:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))
#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/27:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))
#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/28:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))
#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/29:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))
#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/30:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))
#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/31:
filepath = "data/PIM_obsolete_UI_181023.csv"
df = pd.read_csv(filepath, sep=";")

# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")


# Konverterer gitte kolonner til int
integer_colnames = ["Product_SKU","Farmalogg_number", "Farmalogg_number_(Variant)", "Alternative_farmalogg_number"]
for col_name in integer_colnames:
    df[col_name] = df[col_name].fillna(0).astype(int)

# Filtrerer bort reseptvarer og lagrer i ny dataframe
df_otc_tg = df[~df["Category_paths"].str.startswith("Reseptvarer")]
41/32:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification","Primary category"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
# df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
# df = df.dropna(subset=['Classification'])

df = df[~df["Category_paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(6000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/33:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))
#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/34:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/35:
import pandas as pd
import requests
# pd.set_option('display.max_columns', 100)
41/36:
filepath = "data/PIM_obsolete_UI_181023.csv"
df = pd.read_csv(filepath, sep=";")

# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")


# Konverterer gitte kolonner til int
integer_colnames = ["Product_SKU","Farmalogg_number", "Farmalogg_number_(Variant)", "Alternative_farmalogg_number"]
for col_name in integer_colnames:
    df[col_name] = df[col_name].fillna(0).astype(int)

# Filtrerer bort reseptvarer og lagrer i ny dataframe
df_otc_tg = df[~df["Category_paths"].str.startswith("Reseptvarer")]
41/37:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification","Primary category"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
# df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
# df = df.dropna(subset=['Classification'])

df = df[~df["Category_paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(6000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/38:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/39:
import pandas as pd
import requests
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
# pd.set_option('display.max_columns', 100)
41/40:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/41:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/42:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/43:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/44:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/45:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
data = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
data.to_csv('data/PIM_images_400.csv', index=False)
41/46:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/47:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/48:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.8

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/49:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.9

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/50:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# # Create a TF-IDF vectorizer
# vectorizer = TfidfVectorizer()


# # Compute TF-IDF vectors for each item
# tfidf = vectorizer.fit_transform(df['Name'])

# # Compute cosine similarity between each pair of items
# cosine_similarities = linear_kernel(tfidf, tfidf)

# # For each item, find the other items that are too similar (cosine similarity > 0.8)
# too_similar = cosine_similarities > 0.9

# # Remove the items that are too similar
# df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/51:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()


# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.6

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/52:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()


# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.99

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/53:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

df['text'] = df['Name'] + ' ' + df['Category_paths']

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['text'])


# Compute cosine similarity between each pair of items
cosine_similarities = linear_kernel(tfidf, tfidf)

# For each item, find the other items that are too similar (cosine similarity > 0.8)
too_similar = cosine_similarities > 0.99

# Remove the items that are too similar
df = df[~too_similar.any(axis=1)]
print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/54:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer

# Concatenate the 'Name' and 'Category_paths' columns into a single text
df['text'] = df['Name'] + ' ' + df['Category_paths']

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['text'])

# Create a DBSCAN object and fit the data
dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')
clusters = dbscan.fit_predict(tfidf)

# Add the cluster labels to the DataFrame
df['cluster'] = clusters

# Keep only one item from each cluster
df = df.drop_duplicates(subset='cluster', keep='first')

print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/55:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer

# Concatenate the 'Name' and 'Category_paths' columns into a single text
df['text'] = df['Name'] + ' ' + df['Category_paths']

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Create a DBSCAN object and fit the data
dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')
clusters = dbscan.fit_predict(tfidf)

# Add the cluster labels to the DataFrame
df['cluster'] = clusters

# Keep only one item from each cluster
df = df.drop_duplicates(subset='cluster', keep='first')

print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/56:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer

# Concatenate the 'Name' and 'Category_paths' columns into a single text
df['text'] = df['Name'] + ' ' + df['Category_paths']

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Create a DBSCAN object and fit the data
dbscan = DBSCAN(eps=0.3, min_samples=2, metric='cosine')
clusters = dbscan.fit_predict(tfidf)

# Add the cluster labels to the DataFrame
df['cluster'] = clusters

# Keep only one item from each cluster
df = df.drop_duplicates(subset='cluster', keep='first')

print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/57:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer

# Concatenate the 'Name' and 'Category_paths' columns into a single text
df['text'] = df['Name'] + ' ' + df['Category_paths']

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Create a DBSCAN object and fit the data
dbscan = DBSCAN(eps=0.8, min_samples=2, metric='cosine')
clusters = dbscan.fit_predict(tfidf)

# Add the cluster labels to the DataFrame
df['cluster'] = clusters

# Keep only one item from each cluster
df = df.drop_duplicates(subset='cluster', keep='first')

print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/58:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer

# Concatenate the 'Name' and 'Category_paths' columns into a single text
df['text'] = df['Name'] + ' ' + df['Category_paths']

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Create a DBSCAN object and fit the data
dbscan = DBSCAN(eps=0.2, min_samples=2, metric='cosine')
clusters = dbscan.fit_predict(tfidf)

# Add the cluster labels to the DataFrame
df['cluster'] = clusters

# Keep only one item from each cluster
df = df.drop_duplicates(subset='cluster', keep='first')

print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/59:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer

# Concatenate the 'Name' and 'Category_paths' columns into a single text
df['text'] = df['Name'] + ' ' + df['Category_paths']

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Create a DBSCAN object and fit the data
dbscan = DBSCAN(eps=0.5, min_samples=1, metric='cosine')
clusters = dbscan.fit_predict(tfidf)

# Add the cluster labels to the DataFrame
df['cluster'] = clusters

# Keep only one item from each cluster
df = df.drop_duplicates(subset='cluster', keep='first')

print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
41/60:
filepath = "data/PIM_obsolete_UI_181023.csv"

columns = ["Name", "Description", "Category paths","Classification","Primary category"]
df = pd.read_csv(filepath, sep=";", usecols=columns)


# Fjern mellomrom fra alle kolonnenavn. Mellomrom er trbbel.
df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))

# remove if Reseptvarer in Category_paths
#remove if resept in description
# df = df.dropna()
# df = df[~df["Description"].str.contains("Resept", case=False)]

#drop if classification is empty
# df = df.dropna(subset=['Classification'])

df = df[~df["Category_paths"].str.startswith("Reseptvarer")]


# remove na

#only save 50 rows to csv
df = df.head(6000)

# save to csv
df.to_csv('data/PIM_400.csv', index=False)


# Use columns name, description and category_paths
41/61:
import pandas as pd

# Load the pkl file
with open('../df_PIM_embedings_with_images.pkl', 'rb') as f:
    data = pd.read_pickle(f)

# print columns

columns = ["Name", "Description", "Category_paths","Classification",'image_url','URL']

# Use columns name, description and category_paths
df = data[columns]

df.columns = df.columns.str.replace(" ", "_")

# remove all comma and make it to dot in all rows
df = df.apply(lambda x: x.str.replace(',', '.'))


# drop evry line that dont have all the values in all the columns
# df = df.dropna()

# remove if Reseptvarer in Category_paths
df = df[~df["Category_paths"].str.contains("Reseptvarer", case=False)]

print(len(df))
df = df.drop_duplicates(subset='Name', keep='first')
print(len(df))


# Create a TF-IDF vectorizer
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer

# Concatenate the 'Name' and 'Category_paths' columns into a single text
df['text'] = df['Name'] + ' ' + df['Category_paths']

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF vectors for each item
tfidf = vectorizer.fit_transform(df['Name'])

# Create a DBSCAN object and fit the data
dbscan = DBSCAN(eps=0.5, min_samples=1, metric='cosine')
clusters = dbscan.fit_predict(tfidf)

# Add the cluster labels to the DataFrame
df['cluster'] = clusters

# Keep only one item from each cluster
df = df.drop_duplicates(subset='cluster', keep='first')

print(len(df))

#save to csv
df.to_csv('data/PIM_images_400.csv', index=False)
42/1:
import requests
import xml.etree.ElementTree as ET
import json

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = 'https://www.farmasiet.no/googleproductfeed'

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {'g': 'http://base.google.com/ns/1.0'}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find('g:item_group_id', ns)
            #contructing URL
            url = base_url + item_group_id.text

            # Check if the page is available
            page_response = requests.get(url)
            page_available = page_response.status_code == 200

            # Extract the text within the 'g:price' tag
            price = entry.find('g:price', ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(' NOK', '') if price is not None else 'N/A'
            
            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {'price': price_text, 'available': page_available}
    else:
        print('Failed to retrieve the XML file')

    # Save the results to a JSON file
    with open('results.json', 'w') as f:
        json.dump(results, f)

# Call the function
scrape_prices()
42/2:
import requests
import xml.etree.ElementTree as ET
import json

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = 'https://www.farmasiet.no/googleproductfeed'

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {'g': 'http://base.google.com/ns/1.0'}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find('g:item_group_id', ns)
            #contructing URL
            url = base_url + item_group_id.text

            # Check if the page is available
            page_response = requests.get(url)
            page_available = page_response.status_code == 200

            # Extract the text within the 'g:price' tag
            price = entry.find('g:price', ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(' NOK', '') if price is not None else 'N/A'
            
            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {'price': price_text, 'available': page_available}
    else:
        print('Failed to retrieve the XML file')

    # Save the results to a JSON file
    return results

# Call the function
scrape_prices()
42/3:
import requests
import xml.etree.ElementTree as ET

# Define root to product
base_url = "http://farmasiet.no/catalog/"
# Load the XML data from the provided link
url = 'https://www.farmasiet.no/googleproductfeed'

# Send a GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the XML content from the response
    root = ET.fromstring(response.content)

    # Define the namespace to search for the tags properly
    ns = {'g': 'http://base.google.com/ns/1.0'}

    # Create an empty dictionary to store the results
    results = {}

    # Iterate through each 'entry' in the XML
    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
        # Extract the text within the 'g:item_group_id' tag
        item_group_id = entry.find('g:item_group_id', ns)
        #contructing URL
        url = base_url + item_group_id.text

        # Extract the text within the 'g:price' tag
        price = entry.find('g:price', ns)
        # If the 'price' tag is found, remove ' NOK' from the text
        price_text = price.text.replace(' NOK', '') if price is not None else 'N/A'
        
        # Add the results to the dictionary
        if item_group_id is not None and price is not None:
            results[url] = price_text
else:
    print('Failed to retrieve the XML file')

# Print the results
print(results)
42/4:
import requests
import xml.etree.ElementTree as ET
import json
import aiohttp
import asyncio

async def fetch(session, url):
    async with session.get(url) as response:
        return response.status == 200

async def fetch_all(urls):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for url in urls:
            tasks.append(fetch(session, url))
        return await asyncio.gather(*tasks)

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = 'https://www.farmasiet.no/googleproductfeed'

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {'g': 'http://base.google.com/ns/1.0'}

        # Create an empty dictionary to store the results
        results = {}

        # Create a list to store the URLs
        urls = []

        # Iterate through each 'entry' in the XML
        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find('g:item_group_id', ns)
            #contructing URL
            url = base_url + item_group_id.text
            urls.append(url)

            # Extract the text within the 'g:price' tag
            price = entry.find('g:price', ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(' NOK', '') if price is not None else 'N/A'
            
            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {'price': price_text, 'available': False}

        # Check the availability of the pages
        availability = asyncio.run(fetch_all(urls))
        for url, available in zip(urls, availability):
            if url in results:
                results[url]['available'] = available
    else:
        print('Failed to retrieve the XML file')

    # Save the results to a JSON file
    with open('results.json', 'w') as f:
        json.dump(results, f)

# Call the function
scrape_prices()
42/5:
import requests
import xml.etree.ElementTree as ET
import json
import aiohttp
import nest_asyncio
nest_asyncio.apply()

async def fetch(session, url):
    async with session.get(url) as response:
        return response.status == 200

async def fetch_all(urls):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for url in urls:
            tasks.append(fetch(session, url))
        return await asyncio.gather(*tasks)

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = 'https://www.farmasiet.no/googleproductfeed'

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {'g': 'http://base.google.com/ns/1.0'}

        # Create an empty dictionary to store the results
        results = {}

        # Create a list to store the URLs
        urls = []

        # Iterate through each 'entry' in the XML
        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find('g:item_group_id', ns)
            #contructing URL
            url = base_url + item_group_id.text
            urls.append(url)

            # Extract the text within the 'g:price' tag
            price = entry.find('g:price', ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(' NOK', '') if price is not None else 'N/A'
            
            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {'price': price_text, 'available': False}

        # Check the availability of the pages
        loop = asyncio.get_event_loop()
        availability = loop.run_until_complete(fetch_all(urls))
        for url, available in zip(urls, availability):
            if url in results:
                results[url]['available'] = available
    else:
        print('Failed to retrieve the XML file')

    # Save the results to a JSON file
    with open('results.json', 'w') as f:
        json.dump(results, f)

# Call the function
scrape_prices()
42/6:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            print("Request successful")
            print(url)
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"

            available = find_if_product_not_available(url)

            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {"price": price_text, "available": available}
                print(results)

    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)

scrape_prices()
42/7:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            print("Request successful")
            print(url)
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"

            available = find_if_product_not_available(url)

            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {"price": price_text, "available": available}
                print(results)

    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)


from bs4 import BeautifulSoup
def find_if_product_not_available(url):
    # Send a GET request to the URL
    response = requests.get(url)
    # check if requestis 200
    if response.status_code != 200:
        print("Request failed")
        return False

    # Parse the HTML content of the response using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the product availability element using a CSS selector
    availability_element = soup.select_one(".Product__NotAvailableContent")

    # Determine whether the product is not available
    product_not_available = availability_element is not None
    if product_not_available:
        print("Product not available")
        return False
    else:
        return True


scrape_prices()
42/8:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            print("Request successful")
            print(url)
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"

            available = find_if_product_not_available(url)

            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {"price": price_text, "available": available}
                print(results)

    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)


from bs4 import BeautifulSoup
def find_if_product_not_available(url):
    # Send a GET request to the URL
    response = requests.get(url)
    # check if requestis 200
    if response.status_code != 200:
        print("Request failed")
        return False

    # Parse the HTML content of the response using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the product availability element using a CSS selector
    availability_element = soup.select_one(".Product__NotAvailableContent")

    # Determine whether the product is not available
    product_not_available = availability_element is not None
    if product_not_available:
        print("Product not available")
        return False
    else:
        return True


scrape_prices()
42/9:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"

            available = find_if_product_not_available(url)

            print("Request successful")
            print(url)
            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                print(url)
                results[url] = {"price": price_text, "available": available}
                print(results)

    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)


from bs4 import BeautifulSoup
def find_if_product_not_available(url):
    # Send a GET request to the URL
    response = requests.get(url)
    # check if requestis 200
    if response.status_code != 200:
        print("Request failed")
        return False

    # Parse the HTML content of the response using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the product availability element using a CSS selector
    availability_element = soup.select_one(".Product__NotAvailableContent")

    # Determine whether the product is not available
    product_not_available = availability_element is not None
    if product_not_available:
        print("Product not available")
        return False
    else:
        return True


scrape_prices()
42/10:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"

            available = find_if_product_not_available(url)

            print("Request successful")

            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {"price": price_text, "available": available}
                print(results)

    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)


from bs4 import BeautifulSoup
def find_if_product_not_available(url):
    # Send a GET request to the URL
    response = requests.get(url)
    # check if requestis 200
    if response.status_code != 200:
        print("Request failed")
        return False

    # Parse the HTML content of the response using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the product availability element using a CSS selector
    availability_element = soup.select_one(".Product__NotAvailableContent")

    # Determine whether the product is not available
    product_not_available = availability_element is not None
    if product_not_available:
        print("Product not available")
        return False
    else:
        return True


scrape_prices()
42/11:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"

            available = find_if_product_not_available(url)

            print("Request successful")

            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {"price": price_text, "available": available}
                print(results[-1])

    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)


from bs4 import BeautifulSoup
def find_if_product_not_available(url):
    # Send a GET request to the URL
    response = requests.get(url)
    # check if requestis 200
    if response.status_code != 200:
        print("Request failed")
        return False

    # Parse the HTML content of the response using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the product availability element using a CSS selector
    availability_element = soup.select_one(".Product__NotAvailableContent")

    # Determine whether the product is not available
    product_not_available = availability_element is not None
    if product_not_available:
        print("Product not available")
        return False
    else:
        return True


scrape_prices()
42/12:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"

            available = find_if_product_not_available(url)

            print("Request successful")

            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {"price": price_text, "available": available}
                #print last element in results
                print(results[url])


    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)


from bs4 import BeautifulSoup
def find_if_product_not_available(url):
    # Send a GET request to the URL
    response = requests.get(url)
    # check if requestis 200
    if response.status_code != 200:
        print("Request failed")
        return False

    # Parse the HTML content of the response using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the product availability element using a CSS selector
    availability_element = soup.select_one(".Product__NotAvailableContent")

    # Determine whether the product is not available
    product_not_available = availability_element is not None
    if product_not_available:
        print("Product not available")
        return False
    else:
        return True


scrape_prices()
42/13:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"


            print("Request successful")

            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {price_text}
                #print last element in results
                print(results[url])


    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)


from bs4 import BeautifulSoup
def find_if_product_not_available(url):
    # Send a GET request to the URL
    response = requests.get(url)
    # check if requestis 200
    if response.status_code != 200:
        print("Request failed")
        return False

    # Parse the HTML content of the response using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the product availability element using a CSS selector
    availability_element = soup.select_one(".Product__NotAvailableContent")

    # Determine whether the product is not available
    product_not_available = availability_element is not None
    if product_not_available:
        print("Product not available")
        return False
    else:
        return True


scrape_prices()
42/14:

def scrape_prices():
    # Define root to product
    base_url = "http://farmasiet.no/catalog/"
    # Load the XML data from the provided link
    url = "https://www.farmasiet.no/googleproductfeed"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        print("Request successful")
        print(url)
        # Parse the XML content from the response
        root = ET.fromstring(response.content)

        # Define the namespace to search for the tags properly
        ns = {"g": "http://base.google.com/ns/1.0"}

        # Create an empty dictionary to store the results
        results = {}

        # Iterate through each 'entry' in the XML
        for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
            # Extract the text within the 'g:item_group_id' tag
            item_group_id = entry.find("g:item_group_id", ns)
            # contructing URL
            url = base_url + item_group_id.text
            # Extract the text within the 'g:price' tag
            price = entry.find("g:price", ns)
            # If the 'price' tag is found, remove ' NOK' from the text
            price_text = price.text.replace(" NOK", "") if price is not None else "N/A"


            print("Request successful")

            # Add the results to the dictionary
            if item_group_id is not None and price is not None:
                results[url] = {price_text}
                #print last element in results
                print(results[url])


    else:
        print("Failed to retrieve the XML file")

    # Return the results
    with open('results.json', 'w') as f:
        json.dump(results, f)



scrape_prices()
42/15:
import requests
import xml.etree.ElementTree as ET

# Define root to product
base_url = "http://farmasiet.no/catalog/"
# Load the XML data from the provided link
url = 'https://www.farmasiet.no/googleproductfeed'

# Send a GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the XML content from the response
    root = ET.fromstring(response.content)

    # Define the namespace to search for the tags properly
    ns = {'g': 'http://base.google.com/ns/1.0'}

    # Create an empty dictionary to store the results
    results = {}

    # Iterate through each 'entry' in the XML
    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
        # Extract the text within the 'g:item_group_id' tag
        item_group_id = entry.find('g:item_group_id', ns)
        #contructing URL
        url = base_url + item_group_id.text

        # Extract the text within the 'g:price' tag
        price = entry.find('g:price', ns)
        # If the 'price' tag is found, remove ' NOK' from the text
        price_text = price.text.replace(' NOK', '') if price is not None else 'N/A'
        
        # Add the results to the dictionary
        if item_group_id is not None and price is not None:
            results[url] = price_text
else:
    print('Failed to retrieve the XML file')
import json
# Print the results
print(results)
with open('results.json', 'w') as f:
    json.dump(results, f)
43/1:
csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)
43/2:
import numpy as np
import pandas as pd
43/3:
csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)
43/4:
df_url.head()
df_tags.head()
43/5:
print(df_url.head())
df_tags.head()
43/6:
print(df_url.columns())
df_tags.head()
43/7:
# print columns pandas
print(df_url.columns)
df_tags.head()
43/8:
# print columns pandas
print(df_url.columns)
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[4]
print(df_url.columns)

df_tags.head()
43/9:
# print columns pandas
print(df_url.columns)
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[1]
print(df_url.columns)

df_tags.head()
43/10:
# print columns pandas
print(df_url.columns)
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[0]
print(df_url.columns)

df_tags.head()
43/11:
# print columns pandas
print(df_url.columns)
print(df_tags.head(2))
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[0]
print(df_url.columns)

df_tags.head()
43/12:
# print columns pandas
print(df_url.columns)

#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[295]
print(df_url.columns)

df_tags.head()
43/13:
# print columns pandas
print(df_url.columns)

#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[4]
print(df_url.columns)

df_tags.head()
43/14:
# print columns pandas
print(df_url.columns)

#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[1]
print(df_url.columns)

df_tags.head()
43/15:
# print columns pandas
print(df_url.columns)

#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[1]
print(df_url.columns)

print(df_url.head())

df_tags.head()
43/16:
# print columns pandas
print(df_url.columns)

#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print(df_url.head())

df_tags.head()
43/17:
# print columns pandas
print(df_url.columns)

#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print(df_url.head())

# df_tags.head()
43/18:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print(df_url.head())

df_tags.head()
43/19:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print(df_url.head())

df_tags.head()
43/20:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print(df_url.head())

df_tags.head()


# add the url column to the df_tags dataframe and join them by porduct name
df_tags['url'] = df_tags['Product Name'].map(df_url.set_index('Product Name')['URL'])
df_tags.head()
43/21:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
df_tags['url'] = df_tags['Product Name'].map(df_url.set_index('Product Name')['URL'])
df_tags.head()
43/22:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[4]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
df_tags['url'] = df_tags['Product Name'].map(df_url.set_index('Product Name')['URL'])
df_tags.head()
43/23:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[3]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
df_tags['url'] = df_tags['Product Name'].map(df_url.set_index('Product Name')['URL'])
df_tags.head()
43/24:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[2]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
df_tags['url'] = df_tags['Product Name'].map(df_url.set_index('Product Name')['URL'])
df_tags.head()
43/25:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[2]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
df_tags['url'] = df_tags['Product Name'].map(df_url.set_index('Product Name')['URL'])
df_tags.head()
43/26:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[2]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
df_tags['url'] = df_tags['Product Name'].map(df_url.set_index('Product Name')['url'])
df_tags.head()
43/27:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[2]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()
43/28:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[2]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'URL']]
43/29:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[2]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/30:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[2]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/31:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[1]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/32:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[1]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/33:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/34:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/35:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[1]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/36:
import numpy as np
import pandas as pd
43/37:
csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)
43/38:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[1]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/39:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[2]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/40:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[1]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/41:
#change rows 4 to be the header with columns
df_url.columns = df_url.iloc[0]
print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/42:
#change rows 4 to be the header with columns

print(df_url.columns)

print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/43: %history -p
43/44:

print(df_tags.columns) 
print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/45:

print(df_tags.columns) 
print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/46:

print(df_tags.head()) 
print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/47:
import numpy as np
import pandas as pd
43/48:
csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)
43/49:

print(df_tags.head()) 
print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/50:

print(df_tags.head()) 
print(df_tags.columns)
print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
df_tags = df_tags[['Product Name', 'url']]
43/51:
import numpy as np
import pandas as pd
43/52:
csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)
43/53:

print(df_tags.head()) 
print(df_tags.columns)
print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
43/54:

print(df_tags.head()) 
df_tags = df_tags.dropna(axis=0, how='all')
print(df_tags.columns)
print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
43/55:

df_tags = df_tags.dropna(axis=0, how='all')
print(df_tags.columns)
print("__________________________")

df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
43/56:

df_tags = df_tags.dropna(axis=0, how='all')
print(df_tags.columns)
print("__________________________")


print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
43/57:

df_tags = df_tags.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")


print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
43/58:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
43/59:
import numpy as np
import pandas as pd
43/60:
csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)
43/61:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
df_tags = df_tags.merge(df_url, on='Product Name', how='left')
df_tags.head()

# print only name and url columns
43/62:
import numpy as np
import pandas as pd
43/63:
csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)
43/64:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

print(df_tags.columns)


# add the url column to the df_tags dataframe and join them by porduct name
# Merge df_tags and df_url on 'Product Name' using a left join
# df_tags = df_tags.merge(df_url, on='Product Name', how='left')
# df_tags.head()

# print only name and url columns
43/65:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

print(df_tags.columns)


# Merge df_tags and df_url on 'Product Name' using a left join, only adding the 'url' column
df_tags = df_tags.merge(df_url[['Product Name', 'url']], on='Product Name', how='left')
# print only name and url columns
43/66:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

# set row to as header
df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# Merge df_tags and df_url on 'Product Name' using a left join, only adding the 'url' column
df_tags = df_tags.merge(df_url[['Product Name', 'url']], on='Product Name', how='left')
# print only name and url columns
43/67:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

# set row to as header
df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# Merge df_tags and df_url on 'Product Name' using a left join, only adding the 'url' column
df_tags = df_tags.merge(df_url[['Product Name', 'url']], on='Product Name', how='left')
# print only name and url columns
print(df_tags[['Product Name', 'url']])
43/68:
import numpy as np
import pandas as pd
import pprint
43/69:
import numpy as np
import pandas as pd
import pprint
pp = pprint.PrettyPrinter(indent=4)
43/70:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

# set row to as header
df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# Merge df_tags and df_url on 'Product Name' using a left join, only adding the 'url' column
df_tags = df_tags.merge(df_url[['Product Name', 'url']], on='Product Name', how='left')
# print only name and url columns
pp.print(df_tags[['Product Name', 'url']])
43/71:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

# set row to as header
df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# Merge df_tags and df_url on 'Product Name' using a left join, only adding the 'url' column
df_tags = df_tags.merge(df_url[['Product Name', 'url']], on='Product Name', how='left')
# print only name and url columns
print(df_tags[['Product Name', 'url']])
43/72:
csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)
43/73:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

# set row to as header
df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# Merge df_tags and df_url on 'Product Name' using a left join, only adding the 'url' column
df_tags = df_tags.merge(df_url[['url']], on='Product Name', how='left')
# print only name and url columns
print(df_tags[['Product Name', 'url']])
43/74:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

# set row to as header
df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# Merge df_tags and df_url on 'Product Name', only adding the 'url' column
df_tags = df_tags.merge(df_url[['Product Name', 'url']], on='Product Name', how='inner')
# print only name and url columns
print(df_tags[['Product Name', 'url']])
43/75:
import numpy as np
import pandas as pd
import pprint
43/76:

df_url = df_url.dropna(axis=0, how='all')
print(df_url.columns)
print("__________________________")

df_tags = df_tags.dropna(axis=0, how='all')

# set row to as header
df_tags.columns = df_tags.iloc[0]
print(df_tags.columns)


# Merge df_tags and df_url on 'Product Name', only adding the 'url' column
df_tags = df_tags.merge(df_url[['Product Name', 'url']], on='Product Name', how='inner')
# print only name and url columns
print(df_tags[['Product Name', 'url']])
44/1:
import numpy as np
import pandas as pd
import streamlit as st
import openai
import requests
from bs4 import BeautifulSoup

# streamlit config for wide mode
st.set_page_config(layout="wide")


csv_url_path = "./data/Products and Categories.xlsx"
tags_url_path = "./data/dummy - Product category and tags (1).xlsx"

df_url = pd.read_excel(csv_url_path)
df_tags = pd.read_excel(tags_url_path)


df_url = df_url.dropna(axis=0, how="all")
df_tags = df_tags.dropna(axis=0, how="all")

# set row to as header
df_tags.columns = df_tags.iloc[0]

# Merge df_tags and df_url on 'Product Name', only adding the 'url' column
df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="inner")
# print only name and url columns
print(df_tags[["Product Name", "url"]])

st.dataframe(df_tags[["Product Name", "url"]], use_container_width=True)

# drop columns nan, Technology, General, Related products, Varient products
df_tags = df_tags.drop(
    columns=[
        "Technology",
        "General",
        "Related products",
        "Variant products",
        "Contains / made from products",
    ],
    axis=1,
)


def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    for tag in ["header", "nav", "aside", "footer"]:
        for element in soup.find_all(tag):
            element.extract()

    # Remove script and style elements
    for tag in ["script", "style"]:
        for element in soup.find_all(tag):
            element.extract()

    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

    text = "\n".join(chunk for chunk in chunks if chunk)

    return text


def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.7,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)


# add the string "https://www.kongsberg.com/" to the url column for each url
df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)


# add column for new tags
df_tags["new_tags"] = ""


# # read the website and return the text from the website
# st.write(
#     general_gpt(
#         "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"
#     )
# )

# st.write(
#     get_text_from_url(
#         "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"
#     )
# )


# url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"
# product_name = "K-Sync-Syncronization unit"
# product_category = "Autonomous and uncrewed solutions"


# prompt = "Based on this text {}, write five simpel product tags that can be used to describe this and similar products: {} in this product category {}. Write the tags with comma as delimiter".format(
#     get_text_from_url(url), product_name, product_category
# )

# use the general gpt function to generate tags for the product. The prompt is the text from the website and the product name and category

# Iterate over the rows of the dataframe
for index, row in df_tags.head(10).iterrows():
    # Create a prompt using the product name, category, and the text from the website
    url = row["url"]
    product_name = row["Product Name"]
    product_category = row[
        "Product category"
    ]  # Assuming the column name for product category
    website_text = get_text_from_url(url)
    prompt = f"Based on this text {website_text}, write five simple product tags that can be used to describe this and similar products: {product_name} in this product category {product_category}. Do not use the product name as tag, and do not use the product cateogory direct as tags. Write the tags with comma as delimiter."

    # Call the general_gpt function with this prompt
    tags = general_gpt(prompt)

    # Save the generated tags in the 'new_tags' column
    df_tags.loc[index, "new_tags"] = tags

# st.write(general_gpt(prompt))
44/2: print(df_tags[["Product Name", "url", "new_tags"]])
44/3:
# read df new_tags_and_columns.csv
df_new_tags = pd.read_csv("./data/new_tags_and_columns.csv")
# save df_new_tags as xlsx
df_new_tags.to_excel("./data/new_tags_and_columns.xlsx")
44/4:
# read df new_tags_and_columns.csv

df_new_tags = pd.read_csv("./data/new_tags_columns_df.csv")
# save df_new_tags as xlsx
df_new_tags.to_excel("./data/new_tags_and_columns.xlsx")
44/5:
# read df new_tags_and_columns.csv

df_new_tags = pd.read_csv("./data/new_tags_columns_df.csv")

#only use the columns Product Name, product category, url, app_use, tech, general
df_new_tags = df_new_tags[
    ["Product Name", "Product category", "app_use", "tech", "general"]
]
# save df_new_tags as xlsx
df_new_tags.to_excel("./data/new_tags_and_columns.xlsx")
44/6:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    for tag in ["header", "nav", "aside", "footer"]:
        for element in soup.find_all(tag):
            element.extract()

    # Remove script and style elements
    for tag in ["script", "style"]:
        for element in soup.find_all(tag):
            element.extract()

    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

    text = "\n".join(chunk for chunk in chunks if chunk)

    return text


url_text_dict = {}
for index, row in df_new_tags.iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
import json

with open("./data/url_text_dict.json", "w") as fp:
    json.dump(url_text_dict, fp)
44/7:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    for tag in ["header", "nav", "aside", "footer"]:
        for element in soup.find_all(tag):
            element.extract()

    # Remove script and style elements
    for tag in ["script", "style"]:
        for element in soup.find_all(tag):
            element.extract()

    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

    text = "\n".join(chunk for chunk in chunks if chunk)

    return text


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
import json

with open("./data/url_text_dict.json", "w") as fp:
    json.dump(url_text_dict, fp)
44/8:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    for tag in ["header", "nav", "aside", "footer"]:
        for element in soup.find_all(tag):
            element.extract()

    # Remove script and style elements
    for tag in ["script", "style"]:
        for element in soup.find_all(tag):
            element.extract()

    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

    text = "\n".join(chunk for chunk in chunks if chunk)

    return text


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
44/9:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    for tag in ["header", "nav", "aside", "footer"]:
        for element in soup.find_all(tag):
            element.extract()

    # Remove script and style elements
    for tag in ["script", "style"]:
        for element in soup.find_all(tag):
            element.extract()

    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

    text = "\n".join(chunk for chunk in chunks if chunk)

    return text


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
44/10:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    for tag in ["header", "nav", "aside", "footer"]:
        for element in soup.find_all(tag):
            element.extract()

    # Remove script and style elements
    for tag in ["script", "style"]:
        for element in soup.find_all(tag):
            element.extract()

    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

    text = "\n".join(chunk for chunk in chunks if chunk)

    return text


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
44/11:
# save the dict as pickle file
import pickle
with open('./data/url_text_dict.pickle', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
44/12:
# save the dict as pickle file
import pickle
with open('./data/url_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
44/13:
# read df new_tags_and_columns.csv

df_new_tags = pd.read_csv("./data/new_tags_columns_df.csv")

#only use the columns Product Name, product category, url, app_use, tech, general
df_new_tags = df_new_tags[
    ["Product Name", "Product category", "app_use", "tech", "general"]
]
# save df_new_tags as xlsx
df_new_tags.to_excel("./data/new_tags_and_columns.xlsx")
44/14:
# read df new_tags_and_columns.csv

df_new_tags = pd.read_csv("./data/new_tags_columns_df.csv")

#only use the columns Product Name, product category, url, app_use, tech, general
df_new_tags = df_new_tags[
    ["Product Name", "Product category", "app_use", "tech", "general, url"]
]
# save df_new_tags as xlsx
df_new_tags.to_excel("./data/new_tags_and_columns.xlsx")
45/1:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/2:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/3:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/4:
import pandas as pd
import numpy as np
45/5:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/6:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/7: print(df_tags.head())
45/8:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com/discovery" + df_tags["url"].astype(str)

    return df_tags
45/9:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/10: print(df_tags.head())
45/11:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com/discovery/products" + df_tags["url"].astype(str)

    return df_tags
45/12:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/13: print(df_tags.head())
45/14: print(df_tags["url"].head())
45/15: print(df_tags["url"].head(2))
45/16:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/17:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/18: print(df_tags["url"].head(2))
45/19: print(df_tags["url"][0])
45/20: print(df_tags["url"][2])
45/21: print(df_tags["url"][3])
45/22:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Extract headers
    headers = [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]

    # Extract bullet points
    bullet_points = [li.get_text() for li in soup.find_all('li')]

    # Combine headers and bullet points into one string
    text = '\n'.join(headers + bullet_points)

    return text


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
45/23:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Extract headers
    headers = [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]

    # Extract bullet points
    bullet_points = [li.get_text() for li in soup.find_all('li')]

    # Combine headers and bullet points into one string
    text = '\n'.join(headers + bullet_points)

    return text


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict["url"])
45/24:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Extract headers
    headers = [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]

    # Extract bullet points
    bullet_points = [li.get_text() for li in soup.find_all('li')]

    # Combine headers and bullet points into one string
    text = '\n'.join(headers + bullet_points)

    return text


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())
45/25:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Extract headers
    headers = [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]

    # Extract bullet points
    bullet_points = [li.get_text() for li in soup.find_all('li')]

    # Combine headers and bullet points into one string
    text = '\n'.join(headers + bullet_points)

    return text


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/26:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Extract headers
    headers = [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]

    # Extract bullet points
    bullet_points = [li.get_text() for li in soup.find_all('li')]

    # Combine headers and bullet points into one string
    text = '\n'.join(headers + bullet_points)

    # Find the keyword in the text and return only the text that comes after it
    keyword = 'features'
    try:
        start = text.lower().index(keyword) + len(keyword)
        return text[start:]
    except ValueError:
        return ""


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/27:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Extract headers
    headers = [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]

    # Extract bullet points
    bullet_points = [li.get_text() for li in soup.find_all('li')]

    # Combine headers and bullet points into one string
    text = '\n'.join(headers + bullet_points)

    return text


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/28:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Find all headers from h2 to h5
    headers = soup.find_all(['h2', 'h3', 'h4', 'h5'])

    text_sections = []

    for header in headers:
        # Get the next sibling elements until the next header or the end of the section
        next_siblings = []
        for sibling in header.next_siblings:
            if sibling.name and sibling.name.startswith('h'):
                break
            next_siblings.append(sibling)

        # Extract text from the sibling elements
        text = ' '.join(sibling.get_text(strip=True) for sibling in next_siblings if sibling.get_text(strip=True))

        # Add the header and the text under it to the list of text sections
        text_sections.append(f"{header.get_text(strip=True)}\n{text}")

    return '\n\n'.join(text_sections)


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/29:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests
def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Find all headers from h2 to h5
    headers = soup.find_all(['h2', 'h3', 'h4', 'h5'])

    text_sections = []

    for header in headers:
        # Get the next sibling elements until the next header or the end of the section
        next_siblings = []
        for sibling in header.next_siblings:
            if sibling.name and sibling.name.startswith('h'):
                break
            next_siblings.append(sibling)

        # Extract text from the sibling elements and split it on new lines
        text = '\n'.join(sibling.get_text(strip=True).split('\n') for sibling in next_siblings if sibling.get_text(strip=True))

        # Add the header and the text under it to the list of text sections
        text_sections.append(f"{header.get_text(strip=True)}\n{text}")

    return '\n\n'.join(text_sections)

url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/30:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests
def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Find all headers from h2 to h5
    headers = soup.find_all(['h2', 'h3', 'h4', 'h5'])

    text_sections = []

    for header in headers:
        # Get the next sibling elements until the next header or the end of the section
        next_siblings = []
        for sibling in header.next_siblings:
            if sibling.name and sibling.name.startswith('h'):
                break
            next_siblings.append(sibling)

        # Extract text from the sibling elements and split it on new lines
        text = '\n'.join(text for sibling in next_siblings if sibling.get_text(strip=True) for text in sibling.get_text(strip=True).split('\n'))
        # Add the header and the text under it to the list of text sections
        text_sections.append(f"{header.get_text(strip=True)}\n{text}")

    return '\n\n'.join(text_sections)

url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/31:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests
import re

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Find all headers from h2 to h5
    headers = soup.find_all(['h2', 'h3', 'h4', 'h5'])

    text_sections = []

    for header in headers:
        # Get the next sibling elements until the next header or the end of the section
        next_siblings = []
        for sibling in header.next_siblings:
            if sibling.name and sibling.name.startswith('h'):
                break
            next_siblings.append(sibling)

        # Extract text from the sibling elements and split it into bullet points
        text = ' '.join(sibling.get_text(strip=True) for sibling in next_siblings if sibling.get_text(strip=True))
        bullet_points = re.split(r'(?<=[a-z])\s+(?=[A-Z])', text)

        # Add the header and the bullet points under it to the list of text sections
        text_sections.append(f"{header.get_text(strip=True)}\n{'\n'.join(bullet_points)}")

    return '\n\n'.join(text_sections)

url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/32:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests
import re

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Find all headers from h2 to h5
    headers = soup.find_all(['h2', 'h3', 'h4', 'h5'])

    text_sections = []

    for header in headers:
        # Get the next sibling elements until the next header or the end of the section
        next_siblings = []
        for sibling in header.next_siblings:
            if sibling.name and sibling.name.startswith('h'):
                break
            next_siblings.append(sibling)

        # Extract text from the sibling elements and split it into bullet points
        text = ' '.join(sibling.get_text(strip=True) for sibling in next_siblings if sibling.get_text(strip=True))
        bullet_points = re.split(r'(?<=[a-z])\s+(?=[A-Z])', text)

        # Add the header and the bullet points under it to the list of text sections
        text_sections.append(f"{header.get_text(strip=True)}" + "\n" + "\n".join(bullet_points))
    return '\n\n'.join(text_sections)

url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/33:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests
import re

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Find all headers from h2 to h5 and tables
    elements = soup.find_all(['h2', 'h3', 'h4', 'h5', 'table'])

    text_sections = []

    for element in elements:
        if element.name.startswith('h'):
            # Get the next sibling elements until the next header or the end of the section
            next_siblings = []
            for sibling in element.next_siblings:
                if sibling.name and (sibling.name.startswith('h') or sibling.name == 'table'):
                    break
                next_siblings.append(sibling)

            # Extract text from the sibling elements and split it into bullet points
            text = ' '.join(sibling.get_text(strip=True) for sibling in next_siblings if sibling.get_text(strip=True))
            bullet_points = re.split(r'(?<=[a-z])\s+(?=[A-Z])', text)

            # Add the header and the bullet points under it to the list of text sections
            text_sections.append(f"{element.get_text(strip=True)}" + "\n" + "\n".join(bullet_points))
        elif element.name == 'table':
            # Extract text from each cell in the table
            table_text = '\n'.join(cell.get_text(strip=True) for cell in element.find_all('td'))

            # Add the table text to the list of text sections
            text_sections.append(table_text)

    return '\n\n'.join(text_sections)

url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
45/34:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests
import re

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Find all headers from h2 to h5 and tables
    elements = soup.find_all(['h2', 'h3', 'h4', 'h5', 'table'])

    text_sections = []

    for element in elements:
        if element.name.startswith('h'):
            # Get the next sibling elements until the next header or the end of the section
            next_siblings = []
            for sibling in element.next_siblings:
                if sibling.name and (sibling.name.startswith('h') or sibling.name == 'table'):
                    break
                next_siblings.append(sibling)

            # Extract text from the sibling elements and split it into bullet points
            text = ' '.join(sibling.get_text(strip=True) for sibling in next_siblings if sibling.get_text(strip=True))
            bullet_points = re.split(r'(?<=[a-z])\s+(?=[A-Z])', text)

            # Add the header and the bullet points under it to the list of text sections
            text_sections.append(f"{element.get_text(strip=True)}" + "\n" + "\n".join(bullet_points))
        elif element.name == 'table':
            # Extract text from each cell in the table
            table_text = '\n'.join(cell.get_text(strip=True) for cell in element.find_all('td'))

            # Add the table text to the list of text sections
            text_sections.append(table_text)

    return '\n\n'.join(text_sections)

url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
# print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
print(url_text_dict["https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/transponders/cnode-transponders-for-hipap-and-uPAP/cNODE-Midi/"])
45/35:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  
from bs4 import BeautifulSoup
import requests
import re

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")

    # Find all headers from h2 to h5 and tables
    elements = soup.find_all(['h2', 'h3', 'h4', 'h5', 'table'])

    text_sections = []

    for element in elements:
        if element.name.startswith('h'):
            # Get the next sibling elements until the next header or the end of the section
            next_siblings = []
            for sibling in element.next_siblings:
                if sibling.name and (sibling.name.startswith('h') or sibling.name == 'table'):
                    break
                next_siblings.append(sibling)

            # Extract text from the sibling elements and split it into bullet points
            text = ' '.join(sibling.get_text(strip=True) for sibling in next_siblings if sibling.get_text(strip=True))
            bullet_points = re.split(r'(?<=[a-z])\s+(?=[A-Z])', text)

            # Add the header and the bullet points under it to the list of text sections
            text_sections.append(f"{element.get_text(strip=True)}" + "\n" + "\n".join(bullet_points))
        elif element.name == 'table':
            # Extract text from each cell in the table
            table_text = '\n'.join(cell.get_text(strip=True) for cell in element.find_all('td'))

            # Add the table text to the list of text sections
            text_sections.append(table_text)

    return '\n\n'.join(text_sections)

url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = get_text_from_url(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())

#print key 1 and value 1
# print(url_text_dict["https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/"])
print(url_text_dict["https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/transponders/cnode-transponders-for-hipap-and-uPAP/cNODE-Embed/"])
44/15:
# read df new_tags_and_columns.csv

df_new_tags = pd.read_csv("./data/new_tags_columns_df.csv")

#only use the columns Product Name, product category, url, app_use, tech, general
df_new_tags = df_new_tags[
    ["Product Name", "Product category", "app_use", "tech", "general"]
]
# save df_new_tags as xlsx
df_new_tags.to_excel("./data/third_draft.xlsx")
45/36:
url_text_dict

import re

# Create a new dictionary to store the extracted features
features_dict = {}

# Define a regular expression pattern to match the specifications or features section
pattern = re.compile(r'(specifications|features).*', re.IGNORECASE)

# Iterate over the URLs in the url_text_dict
for url, text in url_text_dict.items():
    # Search for the specifications or features section in the text
    match = pattern.search(text)

    # If a match was found, add it to the features_dict
    if match is not None:
        features_dict[url] = match.group()

# Now features_dict contains the specifications or features section for each URL

print(features_dict)
45/37:
url_text_dict

import re

# Create a new dictionary to store the extracted features
features_dict = {}

# Define a regular expression pattern to match the specifications or features section
pattern = re.compile(r'(specifications|features).*', re.IGNORECASE)

# Iterate over the URLs in the url_text_dict
for url, text in url_text_dict.items():
    # Search for the specifications or features section in the text
    match = pattern.search(text)

    # If a match was found, add it to the features_dict
    if match is not None:
        features_dict[url] = match.group()
        print("match")

# Now features_dict contains the specifications or features section for each URL

print(features_dict)
45/38:
url_text_dict
import re

# Create a new dictionary to store the extracted features
features_dict = {}

# Define a regular expression pattern to match the specifications or features section
pattern = re.compile(r'(specifications|features)(.*)', re.IGNORECASE | re.DOTALL)

# Iterate over the URLs in the url_text_dict
for url, text in url_text_dict.items():
    # Search for the specifications or features section in the text
    match = pattern.search(text)

    # If a match was found, add it to the features_dict
    if match is not None:
        features_dict[url] = match.group(2).strip()  # group(2) to get the text after "specifications" or "features"

# Now features_dict contains the specifications or features section for each URL

# Now features_dict contains the specifications or features section for each URL

print(features_dict)
45/39:
url_text_dict
import re

# Create a new dictionary to store the extracted features
features_dict = {}

# Define a regular expression pattern to match the specifications or features section
pattern = re.compile(r'(specifications|features)(.*)', re.IGNORECASE | re.DOTALL)

# Iterate over the URLs in the url_text_dict
for url, text in url_text_dict.items():
    # Search for the specifications or features section in the text
    match = pattern.search(text)

    # If a match was found, add it to the features_dict
    if match is not None:
        features_dict[url] = match.group(2).strip()  # group(2) to get the text after "specifications" or "features"

# Now features_dict contains the specifications or features section for each URL

# Now features_dict contains the specifications or features section for each URL

print(features_dict)
for i in features_dict:
    print(i)
    print(features_dict[i])
45/40:
url_text_dict
import re

# Create a new dictionary to store the extracted features
import re

# Create a new dictionary to store the extracted features
features_dict = {}

# Define a regular expression pattern to match the specifications or features section
pattern = re.compile(r'(specifications|features)([^.]*)(\.|\n|$)', re.IGNORECASE)

# Iterate over the URLs in the url_text_dict
for url, text in url_text_dict.items():
    # Search for the specifications or features section in the text
    match = pattern.search(text)

    # If a match was found, add it to the features_dict
    if match is not None:
        features_dict[url] = match.group(2).strip()  # group(2) to get the text after "specifications" or "features"

# Now features_dict contains the specifications or features section for each URL
# Now features_dict contains the specifications or features section for each URL

# Now features_dict contains the specifications or features section for each URL

print(features_dict)
for i in features_dict:
    print(i)
    print(features_dict[i])
45/41:
from bs4 import BeautifulSoup
import requests

# Get the HTML of the page
url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/#technicalInformation"
response = requests.get(url)
html = response.text

# Parse the HTML with BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')

# Find the link
link = soup.find('a', href='#technicalInformation')

# If the link was found, find the element it links to
if link is not None:
    target_id = link['href'].lstrip('#')
    target_element = soup.find(id=target_id)

    # If the target element was found, get its text
    if target_element is not None:
        text = target_element.get_text(strip=True)
        print(text)
45/42:
from bs4 import BeautifulSoup
import requests

# Get the HTML of the page
url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/k-sync/#technicalInformation"
response = requests.get(url)
html = response.text

# Parse the HTML with BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')

# Find the link
link = soup.find('a', href='#technicalInformation')

# If the link was found, find the element it links to
if link is not None:
    target_id = link['href'].lstrip('#')
    target_element = soup.find(id=target_id)

    # If the target element was found, get its text
    if target_element is not None:
        # Find all the list items in the target element
        list_items = target_element.find_all('li')

        # Extract the text of each list item
        bullet_points = [li.get_text(strip=True) for li in list_items]

        print(bullet_points)
45/43:
from bs4 import BeautifulSoup
import requests

# Get the HTML of the page
url = "https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/transponders/cnode-transponders-for-hipap-and-uPAP/cNODE-Micro/"
response = requests.get(url)
html = response.text

# Parse the HTML with BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')

# Find the link
link = soup.find('a', href='#technicalInformation')

# If the link was found, find the element it links to
if link is not None:
    target_id = link['href'].lstrip('#')
    target_element = soup.find(id=target_id)

    # If the target element was found, get its text
    if target_element is not None:
        # Find all the list items in the target element
        list_items = target_element.find_all('li')

        # Extract the text of each list item
        bullet_points = [li.get_text(strip=True) for li in list_items]

        print(bullet_points)
45/44:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  

from bs4 import BeautifulSoup
import requests

def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.keys())
45/45:
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  

from bs4 import BeautifulSoup
import requests

def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict.values())
45/46:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
45/47:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification  that can be used to describe this and similar products: {product_name} in this product category {product_category}. Do not use the product name as tag, and do not use the product cateogory direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags.csv", index=False)
45/48:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/49:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
45/50: print(url_text_dict["https://www.kongsberg.com/maritime/products/propulsion/propellers/controllable-pitch-propellers/controllable-pitch-propellers/"])
45/51: print(url_text_dict)
45/52:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification  that can be used to describe this and similar products: {product_name} in this product category {product_category}. Do not use the product name as tag, and do not use the product cateogory direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags.csv", index=False)
45/53:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/54:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/55:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/56:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/57:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification  that can be used to describe this and similar products: {product_name} in this product category {product_category}. Do not use the product name as tag, and do not use the product cateogory direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags.csv", index=False)
45/58:
import pandas as pd
import numpy as np
import streamlit as st
45/59:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification  that can be used to describe this and similar products: {product_name} in this product category {product_category}. Do not use the product name as tag, and do not use the product cateogory direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags.csv", index=False)
45/60:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/61:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification  that can be used to describe this and similar products: {product_name} in this product category {product_category}. Do not use the product name as tag, and do not use the product cateogory direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags.csv", index=False)
45/62:
import pandas as pd
import numpy as np
import streamlit as st
import openai
45/63:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification  that can be used to describe this and similar products: {product_name} in this product category {product_category}. Do not use the product name as tag, and do not use the product cateogory direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags.csv", index=False)
45/64:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
45/65:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification  that can be used to describe this and similar products: {product_name} in this product category {product_category}. Do not use the product name as tag, and do not use the product cateogory direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags.csv", index=False)
45/66:
 prompt = "Here is a list of tags: {}. Selsect 15 tags that can be used to describe the technology and the technical aspects of multiple products.  Return the tags in list format with comma as delimiter.".format(
        all_tags
    )
tags = general_gpt(prompt)
45/67:
tags = df_tags_tech["new_tags"]
prompt = "Here is a list of tags: {}. Selsect 15 tags that can be used to describe the technology and the technical aspects of multiple products.  Return the tags in list format with comma as delimiter.".format(
        tags
    )
tags = general_gpt(prompt)
45/68: print(tags)
45/69:
tags = df_tags_tech["new_tags"]
prompt = "Here is a list of tags: {}. Selsect 15 tags that can be used to describe the technology and the technical aspects of multiple products.  Return the tags in list format with comma as delimiter. And return a explanation as a paragraph why you choose these tags.".format(
        tags
    )
tags = general_gpt(prompt)
45/70:
tags = df_tags_tech["new_tags"]
prompt = "Here is a list of tags: {}. Selsect 15 tags that can be used to describe the technology and the technical aspects of multiple products.  Return the tags in list format with comma as delimiter. And return a explanation as a paragraph why you choose these tags.".format(
        tags
    )
tags = general_gpt(prompt)
45/71: print(tags)
45/72:
tags = df_tags_tech["new_tags"]
products = df_tags_tech["Product Name"]
prompt = "Here is a list of tags: {} and a list of products {}. Select 15 tags that can be used to describe the technology and the technical aspects of multiple products.  Return the tags in list format with comma as delimiter. And return a explanation as a paragraph why you choose these tags.".format(
        tags, products
    )
tags = general_gpt(prompt)
45/73: print(tags)
45/74:
tags = df_tags_tech["new_tags"]
products = df_tags_tech["Product Name"]
prompt = "Here is a list of tags: {} and a list of products {}. Select 20 tags that can be used to describe the technology and the technical aspects of multiple products.  Return the tags in list format with comma as delimiter. And return a explanation as a paragraph why you choose these tags.".format(
        tags, products
    )
tags = general_gpt(prompt)
45/75: print(tags)
45/76:
tags = df_tags_tech["new_tags"]
products = df_tags_tech["Product Name"]
prompt = "Here is a list of tags: {} and a list of products {}. Select 20 tags that can be used to describe the technology and the technical aspects of multiple products.  Return the tags in list format with comma as delimiter. And return a explanation as a paragraph why you choose these tags with whick product in mind.".format(
        tags, products
    )
tags = general_gpt(prompt)
45/77: print(tags)
45/78:
tags = df_tags_tech["new_tags"]
products = df_tags_tech["Product Name"]
prompt = "Given the list of tags: {} and the list of products: {}, please select 20 tags that best describe the technology and technical aspects of various products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(        
    tags, products
    )
tags = general_gpt(prompt)
45/79: print(tags)
45/80:
tags = df_tags_tech["new_tags"]
products = df_tags_tech["Product Name"]
prompt = "Given the list of tags: {} and the list of products: {}, please select 20 tags that best describe the technology and technical aspects of various products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(        
    tags, products
    )
tags = general_gpt(prompt)
45/81: print(tags)
45/82:
tags = df_tags_tech["new_tags"]
products = df_tags_tech["Product Name"]
prompt = "Given the list of tags: {} and the list of products: {}, please select 20 tags that best describe the technology and technical aspects of various products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(        
    tags, products
    )
tags = general_gpt(prompt)
45/83: print(tags)
45/84:
tags = df_tags_tech["new_tags"]
products = df_tags_tech["Product Name"]
prompt = "Given the list of tags: {} and the list of products: {}, please select 20 tags that best describe the technology and technical aspects of various products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(        
    tags, products
    )
tags = general_gpt(prompt)
45/85: print(tags)
45/86:
tags = df_tags_tech["new_tags"]
products = df_tags_tech["Product Name"]
prompt = "Given the list of tags: {} and the list of products: {}, please select 20 tags that best describe the technical aspects of various products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(        
    tags, products
    )
tags = general_gpt(prompt)
45/87: print(tags)
45/88: print(len(df_tags_tech))
45/89:
print(len(df_tags_tech))
print(len(df_tags_tech["new_tags"]))
45/90:
print(len(df_tags_tech))
print(len(df_tags_tech["new_tags"]))
print(df_tags_tech["new_tags"][0])
45/91:
print(len(df_tags_tech))
print(len(df_tags_tech["new_tags"]))
print(df_tags_tech["new_tags"][0])
split_tags = df_tags_tech["new_tags"][0].split(",")
45/92:
print(len(df_tags_tech))
print(len(df_tags_tech["new_tags"]))
print(df_tags_tech["new_tags"][0])
split_tags = df_tags_tech["new_tags"][0].split(",")
print(split_tags)
45/93:
#Split all the tags into a list
tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tag_list.append(tags)

print(tag_list)
45/94:
#Split all the tags into a list
tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    tag_list.append(tags)

print(tag_list)
45/95:
#Split all the tags into a list
tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)

print(tag_list)
45/96:
#Split all the tags into a list
tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)

print(tag_list)
print(len(tag_list))
45/97:
#Split all the tags into a list
tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    tags = list(set(tags))
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)

print(tag_list)
print(len(tag_list))
45/98:
#Split all the tags into a list
tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    tags = list(set(tags))
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)

print(tag_list)
print(len(tag_list))
45/99:
#Split all the tags into a list
tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    tags = list(set(tags))
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)

print(tag_list)
print(len(tag_list))
45/100:
#Split all the tags into a list
tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)

for i in tag_list:
    print(i)
print(len(tag_list))
45/101:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/102:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/103:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/104:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/105: %history -p
45/106:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)

for i in tag_list:
    print(i)
print(len(tag_list))
45/107:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/108:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/109:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/110:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/111:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/112:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/113:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)
    

for i in tag_list:
    print(i)
print(len(tag_list))
45/114:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove length over 5 tags
    tags = [tag for tag in tags if len(tag) <= 5]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)
    

for i in tag_list:
    print(i)
print(len(tag_list))
45/115:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove length over 5 tags
    tags = [tag for tag in tags if len(tags) <= 5]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)
    

for i in tag_list:
    print(i)
print(len(tag_list))
45/116:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove length over 5 tags
    tags = [tag for tag in tags if len(tags) <= 5]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/117:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove length over 5 tags
    tags = [tag for tag in tags if len(tags) <= 4]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/118:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove length over 5 tags
    tags = [tag for tag in tags if len(tags) <= 6]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/119:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    if len(tags) < 5:
    # remove duplicates
    # add all tags into one big list
        for tag in tags:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/120:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    if len(tags) < 2:
    # remove duplicates
    # add all tags into one big list
        for tag in tags:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/121:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    print(len(tags))
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/122:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) > 5:
            for tag in tag_split:
                tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/123:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 5:
            for tag in tag_split:
                tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/124:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 5:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/125:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 4:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/126:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/127:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/128:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/129:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/130:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list. Additionally, provide a paragraph explaining your tag choices and the specific products you had in mind while selecting these tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/131:
tags = tag_list
categories = df_tags_tech["Product category"].unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/132: print(tags_dict)
45/133: print(tags_dict["Autonomous and uncrewed solutions"])
45/134:
tags = tag_list
categories = df_tags_tech["Product category"].unique()
categories = [categories.split(",") for categories in categories]
categories = [category.strip() for category in categories]
categories = categories.unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/135:
tags = tag_list
categories = df_tags_tech["Product category"].unique()
# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
categories = [category.strip() for category in categories]
categories = categories.unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/136:
tags = tag_list
categories = df_tags_tech["Product category"].unique()
print(categories)
# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
categories = [category.strip() for category in categories]
categories = categories.unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/137:
tags = tag_list
categories = df_tags_tech["Product category"].unique()
print(categories)

# split the categories that have multiple categories with comma
categories = [str(category).split(",") for category in categories if isinstance(category, str)]
categories = [category.strip() for category in categories]
categories = categories.unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/138:
tags = tag_list
categories = df_tags_tech["Product category"].unique()
for category in categories:
    print(category)
    category.split(",")
    category.strip()
categories = [category.strip() for category in categories]
categories = categories.unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/139:
tags = tag_list
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
categories = [category.strip() for category in categories]
categories = categories.unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/140:
tags = tag_list
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
categories = categories.unique()

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/141:
tags = tag_list
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
print(categories)

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/142:
tags = tag_list
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
print(categories)

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/143: print(tags_dict["Autonomous and uncrewed solutions"])
45/144:
def classify_products(df, tags_dict):
    # Create a new column to store the tags for each product
    df["tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df.iterrows():

        for category, tags in tags_dict.items():
            # Get the products in the current category
            products = df[df["Product category"] == category]["Product Name"]
            print("products", products)
45/145:
def classify_products(df, tags_dict):
    # Create a new column to store the tags for each product
    df["tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df.iterrows():
        product_category = row["Product category"]
        # Get the tags for the product category
        tags = tags_dict[product_category]
        print(tags)
        print(product_category)
45/146:

# Create a new column to store the tags for each product


# Iterate over the rows of the dataframe
for index, row in df.iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]
    print(tags)
    print(product_category)
45/147:

# Create a new column to store the tags for each product
df = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df.iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]
    print(tags)
    print(product_category)
45/148:

# Create a new column to store the tags for each product
df = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df.iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]
    print(type(tags))
    # choose 5 tags from the tags list for each product
45/149:

# Create a new column to store the tags for each product
df = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df.iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]
    print(tags)
    # choose 5 tags from the tags list for each product
45/150:

# Create a new column to store the tags for each product
df = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df.iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that best describe the technical aspects of this product. Return the selected tags as a comma-separated list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df.loc[index, "new_tags"] = tags
    print(tags)
45/151:

# Create a new column to store the tags for each product
df = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that best describe the technical aspects of this product. Return the selected tags as a comma-separated list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df.loc[index, "new_tags"] = tags
print(df.head(10))
45/152:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/153: print(tags_dict["Autonomous and uncrewed solutions"])
45/154:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}:. please select 5 high-level tags that best describe the technical aspects of these products. For each high-level tag, select 5 sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/155: print(tags_dict["Autonomous and uncrewed solutions"])
45/156:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}:. please select 5 high-level tags that best describe the technical aspects of these products. For each high-level tag, select 5 sub-tags. Return the tags as a dictionary where the keys are the high-level tags name and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/157: print(tags_dict["Autonomous and uncrewed solutions"])
45/158: print(tags_dict["Autonomous and uncrewed solutions"])
45/159:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/160:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/161: print(tags_dict["Autonomous and uncrewed solutions"])
45/162:

# Create a new column to store the tags for each product
df = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Return the selected tags as a comma-separated list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df.loc[index, "new_tags"] = tags
print(df.head(10))
45/163:

# Create a new column to store the tags for each product
df = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Only return the selected tags as a comma-separated list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df.loc[index, "new_tags"] = tags
print(df.head(10))
45/164:

# Create a new column to store the tags for each product
df = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df.loc[index, "new_tags"] = tags
print(df.head(10))
45/165:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df_tech_new_tags.loc[index, "new_tags"] = tags
print(df.head(10))
45/166: print(tags_dict["Autonomous and uncrewed solutions"])
45/167: print(tags_dict)
45/168: print(categories)
45/169:
for i in categories
    print(i)
45/170:
for i in categories:
    print(i)
45/171:
for key, value in tags_dict.items():
    print(key, value)
45/172:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/173:
for key, value in tags_dict.items():
    print(key, value)
45/174:
# print each unique category and the tags from df
for category in categories:
    print(category)
    print(df_tags_tech[df_tags_tech["Product category"] == category]["tags"].unique())
45/175:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df_tech_new_tags.loc[index, "new_tags"] = tags
print(df.head(10))
45/176:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df_tech_new_tags.loc[index, "new_tags"] = tags
print(df.head(10))
45/177:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df_tech_new_tags.loc[index, "new_tags"] = tags
print(df_tech_new_tags.head(10))
45/178:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.head(10).iterrows():
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df_tech_new_tags.loc[index, "new_tags"] = tags
print(df_tech_new_tags.head(10))


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/179:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df_tech_new_tags.loc[index, "new_tags"] = tags
print(df_tech_new_tags.head(10))


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/180:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Each sub-tag should correspond to minimun one product. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/181:
for key, value in tags_dict.items():
    print(key, value)
45/182:
# print each unique category and the tags from df
for category in categories:
    print(category)
    print(df_tags_tech[df_tags_tech["Product category"] == category]["tags"].unique())
45/183:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    tags = tags_dict[product_category]

    # choose 5 tags from the tags list for each product
    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
        tags,  row["Product category"], row["Product Name"]
    )
    tags = general_gpt(prompt)
    df_tech_new_tags.loc[index, "new_tags"] = tags
print(df_tech_new_tags.head(10))


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/184:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "new_tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/185:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Each sub-tag should correspond to minimun one product. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/186:
for key, value in tags_dict.items():
    print(key, value)
45/187:
# print each unique category and the tags from df
for category in categories:
    print(category)
    print(df_tags_tech[df_tags_tech["Product category"] == category]["tags"].unique())
45/188:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/189:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/190:
for key, value in tags_dict.items():
    print(key, value)
45/191:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/192:
for key, value in tags_dict.items():
    print(key, value)
45/193:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/194:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of low-level tags: {} and the products: {} in the category {}: please select 5 technical high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags from the given low-level tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags_list, products, category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/195:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of low-level tags: {} and the products: {} in the category {}: please select 5 technical high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags from the given low-level tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags, products, category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/196:
for key, value in tags_dict.items():
    print(key, value)
45/197:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}
# Create a new column to store the tags for each product
df_tags_tech['tags'] = ''

used_tags = set()
for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    available_tags = [tag for tag in tags if tag not in used_tags]
    prompt = "Given the list of available low-level tags: {} and the products: {} in the category {}: please select 5 technical high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags from the given low-level tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        available_tags, products, category
    )
    category_tags = general_gpt(prompt)
    used_tags.update(category_tags.keys())

    # Save the tags in the dictionary
    tags_dict[category] = tags

    # Add the tags to the new column in the dataframe
    df_tags_tech.loc[df_tags_tech["Product category"] == category, 'tags'] = [tags_dict[category]] * len(df_tags_tech[df_tags_tech["Product category"] == category])
45/198:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    print(category)
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 15 tags that best describe the technical aspects of these products. Return the selected tags as a comma-separated list.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/199:
for key, value in tags_dict.items():
    print(key, value)
45/200:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products in the category {}: {}, please select 5 high-level tags that best describe the technical aspects of these products. For each high-level tag, select 5 sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags, category, products
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/201:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/202:
for key, value in tags_dict.items():
    print(key, value)
45/203:
# print each unique category and the tags from df
for category in categories:
    print(category)
    print(df_tags_tech[df_tags_tech["Product category"] == category]["tags"].unique())
45/204:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "new_tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
44/16:
# read df new_tags_and_columns.csv

df_new_tags = pd.read_csv("./data/new_tags_columns_df_1.csv")

# #only use the columns Product Name, product category, url, app_use, tech, general
# df_new_tags = df_new_tags[
#     ["Product Name", "Product category", "app_use", "tech", "general"]
# ]
# save df_new_tags as xlsx
df_new_tags.to_excel("./data/new_tags_columns_df_1.csv.xlsx")
44/17:
# read df new_tags_and_columns.csv

df_new_tags = pd.read_csv("./data/new_tags_columns_df_1.csv")

# #only use the columns Product Name, product category, url, app_use, tech, general
# df_new_tags = df_new_tags[
#     ["Product Name", "Product category", "app_use", "tech", "general"]
# ]
# save df_new_tags as xlsx
df_new_tags.to_excel("./data/new_tags_columns_df_1.xlsx")
45/205:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification that can be used to describe this and similar products: {product_name}. Do not use the product name direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_technology.csv", index=False)
45/206:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/207:
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
45/208:
for i in categories:
    print(i)
45/209:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
45/210:
for key, value in tags_dict.items():
    print(key, value)
45/211:
# print each unique category and the tags from df
for category in categories:
    print(category)
    print(df_tags_tech[df_tags_tech["Product category"] == category]["tags"].unique())
45/212:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "new_tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/213:
tags = tag_list


# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = tags
    df_tags_tech.loc[df_tags_tech["Product category"] == category, "Tags_level"] = str(category_tags)
45/214:
# print each unique category and the tags from df
for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    # Get the tags for the current category
    tags = df_tags_tech[df_tags_tech["Product category"] == category]["Tags_level"]

    # Print the category and the tags
    print(tags)
45/215:
for key, value in tags_dict.items():
    print(key, value)
45/216:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags

    # Save the tags in the DataFrame
    df_tags_tech.loc[df_tags_tech["Product category"] == category, "Tags_level"] = str(category_tags)

df_tags_tech.to_csv("./data/df_tags_technology.csv", index=False)
45/217:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = "Given the list of tags: {} and the products: {} in the category {}: please select 5 high-level tags that can be used as filters to find these products on a website. For each high-level tag, select 5 specific and descriptive sub-tags. Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.".format(
        tags,  products,category
    )
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags

    # Save the tags in the DataFrame
    df_tags_tech.loc[df_tags_tech["Product category"] == category, "Tags_level"] = str(category_tags)
45/218:
for key, value in tags_dict.items():
    print(key, value)
45/219: df_tags_tech.loc[df_tags_tech["Product category"] == category, "Tags_level"] = str(category_tags)
45/220:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
45/221:
for key, value in tags_dict.items():
    print(key, value)
45/222:
# print each unique category and the tags from df
for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    # Get the tags for the current category
    tags = df_tags_tech[df_tags_tech["Product category"] == category]["Tags_level"]

    # Print the category and the tags
    print(tags)
45/223:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "new_tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/224: df_tags_tech.loc[df_tags_tech["Product category"] == category, "Tags_level"] = str(category_tags)
45/225:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "new_tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/226:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/227:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["tags_dict"][0])
45/228:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["tags_dict"][1])
45/229:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Maritime communications"][1])
45/230:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Maritime communications"][1])
45/231:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Maritime communications"][1])
45/232:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Product category"]["Maritime communications"])
45/233:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Product category"])
45/234:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Product category"] = "Maritime communications")
45/235:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech["Product category"] = "Maritime communications"
print(df_tags_tech)
45/236:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "Application",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
            "Application / use",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/237:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


#print the maritime communication tags_level
print(tags_dict["Maritime communication"])
45/238:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


#print the maritime communication tags_level
print(df_tags_tech["Maritime communication"])
45/239:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["tags_dict"][0])
45/240:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Product category"][0])
45/241:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Product category"]["Maritime communications"])
45/242:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


print(df_tags_tech["Product category"][0])
45/243:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/244:
from multiprocessing import Pool

def get_category_tags(category):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    return category, general_gpt(prompt)

with Pool(5) as p:
    results = p.map(get_category_tags, categories)

tags_dict = {category: tags for category, tags in results}
45/245:
from pathos.multiprocessing import ProcessingPool as Pool

def get_category_tags(category):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    return category, general_gpt(prompt)

with Pool(5) as p:
    results = p.map(get_category_tags, categories)

tags_dict = {category: tags for category, tags in results}
45/246:
from pathos.multiprocessing import ProcessingPool as Pool

def get_category_tags(category):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    return category, general_gpt(prompt)

with Pool(5) as p:
    results = p.map(get_category_tags, categories)

tags_dict = {category: tags for category, tags in results}
45/247:
from pathos.multiprocessing import ProcessingPool as Pool

def get_category_tags(category):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    return category, general_gpt(prompt)

with Pool(5) as p:
    results = p.map(get_category_tags, categories)

tags_dict = {category: tags for category, tags in results}
45/248:
from pathos.multiprocessing import ProcessingPool as Pool

def get_category_tags(category):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    return category, general_gpt(prompt)

with Pool(5) as p:
    results = p.map(get_category_tags, categories)

tags_dict = {category: tags for category, tags in results}
45/249:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
45/250:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/251:
# delete the column tags_level
df_tags_tech = df_tags_tech.drop(columns=["tags_level"], axis=1)
45/252:
# delete the column tags_level
df_tags_tech = df_tags_tech.drop(columns=["Tags_level"], axis=1)
45/253:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/254:
# delete the column tags_level
df_tags_tech = df_tags_tech.drop(columns=["tags"], axis=1)
45/255:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/256: print(tags_dict)
45/257:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/258:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/259:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/260:
#save tags_dict as a colum with coresponding category
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/261:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
45/262:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "Application",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
            "Application / use",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/263:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/264:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "Application",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
            "Application / use ",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/265:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
45/266:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "Application",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/267:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/268:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
45/269:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "Application",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/270:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/271:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
45/272:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/273:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/274:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
45/275:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
45/276: print(url_text_dict)
45/277:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
45/278:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    # gpt_model = "gpt-35-turbo-16k"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/279:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification that can be used to describe this and similar products: {product_name}. Do not use the product name direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_technology.csv", index=False)
45/280:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/281:
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
45/282:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
45/283:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
45/284:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
45/285:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
45/286:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
45/287:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
45/288: print(url_text_dict)
45/289:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
45/290:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    # gpt_model = "gpt-35-turbo-16k"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/291:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the product name, category, and the text from the website
        url = row["url"]
        product_name = row["Product Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification that can be used to describe this and similar products: {product_name}. Do not use the product name direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_technology.csv", index=False)
45/292:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
45/293:
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
45/294:
for i in categories:
    print(i)
45/295:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
45/296:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
45/297:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/298:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
45/299:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
45/300: print(tags_dict)
45/301:
#save tags_dict as a colum with coresponding category
df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_test.csv", index=False)
45/302:
# delete the column tags_level
df_tags_tech = df_tags_tech.drop(columns=["tags"], axis=1)
45/303:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/304:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/305:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/306:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        return img['src']
    else:
        return None

# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tech_new_tags_test.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test_images.csv", index=False)
45/307: st.write(df_tech_new_tags.head())
45/308: print(df_tech_new_tags.head())
45/309:
print(df_tech_new_tags["image_url"][3])
print(df_tech_new_tags["url"][3])
45/310:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com/"+img['src']
        return img_url
    else:
        return None

# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tech_new_tags_test.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test_images.csv", index=False)
45/311:
print(df_tech_new_tags["image_url"][3])
print(df_tech_new_tags["url"][3])
45/312:
print(df_tech_new_tags["image_url"][3])
print(df_tech_new_tags["url"][3])
df_tech_new_tags.to_csv("./data/good_data/df_tech_new_tags_test.csv", index=False)
45/313:
print(df_tech_new_tags["image_url"][3])
print(df_tech_new_tags["url"][3])
df_tech_new_tags.to_csv("./data/good_data/df_tech_new_tags_test.csv", index=False)
45/314:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tech_new_tags_test.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test_images.csv", index=False)
45/315:
print(df_tech_new_tags["image_url"][3])
print(df_tech_new_tags["url"][3])
df_tech_new_tags.to_csv("./data/good_data/df_tech_new_tags_test.csv", index=False)
45/316:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tech_new_tags_test.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test_images.csv", index=False)
45/317:
print(df_tech_new_tags["image_url"][3])
print(df_tech_new_tags["url"][3])
df_tech_new_tags.to_csv("./data/good_data/df_tech_new_tags_test.csv", index=False)
45/318:
print(df_tech_new_tags["image_url"][3])
print(df_tech_new_tags["url"][3])
df_tech_new_tags.to_csv("./data/good_data/df_tech_new_tags_test.csv", index=False)

#rename column Product Name to Product_Name
df_tech_new_tags = df_tech_new_tags.rename(columns={"Product Name": "Product_Name"})
df_tech_new_tags.to_csv("./data/good_data/df_tech_new_tags_test.csv", index=False)
45/319: df_tech_new_tags.to_xlsx("./data/good_data/df_tech_new_tags_test", index=False)
45/320: df_tech_new_tags.to_excel("./data/good_data/df_tech_new_tags_test", index=False)
45/321: df_tech_new_tags.to_excel("./data/good_data/df_tech_new_tags_test", index=False)
45/322:
#save to excel
df_tech_new_tags.to_excel("./data/df_tech_new_tags_test.xlsx", index=False)
45/323:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")
        
        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            tags = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tech_new_tags_test.csv", index=False)
45/324:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            tags = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tech_new_tags_test.xlsx", index=False)
45/325:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            tags = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tech_new_tags_test.xlsx", index=False)
45/326:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tech_new_tags_test.xlsx", index=False)
45/327:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tech_new_tags_test.xlsx", index=False)
45/328:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tech_new_tags_test.xlsx", index=False)
45/329: print(df_tech_new_tags["Product_Name"][3])
46/1:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tech_new_tags_test.xlsx", index=False)
46/2:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
46/3:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product_Name", "url"]], on="Product_Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)

    return df_tags
46/4:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
46/5:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product_Name", "url"]], on="ProductName", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
46/6:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
46/7:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product_Name", "url"]], on="ProductName", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
46/8:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
46/9:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product_Name", "url"]], on="ProductName", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
46/10:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
46/11:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
46/12:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
46/13:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
46/14:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
46/15:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
46/16: print(url_text_dict)
46/17:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
46/18:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
46/19:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification that can be used to describe this and similar products: {product_name}. Do not use the Product_Name direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_technology_15_11.csv", index=False)
46/20:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
46/21:
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
46/22:
for i in categories:
    print(i)
46/23:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
46/24: print(tags_dict)
46/25:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_15_11.csv", index=False)
46/26:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_technology_15_11.csv", index=False)
46/27:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_technology_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_technology_15_11.csv", index=False)
46/28:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_technology_15_11.xlsx", index=False)
46/29:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple TECHNOLOGY tags that could be aggragation from the technical specification that can be used to describe this and similar products: {product_name}. Do not use the Product_Name direct as tags. Write the tags with comma as delimiter."

        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_technology_15_11.csv", index=False)
46/30:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
46/31:
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
46/32:
for i in categories:
    print(i)
46/33:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
46/34: print(tags_dict)
46/35:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_technology_15_11.csv", index=False)
46/36:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_technology_15_11.csv", index=False)
46/37:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_technology_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_technology_15_11.csv", index=False)
46/38:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_technology_15_11.xlsx", index=False)
47/1:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
47/2:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
47/3:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
47/4:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
47/5:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
47/6: print(url_text_dict)
47/7:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
47/8:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
47/9:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Based on this text {website_text}, write five simple APPLICATION AND USAGE tags that could be derived from the product description that can be used to describe this and similar products: {product_name}. Do not use the Product_Name directly as tags. Write the tags with a comma as delimiter."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/10:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
47/11:
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
47/12:
for i in categories:
    print(i)
47/13:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-lever tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/14: print(tags_dict)
47/15:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/16:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/17:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/18:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/19:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', or 'Products'. 
        These tags can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-level tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/20: print(tags_dict)
47/21:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/22:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/23:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/24:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/25: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/26:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    if "Product Type" in str(row["tags"]):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            tags_dict = tags_dict[product_category]

            # Check if high-level tag is in tags_dict
            if high_level_tag in tags_dict:
                # Repeat the process
                prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                    tags_dict,  row["Product category"], row["Product_Name"]
                )
                tags = general_gpt(prompt)
                df_tech_new_tags.loc[index, "tags"] = tags
                print(tags)
            else:
                print(f"High-level tag '{high_level_tag}' not found in tags_dict")
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/27:
# Ensure tags_dict is a dictionary
if isinstance(tags_dict, dict):
    # Iterate over the rows of the dataframe
    for index, row in df_tech_new_tags.iterrows():
        print(index)

        if "Product Type" in str(row["tags"]):
            print("found occurence")

            product_category = row["Product category"]
            # Get the tags for the product category
            if product_category in tags_dict:
                print("found category")
                category_tags_dict = tags_dict[product_category]

                # Check if high-level tag is in category_tags_dict
                if high_level_tag in category_tags_dict:
                    # Repeat the process
                    prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                        category_tags_dict,  row["Product category"], row["Product_Name"]
                    )
                    tags = general_gpt(prompt)
                    df_tech_new_tags.loc[index, "tags"] = tags
                    print(tags)
                else:
                    print(f"High-level tag '{high_level_tag}' not found in category_tags_dict")
            else:
                print(f"Category '{product_category}' not found in tags_dict")
else:
    print("tags_dict is not a dictionary")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/28:
# Ensure tags_dict is a dictionary
if isinstance(tags_dict, dict):
    # Iterate over the rows of the dataframe
    for index, row in df_tech_new_tags.iterrows():
        print(index)



        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            category_tags_dict = tags_dict[product_category]

            # Check if high-level tag is in category_tags_dict
            if high_level_tag in category_tags_dict:
                # Repeat the process
                prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                    category_tags_dict,  row["Product category"], row["Product_Name"]
                )
                tags = general_gpt(prompt)
                df_tech_new_tags.loc[index, "tags"] = tags
                print(tags)
            else:
                print(f"High-level tag '{high_level_tag}' not found in category_tags_dict")
        else:
            print(f"Category '{product_category}' not found in tags_dict")
else:
    print("tags_dict is not a dictionary")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/29:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    product_category = row["Product category"]
    tags_dict_str = row["tags_dict"]

    # Convert the string representation of dictionary to a dictionary
    tags_dict = ast.literal_eval(tags_dict_str)

    # Check if high_level_tag is in tags_dict
    if high_level_tag in tags_dict:
        # Repeat the process
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"High-level tag '{high_level_tag}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/30:
import ast
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    product_category = row["Product category"]
    tags_dict_str = row["tags_dict"]

    # Convert the string representation of dictionary to a dictionary
    tags_dict = ast.literal_eval(tags_dict_str)

    # Check if high_level_tag is in tags_dict
    if high_level_tag in tags_dict:
        # Repeat the process
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"High-level tag '{high_level_tag}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/31:
import ast
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    product_category = row["Product category"]
    tags_dict_str = row["tags_dict"]

    # Convert the string representation of dictionary to a dictionary
    tags_dict = ast.literal_eval(tags_dict_str)
    print(tags_dict)
    # Check if high_level_tag is in tags_dict
    if high_level_tag in tags_dict:
        # Repeat the process
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"High-level tag '{high_level_tag}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/32:
import ast
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    product_category = row["Product category"]
    tags_dict_str = row["tags_dict"]

    # Convert the string representation of dictionary to a dictionary
    tags_dict = ast.literal_eval(tags_dict_str)
    print(tags_dict)
    # Check if high_level_tag is in tags_dict
    if "High-Level Tag" in tags_dict.values():
        # Repeat the process
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"High-level tag '{high_level_tag}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/33:
import ast
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    product_category = row["Product category"]
    tags_dict_str = row["tags_dict"]

    # Convert the string representation of dictionary to a dictionary
    tags_dict = ast.literal_eval(tags_dict_str)
    print(tags_dict)
    # Check if high_level_tag is in tags_dict
    if "High-Level Tag" in tags_dict.values():
        # Repeat the process
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"High-level tag  not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/34:
import ast
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    product_category = row["Product category"]
    tags_dict_str = row["tags_dict"]

    if pd.notna(tags_dict_str):
        # Convert the string representation of dictionary to a dictionary
        tags_dict = ast.literal_eval(tags_dict_str)

        # Check if "High-Level Tag" is in the values of tags_dict
        if "High-Level Tag" in tags_dict.values():
            # Repeat the process
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"'High-Level Tag' not found in tags_dict values")
    else:
        print("tags_dict_str is NaN")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
46/39:
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    # Check if the "tags" column starts with "product"
    # Check if the "tags" column starts with "product" or "Product"
    if "Product Type" in str(row["tags"]):
    # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
        print("found occurence")

        product_category = row["Product category"]
        # Get the tags for the product category
        if product_category in tags_dict:
            print("found category")
            tags_dict = tags_dict[product_category]

            # choose 5 tags from the tags list for each product
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"Category '{product_category}' not found in tags_dict")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_technology_15_11.xlsx", index=False)
47/35:
import ast
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    product_category = row["Product category"]
    tags_dict_str = row["tags_dict"]

    if pd.notna(tags_dict_str):
        # Convert the string representation of dictionary to a dictionary
        tags_dict = ast.literal_eval(tags_dict_str)

        # Check if "High-Level Tag" is in the values of tags_dict
        if "High-Level Tag" in tags_dict.values():
            # Repeat the process
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"'High-Level Tag' not found in tags_dict values")
    else:
        print("tags_dict_str is NaN")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/36:
import ast
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)

    product_category = row["Product category"]
    tags_dict_str = row["tags_dict"]

    if pd.notna(tags_dict_str):
        # Convert the string representation of dictionary to a dictionary
        tags_dict = ast.literal_eval(tags_dict_str)

        # Check if "High-Level Tag" is in the values of tags_dict
        if "High-Level Tag" in tags_dict.values():
            # Repeat the process
            prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
                tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            df_tech_new_tags.loc[index, "tags"] = tags
            print(tags)
        else:
            print(f"'High-Level Tag' not found in tags_dict values")
    else:
        print("tags_dict_str is NaN")

# save the dataframe as excel file
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/37:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
47/38:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
47/39:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
47/40:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
47/41:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
47/42: print(url_text_dict)
47/43:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
47/44:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
47/45:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', please identify five APPLICATION AND USAGE tags. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of exactly five tags."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/46:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
47/47:
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
47/48:
for i in categories:
    print(i)
47/49:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    prompt = """
        Given the list of low-level tags: {} and the products: {} in the category {}: 
        please select 5 high-level tags that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', or 'Products'. 
        These tags can be used as filters to find these products on a website. 
        For each high-level tag, select 5 specific and descriptive sub-tags/low-level tags. 

        For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
        and 'Size' having sub-tags ['small', 'large', 'medium'].

        Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
        """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/50: print(tags_dict)
47/51:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/52:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/53:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # choose 5 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        # if tags contains the word "High-Level Tag" run the prompt again
        while "High-Level Tag" in tags:
            print("High-Level Tag in tags")
            tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/54:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # Remove "High-Level Tag" from the tags
        tags = [tag for tag in tags if tag != "High-Level Tag"]

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word tag as a tag.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/55:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    prompt = """
    Given the list of low-level tags: {} and the products: {} in the category {}: 
    please select 5 high-level tags that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'High-Level Tag', or 'Sub-Tag'. 
    These tags can be used as filters to find these products on a website. 
    For each high-level tag, select 5 specific and descriptive sub-tags/low-level tags. 

    For example, if the low-level tags are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
    and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
    the high-level tags could be ['Color', 'Size'] with 'Color' having sub-tags ['red', 'blue', 'green'] 
    and 'Size' having sub-tags ['small', 'large', 'medium'].

    Return the tags as a dictionary where the keys are the high-level tags and the values are lists of the sub-tags.
    """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/56: print(tags_dict)
47/57:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/58:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # Remove "High-Level Tag" from the tags
        tags = [tag for tag in tags if tag != "High-Level Tag"]

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word tag as a tag.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/59:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # Remove "High-Level Tag" from the tags
        tags = [tag for tag in tags if tag != "High-Level Tag"]

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/60:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    prompt = """
    Given the list of attributes: {} and the products: {} in the category {}: 
    please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
    These categories can be used as filters to find these products on a website. 
    For each category, select 5 specific and descriptive attributes. 

    For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
    and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
    the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
    and 'Size' having attributes ['small', 'large', 'medium'].

    Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes.
    """.format(tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/61: print(tags_dict)
47/62:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/63:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # Remove "High-Level Tag" from the tags
        tags = [tag for tag in tags if tag != "High-Level Tag"]

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/64:
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # Remove "High-Level Tag" from the tags
        tags = [tag for tag in tags if tag != "High-Level Tag"]

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")
47/65:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of attributes: {} and the products: {} in the category {}: 
    please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
    These categories can be used as filters to find these products on a website. 
    For each category, select 5 specific and descriptive attributes. 

    For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
    and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
    the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
    and 'Size' having attributes ['small', 'large', 'medium'].

    Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/66: print(tags_dict)
47/67:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    # gpt_model = "gpt-35-turbo-16k"
    gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
47/68:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of attributes: {} and the products: {} in the category {}: 
    please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
    These categories can be used as filters to find these products on a website. 
    For each category, select 5 specific and descriptive attributes. 

    For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
    and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
    the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
    and 'Size' having attributes ['small', 'large', 'medium'].

    Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/69:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Limit the number of products and tags to reduce the number of tokens
    limited_products = products
    limited_tags = filtered_tags[:1000]

    prompt = """
    Given the list of attributes: {} and the products: {} in the category {}: 
    please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
    These categories can be used as filters to find these products on a website. 
    For each category, select 5 specific and descriptive attributes. 

    For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
    and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
    the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
    and 'Size' having attributes ['small', 'large', 'medium'].

    Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes.
    """.format(limited_tags,  limited_products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/70:

import time

tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Split the tags into chunks of 1000
    chunks = [filtered_tags[i:i + 100] for i in range(0, len(filtered_tags), 100)]

    for chunk in chunks:
        prompt = """
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes.
        """.format(chunk,  products, category)

        # Generate the tags for the current chunk
        category_tags = general_gpt(prompt)

        # Save the tags in the dictionary
        tags_dict[category] = category_tags

        # Wait for 5 seconds before processing the next chunk
        time.sleep(10)
47/71:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    # gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
47/72:

import time

tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Split the tags into chunks of 1000
    chunks = [filtered_tags[i:i + 100] for i in range(0, len(filtered_tags), 100)]

    for chunk in chunks:
        prompt = """
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes.
        """.format(chunk,  products, category)

        # Generate the tags for the current chunk
        category_tags = general_gpt(prompt)

        # Save the tags in the dictionary
        tags_dict[category] = category_tags

        # Wait for 5 seconds before processing the next chunk
        time.sleep(10)
47/73:

import time

tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Split the tags into chunks of 1000
    chunks = [filtered_tags[i:i + 100] for i in range(0, len(filtered_tags), 100)]

    for chunk in chunks:
        prompt = """
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes.
        """.format(chunk,  products, category)

        # Generate the tags for the current chunk
        category_tags = general_gpt(prompt)
        print(category_tags)

        # Save the tags in the dictionary
        tags_dict[category] = category_tags

        # Wait for 5 seconds before processing the next chunk
        time.sleep(10)
47/74:

import time

tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Split the tags into chunks of 1000
    chunks = [filtered_tags[i:i + 100] for i in range(0, len(filtered_tags), 100)]

    for chunk in chunks:
        prompt = """
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed.
        """.format(chunk,  products, category)

        # Generate the tags for the current chunk
        category_tags = general_gpt(prompt)
        print(category_tags)

        # Save the tags in the dictionary
        tags_dict[category] = category_tags

        # Wait for 5 seconds before processing the next chunk
        time.sleep(10)
47/75:

import time

tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Split the tags into chunks of 1000
    chunks = [filtered_tags[i:i + 100] for i in range(0, len(filtered_tags), 100)]

    for chunk in chunks:
        prompt = """
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc.
        """.format(chunk,  products, category)

        # Generate the tags for the current chunk
        category_tags = general_gpt(prompt)
        print(category_tags)

        # Save the tags in the dictionary
        tags_dict[category] = category_tags

        # Wait for 5 seconds before processing the next chunk
        time.sleep(10)
47/76:

import time

tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Split the tags into chunks of 1000
    chunks = [filtered_tags[i:i + 100] for i in range(0, len(filtered_tags), 100)]

    for chunk in chunks:
        prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(chunk,  products, category)

        # Generate the tags for the current chunk
        category_tags = general_gpt(prompt)
        print(category_tags)

        # Save the tags in the dictionary
        tags_dict[category] = category_tags

        # Wait for 5 seconds before processing the next chunk
        time.sleep(10)
47/77:

import time

tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Split the tags into chunks of 1000
    chunks = [filtered_tags[i:i + 100] for i in range(0, len(filtered_tags), 100)]

    for chunk in chunks:
        prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(chunk,  products, category)

        # Generate the tags for the current chunk
        category_tags = general_gpt(prompt)
        print(category)
        print(category_tags)

        # Save the tags in the dictionary
        tags_dict[category] = category_tags

        # Wait for 5 seconds before processing the next chunk
        time.sleep(10)
47/78:
categories = df_tags_tech["Product category"].dropna().unique()

# split the categories that  have multiple categories with comma
categories = [category.split(",") for category in categories]
# make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))
# strip the whitespace
categories = [category.strip() for category in categories]
47/79:
for i in categories:
    print(i)
47/80:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

# Strip the whitespace and filter out unwanted categories
unwanted_categories = ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']
categories = [category.strip() for category in categories if category.lower() not in unwanted_categories]
47/81:
for i in categories:
    print(i)
47/82:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
47/83:

import time

tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in unique_categories:
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    # Split the tags into chunks of 1000
    chunks = [filtered_tags[i:i + 100] for i in range(0, len(filtered_tags), 100)]

    for chunk in chunks:
        prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(chunk,  products, category)

        # Generate the tags for the current chunk
        category_tags = general_gpt(prompt)
        print(category)
        print(category_tags)

        # Save the tags in the dictionary
        tags_dict[category] = category_tags

        # Wait for 5 seconds before processing the next chunk
        time.sleep(10)
47/84:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/85:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
47/86:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}: 
        please select 5 categories that are NOT related to 'Technology', 'Applications', 'Specifications', 'Communications', 'Sensors', 'Products', 'Category 1', or 'Attribute 1'. 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(10)
47/87:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}:
        Use these attributes to create categories: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(10)
47/88:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}:
        Use these attributes to create categories: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes from the list of attributes. always use 5 attributes for each category. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium'] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green'] 
        and 'Size' having attributes ['small', 'large', 'medium'].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
47/89:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}:
        Use these attributes to create categories: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes from the list of attributes. always use 5 attributes for each category. 

        For example, if the attributes are ['red', 'blue', 'green', 'small', 'large', 'medium', "extra large", "extra small", 'yellow', 'orange', ] 
        and the products are ['Product A', 'Product B', 'Product C'] in the category 'Colorful Products', 
        the categories could be ['Color', 'Size'] with 'Color' having attributes ['red', 'blue', 'green', 'yellow', 'orange'] 
        and 'Size' having attributes ['small', 'large', 'medium', "extra large", "extra small].

        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
47/90:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
        Do not call the categories for Category 1, Category 2, etc.
        Given the list of attributes: {} and the products: {} in the category {}:
        Use these attributes to create categories: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
        These categories can be used as filters to find these products on a website. 
        For each category, select 5 specific and descriptive attributes from the list of attributes. always use 5 attributes for each category. 
        
        Return the categories as a dictionary where the keys are the categories and the values are lists of the attributes. Only return the dictionary no explanation is needed. Do not call the categories for Category 1, Category 2, etc. Only use tags from the list of attributes
        """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
47/91:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these sub-tags to create top-level tags: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
    These top-level tags can be used as filters to find these products on a website. 
    For each top-level tag, select 5 specific and descriptive sub-tags from the list of sub-tags. Always use 5 sub-tags for each top-level tag. 

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Do not call the top-level tags for Top-level Tag 1, Top-level Tag 2, etc. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
47/92: print(tags_dict)
47/93:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/94:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # Remove "High-Level Tag" from the tags
        tags = [tag for tag in tags if tag != "High-Level Tag"]

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/95:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # Remove "High-Level Tag" from the tags
        tags = [tag for tag in tags if tag != "High-Level Tag"]

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/96:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags_dict = tags_dict[product_category]


        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/97:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/98: print(tags_dict)
47/99:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
47/100:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

# Strip the whitespace and filter out unwanted categories
unwanted_categories = ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']
categories = [category.strip() for category in categories if category.lower() not in unwanted_categories]
47/101:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
47/102:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these sub-tags to create top-level tags: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
    These top-level tags can be used as filters to find these products on a website. 
    For each top-level tag, select 5 specific and descriptive sub-tags from the list of sub-tags. Always use 5 sub-tags for each top-level tag. 

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Do not call the top-level tags for Top-level Tag 1, Top-level Tag 2, etc. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
47/103: print(tags_dict)
47/104:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/105:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags_dict = tags_dict[product_category]


        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/106:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        tags_dict = tags_dict[product_category]



        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/107:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        tags_dict = tags_dict[product_category]



        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/108:
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = row["Product category"]
    # Get the tags for the product category
    if product_category in tags_dict:
        tags = tags_dict[product_category]
        print(tags)

        # Remove "High-Level Tag" from the tags
        tags = [tag for tag in tags if tag != "High-Level Tag"]

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
            tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
    else:
        print(f"Category '{product_category}' not found in tags_dict")
47/109:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        tags_dict = tags_dict[product_category]



        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/110:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
47/111:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
47/112: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/113:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/114:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
47/115:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        tags_dict = tags_dict[product_category]



        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/1:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
48/2:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
48/3:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
48/4:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
48/5:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/6: print(url_text_dict)
48/7:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
48/8:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    # gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
48/9:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', please identify five APPLICATION AND USAGE tags. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of exactly five tags."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/10:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
48/11:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

# Strip the whitespace and filter out unwanted categories
unwanted_categories = ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']
categories = [category.strip() for category in categories if category.lower() not in unwanted_categories]
48/12:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
48/13:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these sub-tags to create top-level tags: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
    These top-level tags can be used as filters to find these products on a website. 
    For each top-level tag, select 5 specific and descriptive sub-tags from the list of sub-tags. Always use 5 sub-tags for each top-level tag. 

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Do not call the top-level tags for Top-level Tag 1, Top-level Tag 2, etc. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
48/14: print(tags_dict)
48/15:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/16:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        tags_dict = tags_dict[product_category]



        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/17:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable




        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/18:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable




        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/19:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/20: print(tags_dict)
48/21:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these sub-tags to create top-level tags: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
    These top-level tags can be used as filters to find these products on a website. 
    For each top-level tag, select 5 specific and descriptive sub-tags from the list of sub-tags. Always use 5 sub-tags for each top-level tag. 

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Do not call the top-level tags for Top-level Tag 1, Top-level Tag 2, etc. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
48/22:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these sub-tags to create top-level tags: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
    These top-level tags can be used as filters to find these products on a website. 
    For each top-level tag, select 5 specific and descriptive sub-tags from the list of sub-tags. Always use 5 sub-tags for each top-level tag. 

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Do not call the top-level tags for Top-level Tag 1, Top-level Tag 2, etc. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
48/23: print(tags_dict)
48/24:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/25:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable




        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/26:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
48/27:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/28: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/29:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/30:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/31:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    ant_tags = row["tags"].split(",")
    if len(ant_tags) > 10:
        ant += 1
        print(index)
        print(row["tags"])
        print(len(ant_tags))
        print("found more than 10 tags")

print(ant)
48/32:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

print(ant)
48/33:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

print(ant)
48/34:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

print("ant, "+ant)
48/35:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

print("ant, ",ant)
48/36:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            if ant_tags[0] == "Product Type":
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")

                # update the tags column with only 8 tags


print("ant, ",ant)
48/37:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            if ant_tags.contains("Product Type"):
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")

                # update the tags column with only 8 tags


print("ant, ",ant)
48/38:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            if "Product Type" in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")

                # update the tags column with only 8 tags


print("ant, ",ant)
48/39:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")

                # update the tags column with only 8 tags


print("ant, ",ant)
48/40:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        if len(ant_tags) > 10:
            # if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/41:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        ant_tags = ant_tags[0].split(" ")
        if len(ant_tags) > 10:
            # if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/42:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        ant_tags = ant_tags[0].split(":")
        if len(ant_tags) > 10:
            # if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/43:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        ant_tags = ant_tags[0].split(": ")
        if len(ant_tags) > 10:
            # if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/44:
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        ant_tags = row["tags"].split(",")
        ant_tags = ant_tags[0].split(" : ")
        if len(ant_tags) > 10:
            # if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/45:
import re
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            # if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/46:
import re
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")

                # update the tags column with only 8 tags


print("ant, ",ant)
48/47:
import re
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            # if "Product Type" in ant_tags or  "Product Type: " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/48:
import re
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "'Product Type':" in ant_tags or  "Product Type: " in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/49:
import re
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "'Product Type'" in ant_tags or  "Product Type " in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/50:
import re
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")

            # update the tags column with only 8 tags


print("ant, ",ant)
48/51: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/52:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")
                print("updating tags")
                prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
                tags_dict,  row["Product category"], row["Product_Name"]
                )
                tags = general_gpt(prompt)
                print("New tags: ", tags)
                df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/53:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")
                print("------------------")
                print(row["tags_dict"])
                print("updating tags")
                prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
                tags_dict,  row["Product category"], row["Product_Name"]
                )
                tags = general_gpt(prompt)
                print("New tags: ", tags)
                df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/54:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")
                print("------------------")
                print(row["tags_dict"])
                print("updating tags")
                prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
                tags_dict,  row["Product category"], row["Product_Name"]
                )
                tags = general_gpt(prompt)
                print("New tags: ", tags)
                df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/55:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")
                print("------------------")
                print(row["tags_dict"])
                print("updating tags")
                prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
                tags_dict,  row["Product category"], row["Product_Name"]
                )
                tags = general_gpt(prompt)
                print("New tags: ", tags)
                df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/56:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
                ant += 1
                print(index)
                print(row["tags"])
                print(len(ant_tags))
                print("found more than 10 tags")
                print("------------------")
                print(row["tags_dict"])
                print("updating tags")
                prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
                tags_dict,  row["Product category"], row["Product_Name"]
                )
                tags = general_gpt(prompt)
                print("New tags: ", tags)
                df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/57:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/58:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 10:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/59:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/60:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/61: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/62:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/63:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    text = [element.get_text(strip=True) for element in elements]

    return text

url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/64:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    text = rich_text +" \n "+ bullet_points
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/65:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)
    text = rich_text +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/66:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/67:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/68:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
48/69:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
48/70:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
48/71:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
48/72:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/73:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/74: print(url_text_dict)
48/75:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
48/76:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    # gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
48/77:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', please identify five APPLICATION AND USAGE tags. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of exactly five tags."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/78:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
48/79:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

# Strip the whitespace and filter out unwanted categories
unwanted_categories = ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']
categories = [category.strip() for category in categories if category.lower() not in unwanted_categories]
48/80:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
48/81:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these sub-tags to create top-level tags: ['Product Type', 'Technology', 'Application', 'Navigation', 'Communication'] 
    These top-level tags can be used as filters to find these products on a website. 
    For each top-level tag, select 5 specific and descriptive sub-tags from the list of sub-tags. Always use 5 sub-tags for each top-level tag. 

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Do not call the top-level tags for Top-level Tag 1, Top-level Tag 2, etc. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
48/82: print(tags_dict)
48/83:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/84:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable




        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/85:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
48/86:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/87: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/88:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/89:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/90: # df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/91:
# import re
# ant = 0
# for index, row in df_tech_new_tags.iterrows():
#     if not pd.isna(row["tags"]):
#         # ant_tags = row["tags"].split(",")
#         ant_tags = re.split(',|:', row["tags"])
#         if len(ant_tags) > 11:
#             # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
#             ant += 1
#             print(index)
#             print(row["tags"])
#             print(len(ant_tags))
#             print("found more than 10 tags")
#             print("------------------")
#             # print(row["tags_dict"])
#             print("updating tags")
#             prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
#             tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             print("New tags: ", tags)
#             df_tech_new_tags.loc[index, "tags"] = tags

#             # update the tags column with only 8 tags


# print("ant, ",ant)

# print(df_tech_new_tags["tags"])
48/92:
# df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/93:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/94:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/95:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/96:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/97:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/98:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/99:
# df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/100:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/101:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these sub-tags to create top-level tags: ['Product Type', 'Technology', 'Application'] 
    These top-level tags can be used as filters to find these products on a website. 
    For each top-level tag, select 5 specific and descriptive sub-tags from the list of sub-tags. Always use 5 sub-tags for each top-level tag. 

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Do not call the top-level tags for Top-level Tag 1, Top-level Tag 2, etc. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
48/102:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these sub-tags to create top-level tags: ['Product Type', 'Technology', 'Application'] 
    These top-level tags can be used as filters to find these products on a website. 
    For each top-level tag, select 8 specific and descriptive sub-tags from the list of sub-tags. Always use 5 sub-tags for each top-level tag. 

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Do not call the top-level tags for Top-level Tag 1, Top-level Tag 2, etc. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
48/103:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/104:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/105:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable




        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "GUse this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            category_tags,  row["Product category"], row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/106:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable




        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/107: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/108:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 11:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/109:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/110:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/111:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/112:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/113: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/114:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/115:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/116:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/117:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/118: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/119:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/120:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/121:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/122:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable




        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Use this list of tags {}. And for this product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/123:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
48/124:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/125: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/126:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/127:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/128: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/129:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/130:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/131:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these top-level tags: ['Product Type', 'Technology', 'Application'] 
    For each top-level tag, select 8 specific and descriptive sub-tags from the list of sub-tags. Always use 8 sub-tags for each top-level tag. Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag.

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
48/132: print(tags_dict)
48/133:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/134:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable




        # Remove "High-Level Tag" from the tags
       

        # choose 8 tags from the tags list for each product
        prompt = "Use this list of tags {}. And for this product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/135:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
48/136:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/137: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/138:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/139:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/140: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/141:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
            # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/142:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/143:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/144:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/145:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/146:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/147: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/148:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/149:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/150:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/151:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/152:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        print(ant_tags)
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/153:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        print(ant_tags)
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Given the list of tags: {} and the product category: {}, and product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Each second-level tag should correspond to a minimum of one product each. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/154:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        print(ant_tags)
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/155:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        print(ant_tags)
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags

            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/156:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 4 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 4 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/157:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/158:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
print(all_tags_by_category)
48/159:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
print(all_tags_by_category)

# print fish finding group tags
print(all_tags_by_category["Fish finding"])
48/160:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)

# print fish finding group tags
print(all_tags_by_category["Fish finding"])
48/161:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)

# print fish finding group tags
# print(all_tags_by_category["Fish finding"])

# print dict_tags for fish finding Product category
print(df_tags_tech.loc[df_tags_tech["Product category"] == "Fish finding", "tags_dict"].iloc[0])
48/162:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)

# print fish finding group tags
print(all_tags_by_category["Fish finding"])

# print dict_tags for fish finding Product category
print(df_tags_tech.loc[df_tags_tech["Product category"] == "Fish finding", "tags_dict"].iloc[0])
48/163:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        category_tags = all_tags_by_category[category]
        # check if there is at least one tag that is the same
        if any(tag in category_tags for tag in tags_dict.values()):
            print(f"At least one tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
        else:
            print(f"No tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
    else:
        print(f"tags_dict_str is NaN for row {index}")
48/164:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        category_tags = all_tags_by_category[category]
        # check if there is at least one tag that is the same
        # check if there is at least one tag that is the same
        if any(' '.join(tag) in category_tags for tag in tags_dict.values()):
            print(f"At least one tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
        else:
            print(f"No tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")    else:
    else:
        print(f"tags_dict_str is NaN for row {index}")
48/165:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        category_tags = all_tags_by_category[category]
        # check if there is at least one tag that is the same
        # check if there is at least one tag that is the same
        if any(' '.join(tag) in category_tags for tag in tags_dict.values()):
            print(f"At least one tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
        else:
            print(f"No tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")    
    else:
        print(f"tags_dict_str is NaN for row {index}")
48/166:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        category_tags = all_tags_by_category[category]
        # check if there is at least one tag that is the same
        # check if there is at least one tag that is the same
        if any(any(t in category_tags for t in tag) for tag in tags_dict.values()):
            print(f"At least one tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
        else:
            print(f"No tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
    else:
        print(f"tags_dict_str is NaN for row {index}")
48/167:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')

        # check if there is at least one tag that is the same
        if any(any(t in category_tags for t in tag) for tag in tags_dict.values()):
            print(f"At least one tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
        else:
            print(f"No tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
    else:
        print(f"tags_dict_str is NaN for row {index}")
48/168:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        print(category_tags)

        # check if there is at least one tag that is the same
        if any(any(t in category_tags for t in tag) for tag in tags_dict.values()):
            print(f"At least one tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
        else:
            print(f"No tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
    else:
        print(f"tags_dict_str is NaN for row {index}")
48/169:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        print(category_tags)
        print("_______________")
        print(tags_dict.values())

        # check if there is at least one tag that is the same
        if any(any(t in category_tags for t in tag) for tag in tags_dict.values()):
            print(f"At least one tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
        else:
            print(f"No tag in tags_dict for category '{category}' is the same as the tags in all_tags_by_category")
    else:
        print(f"tags_dict_str is NaN for row {index}")
48/170:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        print(category_tags)
        print("_______________")
        print(tags_dict.values())

        # check if there is at least one tag that is the same
       # get the tags for the current category
        # check if each tag in tags_dict.values() has a corresponding tag in category_tags
        for tag in tags_dict.values():
            if all(t in category_tags for t in tag):
                print(f"All tags in {tag} for category '{category}' are the same as the tags in all_tags_by_category")
            else:
                print(f"Some tags in {tag} for category '{category}' are not the same as the tags in all_tags_by_category")    else:
        print(f"tags_dict_str is NaN for row {index}")
48/171:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        print(category_tags)
        print("_______________")
        print(tags_dict.values())

        # check if there is at least one tag that is the same
        # get the tags for the current category
        # check if each tag in tags_dict.values() has a corresponding tag in category_tags
        for tag in tags_dict.values():
            if all(t in category_tags for t in tag):
                print(f"All tags in {tag} for category '{category}' are the same as the tags in all_tags_by_category")
            else:
                print(f"Some tags in {tag} for category '{category}' are not the same as the tags in all_tags_by_category")    else:
        print(f"tags_dict_str is NaN for row {index}")
48/172:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        print(category_tags)
        print("_______________")
        print(tags_dict.values())

        # check if there is at least one tag that is the same
        # get the tags for the current category
        # check if each tag in tags_dict.values() has a corresponding tag in category_tags
        for tag in tags_dict.values():
            if all(t in category_tags for t in tag):
                print(f"All tags in {tag} for category '{category}' are the same as the tags in all_tags_by_category")
            else:
                print(f"Some tags in {tag} for category '{category}' are not the same as the tags in all_tags_by_category")    
    else:
        print(f"tags_dict_str is NaN for row {index}")
48/173:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        print(category_tags)
        print("_______________")
        print(tags_dict.values())

        # check if there is at least one tag that is the same
        # get the tags for the current category
        # check if each tag in tags_dict.values() has a corresponding tag in category_tags
        non_matching_tags = []

        for tag in tags_dict.values():
            if all(t in category_tags for t in tag):
                print(f"All tags in {tag} for category '{category}' are the same as the tags in all_tags_by_category")
            else:
                non_matching_tags.extend([t for t in tag if t not in category_tags])

        if non_matching_tags:
            print(f"The following tags do not have any corresponding tags in category_tags: {non_matching_tags}")
        else:
            print(f"tags_dict_str is NaN for row {index}")
48/174:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
non_matching_tags = []
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        print(category_tags)
        print("_______________")
        print(tags_dict.values())

        # check if there is at least one tag that is the same
        # get the tags for the current category
        # check if each tag in tags_dict.values() has a corresponding tag in category_tags
        

        for tag in tags_dict.values():
            if all(t in category_tags for t in tag):
                print(f"All tags in {tag} for category '{category}' are the same as the tags in all_tags_by_category")
            else:
                non_matching_tags.extend([t for t in tag if t not in category_tags])

if non_matching_tags:
    print(f"The following tags do not have any corresponding tags in category_tags: {non_matching_tags}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/175:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
non_matching_tags = []
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        # print(category_tags)
        # print("_______________")
        # print(tags_dict.values())

        # check if there is at least one tag that is the same
        # get the tags for the current category
        # check if each tag in tags_dict.values() has a corresponding tag in category_tags
        

        for tag in tags_dict.values():
            if all(t in category_tags for t in tag):
                print(f"All tags in {tag} for category '{category}' are the same as the tags in all_tags_by_category")
            else:
                non_matching_tags.extend([t for t in tag if t not in category_tags])

if non_matching_tags:
    print(f"The following tags do not have any corresponding tags in category_tags: {non_matching_tags}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/176:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
non_matching_tags = []
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        # print(category_tags)
        # print("_______________")
        # print(tags_dict.values())

        # check if there is at least one tag that is the same
        # get the tags for the current category
        # check if each tag in tags_dict.values() has a corresponding tag in category_tags
        

        for tag in tags_dict.values():
            if all(t in category_tags for t in tag):
                print(f"All tags in {tag} for category '{category}' are the same as the tags in all_tags_by_category")
            else:
                non_matching_tags.extend([t for t in tag if t in category_tags])

if non_matching_tags:
    print(f"The following tags do not have any corresponding tags in category_tags: {non_matching_tags}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/177:
import pandas as pd
# read the dataframe from the csv file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# define a function to get all the tags for a group
def get_all_tags(group):
    # concatenate all the tags for the group
    all_tags = ",".join(group["tags"].dropna())
    return all_tags

# group the dataframe by "Product category" and apply the function to each group
all_tags_by_category = df_tags_tech.groupby("Product category").apply(get_all_tags)

# print the result
# print(all_tags_by_category)
import ast
# print fish finding group tags
# iterate over the rows of the dataframe
non_matching_tags = []
for index, row in df_tags_tech.iterrows():
    # get the tags_dict for the current row
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        # get the tags for the current category
        # get the tags for the current category
        category_tags = all_tags_by_category[category].split(',')
        # print(category_tags)
        # print("_______________")
        # print(tags_dict.values())

        # check if there is at least one tag that is the same
        # get the tags for the current category
        # check if each tag in tags_dict.values() has a corresponding tag in category_tags
        

        for index, row in df_tags_tech.iterrows():
            tags_dict_str = row["tags_dict"]
            if pd.notna(tags_dict_str):
                tags_dict = ast.literal_eval(tags_dict_str)
                category = row["Product category"]
                category_tags = all_tags_by_category[category].split(',')

                for tag in tags_dict.values():
                    non_matching_tags.extend([t for t in tag if t not in category_tags])

if non_matching_tags:
    print(f"The following tags do not have any corresponding tags in category_tags: {non_matching_tags}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/178:
non_matching_tags = []

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = all_tags_by_category[category].split(',')

        for tag in tags_dict.values():
            non_matching_tags.extend([t for t in tag if t not in category_tags])

if non_matching_tags:
    print(f"The following tags do not have any corresponding tags in category_tags: {set(non_matching_tags)}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/179:
non_matching_tags = []

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = all_tags_by_category[category].split(',')

        for tag in tags_dict.values():
            non_matching_tags.extend([(t, category) for t in tag if t not in category_tags])

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/180:
non_matching_tags = []

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = all_tags_by_category[category].split(',').strip()

        for tag in tags_dict.values():
            non_matching_tags.extend([(t, category) for t in tag if t not in category_tags])

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/181:
non_matching_tags = []

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = all_tags_by_category[category].split(',')

        for tag in tags_dict.values():
            non_matching_tags.extend([(t, category) for t in tag if t not in category_tags])

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/182:
non_matching_tags = []

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            non_matching_tags.extend([(t.strip(), category) for t in tag if t.strip() not in category_tags])

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/183:
non_matching_tags = []
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            non_matching_tags.extend([(t.strip(), category) for t in tag if t.strip() not in category_tags])

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/184:
non_matching_tags = []
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            print(tag)
            non_matching_tags.extend([(t.strip(), category) for t in tag if t.strip() not in category_tags])

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/185:
non_matching_tags = []
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            for t in tag:
                print(tag)
                non_matching_tags.extend([(t.strip(), category) for t in tag if t.strip() not in category_tags])

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/186:
non_matching_tags = []
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            for t in tag:
                print(t)
                non_matching_tags.extend([(t.strip(), category) for t in tag if t.strip() not in category_tags])

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/187:
non_matching_tags = []
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            for t in tag:
                t = t.strip()
                if t not in category_tags:
                    print(t, category_tags)
                    non_matching_tags.append((t, category))
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/188:
non_matching_tags = []
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            for t in tag:
                t = t.strip()
                for tags_cat in category_tags:
                    if t not in tags_cat:
                        print(t, tags_cat)
                        non_matching_tags.append((t, category))
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/189:
non_matching_tags = []
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            for t in tag:
                t = t.strip()
                for tags_cat in category_tags:
                    if t not in tags_cat:
                        print(t, "not in ", tags_cat)
                        non_matching_tags.append((t, category))
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/190:
non_matching_tags = []
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict = ast.literal_eval(tags_dict_str)
        category = row["Product category"]
        category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]

        for tag in tags_dict.values():
            for t in tag:
                t = t.strip()
                if t not in category_tags:
                    print(t, " not in " , category_tags)
                    non_matching_tags.append((t, category))
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/191:
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict = ast.literal_eval(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    if t not in category_tags:
                        print(t, " not in " , category_tags)
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/192:
import json

non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/193:
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            # Replace single quotes with double quotes
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/194:
non_matching_tags = []
import json
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            # Replace single quotes with double quotes
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/195:
non_matching_tags = []
import json
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            # Replace single quotes with double quotes
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    print(t)
                    print(category_tags)
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/196:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters from the category tags
    category_tags = [tag.strip("'") for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/197:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters from the category tags
    category_tags = [tag.strip("'") for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    print(t)
                    print(category_tags)
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/198:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/199:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    print(t)
                    print(category_tags)
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/200:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    print(t)
                    # add ' ' to the tag if it is not already there
                    if t[0] != "'":
                        t = "'" + t
                    print(t)
                    print(category_tags)
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/201:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    print(t)
                    # add ' ' to the tag if it is not already there
                    if t[0] != "'":
                        t = "'" + t + "'"
                    print(t)
                    print(category_tags)
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/202:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    # add ' ' to the tag if it is not already there
                    if t[0] != "'":
                        t = "'" + t + "'"
                    print(t)
                    print(category_tags)
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/203:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    # add ' ' to the tag if it is not already there
                    if t[0] != "'":
                        t = "'" + t + "'"

                    print(type(category_tags))
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/204:

import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    # add ' ' to the tag if it is not already there
                    if t[0] != "'":
                        t = "'" + t + "'"

                    if t not in category_tags:
                        non_matching_tags.append((t, category))
                    else:
                        print(t)
                        print(category_tags)

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/205:
import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    # Add ' ' to the tags if it is not already there
    category_tags = ["'" + tag + "'" if tag[0] != "'" else tag for tag in category_tags]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    # add ' ' to the tag if it is not already there
                    if t[0] != "'":
                        t = "'" + t + "'"

                    if t not in category_tags:
                        non_matching_tags.append((t, category))
                    else:
                        print(t)
                        print(category_tags)

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/206:
import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    # Add ' ' to the tags if it is not already there
    category_tags = ["'" + tag + "'" if tag[0] != "'" else tag for tag in category_tags]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    # add ' ' to the tag if it is not already there
                    if t[0] != "'":
                        t = "'" + t + "'"

                    if t not in category_tags:
                        non_matching_tags.append((t, category))


if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/207:
import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    # Add ' ' to the tags if it is not already there
    category_tags = ["'" + tag + "'" if tag and tag[0] != "'" else tag for tag in category_tags]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    t = t.strip()
                    # add ' ' to the tag if it is not already there
                    if t and t[0] != "'":
                        t = "'" + t + "'"

                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/208:
import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    # Strip leading/trailing whitespace and quotes from t
                    t = t.strip("' ").strip()
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/209:
import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    # Strip leading/trailing whitespace and quotes from t
                    t = t.strip("' ").strip()
                    if t not in category_tags:
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/210:
import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    # Strip leading/trailing whitespace and quotes from t
                    t = t.strip("' ").strip()
                    if t not in category_tags:
                        print(f"t: {t}")
                        print(f"category_tags: {category_tags}")
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/211:
import json
non_matching_tags = []

for category, group in df_tags_tech.groupby("Product category"):
    # Remove extra characters and whitespace from the category tags
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    # Strip leading/trailing whitespace and quotes from t
                    t = t.strip("' ").strip()
                    if t not in category_tags:
                        print(f"t: {t}")
                        print(f"category_tags: {category_tags}")
                        non_matching_tags.append((t, category))

if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print(f"tags_dict_str is NaN for row {index}")
48/212:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
else:
    print("All tags in dict_tags have corresponding product tags.")
48/213:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/214:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for category, group in df_tags_tech.groupby("Product category"):
    for index, row in group.iterrows():
        tags_dict_str = row["tags_dict"]
        if pd.notna(tags_dict_str):
            tags_dict_str = tags_dict_str.replace("'", '"')
            tags_dict = json.loads(tags_dict_str)

            for tag in tags_dict.values():
                for t in tag:
                    # Strip leading/trailing whitespace and quotes from t
                    t = t.strip("' ").strip()
                    if t not in product_tags:
                        non_matching_tags.append((t, category))

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag, category in set(non_matching_tags):
        print(f"Tag: {tag}, Category: {category}")
else:
    print("All tags in dict_tags have corresponding product tags.")
48/215:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/216:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        keys_to_delete = []
        for key, tag in tags_dict.items():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)
                    keys_to_delete.append(key)

        # Delete keys from tags_dict
        for key in keys_to_delete:
            del tags_dict[key]

        # Update the row with the modified tags_dict
        row["tags_dict"] = json.dumps(tags_dict)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/217:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/218:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for key, tag in list(tags_dict.items()):  # Create a copy of items for iteration
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)
                    # Remove the non-matching tag from the list in tags_dict
                    tag.remove(t)

            # If the list in tags_dict is empty after removals, delete the key
            if not tag:
                del tags_dict[key]

        # Update the row with the modified tags_dict
        row["tags_dict"] = json.dumps(tags_dict)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/219:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for key, tag in list(tags_dict.items()):  # Create a copy of items for iteration
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)
                    # Remove the non-matching tag from the list in tags_dict
                    tag.remove(t)

            # If the list in tags_dict is empty after removals, delete the key
            if not tag:
                del tags_dict[key]

        # Update the DataFrame with the modified tags_dict
        df_tags_tech.loc[index, "tags_dict"] = json.dumps(tags_dict)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/220:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for key, tag in list(tags_dict.items()):  # Create a copy of items for iteration
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)
                    # Remove the non-matching tag from the list in tags_dict
                    tag.remove(t)

            # If the list in tags_dict is empty after removals, delete the key
            if not tag:
                del tags_dict[key]

        # Update the DataFrame with the modified tags_dict
        df_tags_tech.loc[index, "tags_dict"] = json.dumps(tags_dict)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/221: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11_test.xlsx", index=False)
48/222: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11_test.csv", index=False)
48/223: df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11_test.csv", index=False)
48/224:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for key, tag in list(tags_dict.items()):  # Create a copy of items for iteration
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)
                    # Remove the non-matching tag from the list in tags_dict
                    tag.remove(t)

            # If the list in tags_dict is empty after removals, delete the key
            if not tag:
                del tags_dict[key]

        # Update the DataFrame with the modified tags_dict
        df_tags_tech.loc[index, "tags_dict"] = json.dumps(tags_dict)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/225: df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11_test.csv", index=False)
48/226:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for key, tag in list(tags_dict.items()):  # Create a copy of items for iteration
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)
                    # Remove the non-matching tag from the list in tags_dict
                    tag.remove(t)

            # If the list in tags_dict is empty after removals, delete the key
            if not tag:
                del tags_dict[key]

        # Update the DataFrame with the modified tags_dict
        df_tags_tech.loc[index, "tags_dict"] = json.dumps(tags_dict)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/227:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
       
        # choose 8 tags from the tags list for each product
        prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/228:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
       
        # choose 8 tags from the tags list for each product
        prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/229:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
48/230:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
48/231:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
48/232:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
48/233:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/234:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/235: print(url_text_dict)
48/236:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
48/237:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    # gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
48/238:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', please identify five APPLICATION AND USAGE tags. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of exactly five tags."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/239:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
48/240:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

# Strip the whitespace and filter out unwanted categories
unwanted_categories = ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']
categories = [category.strip() for category in categories if category.lower() not in unwanted_categories]
48/241:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
48/242:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these top-level tags: ['Product Type', 'Technology', 'Application'] 
    For each top-level tag, select 8 specific and descriptive sub-tags from the list of sub-tags. Always use 8 sub-tags for each top-level tag. Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag.

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
48/243: print(tags_dict)
48/244:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/245:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
       
        # choose 8 tags from the tags list for each product
        prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/246:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
48/247:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
48/248: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/249:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/250:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/251: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/252:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/253:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/254:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        tags_dict = json.loads(tags_dict_str)

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/255:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
48/256:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/257:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/258: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
48/259:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
48/260:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
48/261:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
48/262:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
48/263:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
48/264:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
48/265:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/266:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
48/267: print(url_text_dict)
48/268:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
48/269:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    # gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
48/270:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', please identify five APPLICATION AND USAGE tags. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of exactly five tags."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/1:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
49/2:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
49/3:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
49/4:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
49/5:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
49/6:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
49/7: print(url_text_dict)
49/8:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
49/9:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    # gpt_model = "gpt-35-turbo-16k"
    gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
49/10:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', please identify five APPLICATION AND USAGE tags. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of exactly five tags."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/11:
import json

# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
49/12:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
49/13: %history -p
49/14:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

# Iterate over the groups
for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    tags = [tag.strip("' ").strip() for tags in tags_list for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
49/15:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

# Iterate over the groups
for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    if isinstance(tags_list, list):
        tags = [tag.strip("' ").strip() for tags in tags_list for tag in tags.split(",")]
        # Remove duplicates
        tags = list(set(tags))
        # Add the tags to the dictionary
        all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
49/16:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

# Iterate over the groups
for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    #check if tags are na
    if not pd.isna(tags_list):
        tags = [tag.strip("' ").strip() for tags in tags_list for tag in tags.split(",")]
        # Remove duplicates
        tags = list(set(tags))
        # Add the tags to the dictionary
        all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
49/17:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

# Iterate over the groups
for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    #check if tags are na
    tags = [tag.strip("' ").strip() for tags in tags_list for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
49/18:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
49/19:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
49/20:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
49/21:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
49/22:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
49/23:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
49/24:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
49/25:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
49/26:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
49/27:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
49/28: print(url_text_dict)
49/29:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
49/30:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    # gpt_model = "gpt-35-turbo-16k"
    gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
49/31:

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/32:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/33:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    gpt_model = "gpt-35-turbo"
    # gpt_model = "gpt-35-turbo-16k"
    # gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
49/34:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                tags = general_gpt(prompt)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/35:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
49/36:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

categories = [category.strip() for category in categories if category.lower() not in unwanted_categories]
49/37:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

categories = [category.strip() for category in categories]
49/38:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
49/39:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    filtered_tags = [tag for tag in tags if tag.lower() not in ['category', 'attribute', 'technology', 'applications', 'specifications', 'communications', 'sensors', 'products']]

    prompt = """
    Given the list of sub-tags: {} and the products: {} in category {}:
    Use these top-level tags: ['Product Type', 'Technology', 'Application'] 
    For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is neccecery to describe all the sides of the product. Try to have sub-tags for each top-level tag. Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag.

    Return the top-level tags as a dictionary where the keys are the top-level tags and the values are lists of the sub-tags. Only return the dictionary no explanation is needed. Only use sub-tags from the list of sub-tags.
    """.format(filtered_tags,  products, category)
        
    category_tags = general_gpt(prompt)
    print(category)
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
    time.sleep(2)
49/40: print(tags_dict)
49/41:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/42:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
       
        # choose 8 tags from the tags list for each product
        prompt = "Please refer to this list of tags: {}. For the product: {}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags.".format(
                    category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/43:
def general_gpt(prompt: str):
    # make chat gpt completion function with streamlit
    openai.api_key = st.secrets["openai"]["api_key"]
    openai.api_type = "azure"
    openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
    openai.api_version = "2023-03-15-preview"
    # gpt_model = "gpt-35-turbo"
    gpt_model = "gpt-35-turbo-16k"
    # gpt_model = "gpt-4"
    completion = openai.ChatCompletion.create(
        deployment_id=gpt_model,
        messages=[
            {
                "role": "user",
                "content": "{}".format(prompt),
            }
        ],
        temperature=0.3,
        max_tokens=1500,
        top_p=1.0,
        frequency_penalty=0.1,
        presence_penalty=0.1,
    )
    return str(completion.choices[0].message.content)
49/44:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
       
        # choose 8 tags from the tags list for each product
        prompt = "Please refer to this list of tags: {}. For the product: {}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags.".format(
                    category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/45:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
49/46:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/47: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
49/48:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
49/49:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
49/50: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
49/51:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
49/52:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
49/53:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
49/54:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
49/55:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
       
        # choose 8 tags from the tags list for each product
        prompt = "Please refer to this list of tags: {}. For the product: {}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags.".format(
                    category_tags, row["Product_Name"]
        )
        tags = general_gpt(prompt)
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/56:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
49/57:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
49/58: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
49/59:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
49/60:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
49/61: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
49/62:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
49/63:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
49/64:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
49/65:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
49/66:
from openai import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
49/67:
from openai import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
49/68:
from openai import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
49/69:
from openai import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
49/70:
from openai import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
49/71:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
49/72:
response = client.Completion.create(
  engine="gpt-4-1106-preview",
  prompt="What are kongsberg",
  max_tokens=60
)

print(response.choices[0].text.strip())
49/73:
response = client.chat.Completion.create(
  engine="gpt-4-1106-preview",
  prompt="What are kongsberg",
  max_tokens=60
)

print(response.choices[0].text.strip())
49/74:
response = client.chat.completion.create(
  engine="gpt-4-1106-preview",
  prompt="What are kongsberg",
  max_tokens=60
)

print(response.choices[0].text.strip())
49/75:
response = client.chat.completions.create(

  engine="gpt-4-1106-preview",
  prompt="What are kongsberg",
  max_tokens=60
)

print(response.choices[0].text.strip())
49/76:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  prompt="What are kongsberg",
  max_tokens=60
)

print(response.choices[0].text.strip())
49/77:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  message="What are kongsberg",
  max_tokens=60
)

print(response.choices[0].text.strip())
49/78:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages="What are kongsberg",
  max_tokens=60
)

print(response.choices[0].text.strip())
50/1:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
50/2:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
50/3:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
50/4:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages="What are kongsberg",
  max_tokens=60
)

print(response.choices[0].text.strip())
50/5:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  max_tokens=60
)

print(response.choices[0].text.strip())
50/6:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}],
  max_tokens=60
)

print(response.choices[0].text.strip())
50/7:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}],
  max_tokens=60
)

print(response.choices[0].messages.strip())
50/8:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}],
  max_tokens=60
)

print(response.choices[0].message.strip())
50/9:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}],
  max_tokens=60
)

# print(response.choices[0].message.strip())
print(response.choices[0].message.strip())
50/10:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}],
  max_tokens=60
)

# print(response.choices[0].message.strip())
print(response.choices[0].message)
50/11:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}],
  max_tokens=60
)

# print(response.choices[0].message.strip())
print(response.choices[0].message.content.strip())
50/12:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}],
  max_tokens=60,
  response_format={ "type": "json_object" },
)

# print(response.choices[0].message.strip())
print(response.choices[0].message.content.strip())
50/13:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant. asnwer in json"},
    {"role": "user", "content": "Hello!"}],
  max_tokens=60,
  response_format={ "type": "json_object" },
)

# print(response.choices[0].message.strip())
print(response.choices[0].message.content.strip())
50/14:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
50/15:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
50/16:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
50/17:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
50/18:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
50/19:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
50/20:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
50/21: print(url_text_dict)
50/22:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
50/23:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
50/24:
response = client.chat.completions.create(
    
  model="gpt-4-1106-preview",
  messages=[
    {"role": "system", "content": "You are a helpful assistant. asnwer in json"},
    {"role": "user", "content": "Hello!"}],
  max_tokens=200,
  response_format={ "type": "json_object" },
)

# print(response.choices[0].message.strip())
print(response.choices[0].message.content.strip())
50/25:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."},
                        {"role": "user", "content": "{}".format(prompt)}
                        ],
                    max_tokens=200,
                    response_format={ "type": "json_object" }
                )
                tags = response.choices[0].message.content.strip()
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/26:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."},
                        {"role": "user", "content": "{}".format(prompt)}
                        ],
                    max_tokens=200,
                    response_format={ "type": "json_object" }
                )
                tags = response.choices[0].message.content.strip()
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                st.write(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                st.info(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/27:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."},
                        {"role": "user", "content": "{}".format(prompt)}
                        ],
                    max_tokens=200,
                    response_format={ "type": "json_object" }
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                print(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                print(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/28:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                        ],
                    max_tokens=200,
                    response_format={ "type": "json_object" }
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                print(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                print(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/29:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                        ],
                    max_tokens=200,
                    response_format={ "type": "json_object" }
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                print(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                print(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/30:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print(len(tag_list))
50/31:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        if len(tag_split) < 3:
            tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print((tag_list))
50/32:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        
        tag_list.append(tag)
    

# for i in tag_list:
#     print(i)
print((tag_list))
50/33:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        
        tag_list.append(tag.values())
    

# for i in tag_list:
#     print(i)
print((tag_list))
50/34:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        # make tag_split to dict
        tag = dict(zip(tag_split, tag_split))
        print(tag.values)
        tag_list.append(tag.values)
    

# for i in tag_list:
#     print(i)
print((tag_list))
50/35:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        # make tag_split to dict
        tag = dict(zip(tag_split, tag_split))
        print(tag.values())
        tag_list.append(tag.values)
    

# for i in tag_list:
#     print(i)
print((tag_list))
50/36:

tag_list = []
for row in df_tags_tech.itertuples():
    tags = row.new_tags
    tags = tags.split(",")
    tags = [tag.strip() for tag in tags]
    tags = [tag.rstrip(".") for tag in tags]
    # remove duplicates
    # add all tags into one big list
    for tag in tags:
        tag_split = tag.split(" ")
        # make tag_split to dict
        tag = dict(zip(tag_split, tag_split))
        print(tag_split)
        print(tag.values())
        tag_list.append(tag.values)
    

# for i in tag_list:
#     print(i)
print((tag_list))
50/37:
tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict = row.new_tags
    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
50/38:
import ast

tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict_str = row.new_tags
    try:
        tags_dict = ast.literal_eval(tags_dict_str)
    except (SyntaxError, ValueError):
        print(f"Error parsing string to dict: {tags_dict_str}")
        continue

    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
50/39:
import ast

tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict_str = row.new_tags
    try:
        tags_dict = ast.literal_eval(tags_dict_str)
    except (SyntaxError, ValueError):
        print(f"Error parsing string to dict: {tags_dict_str}")
        continue

    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
print(len(tag_list))
50/40:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

categories = [category.strip() for category in categories]
50/41:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
50/42:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    tags
    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags: ['Product Type', 'Technology', 'Application']. For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have sub-tags for each top-level tag. Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag."
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/43:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    tags
    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags: ['Product Type', 'Technology', 'Application']. For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag."
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/44:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    tags
    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags: ['Product Type', 'Technology', 'Application']. For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/45:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    tags
    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/46:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]

    # Filter out unwanted words from tags
    tags
    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/47:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    print(products)
    # Filter out unwanted words from tags
    tags
    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/48:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['category'] == category]['tags']
    print(tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/49:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['tags']
    print(tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/50:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['Tags']
    print(tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/51:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']
    print(tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/52:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/53:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/54:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as 4 subtags for each top-level tag. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/55:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {tags} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as 5 subtags for each top-level tag. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/56:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as 5 subtags for each top-level tag. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/57:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category, but no more than 8. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/58:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/59:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/60:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/61: print(tags_dict)
50/62:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/63:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
       
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/64:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable

        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/65:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

        # Remove duplicates
        all_tags_in_category = list(set(all_tags_in_category))
        print(all_tags_in_category)

        messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
        },
        {
            "role": "user",
            "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
            ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
            For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
            Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
        }
        ]

        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        print(category)
        category_tags = response.choices[0].message.content.strip()
        print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
50/66:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
tags_dict[category] = category_tags
50/67: print(tags_dict)
50/68:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))
    print(all_tags_in_category)

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    print(category)
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
50/69:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict = ast.literal_eval(tags_dict_str)
            for tags in tags_dict.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
50/70:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict_temp = ast.literal_eval(tags_dict_str)
            for tags in tags_dict_temp.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
50/71:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/72:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable

        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/73:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/74:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
50/75: df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
50/76:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
50/77:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
50/78: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
50/79:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        # if len(ant_tags) > 12:
        if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            tags_dict,  row["Product category"], row["Product_Name"]
            )
            tags = general_gpt(prompt)
            print("New tags: ", tags)
            df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

print(df_tech_new_tags["tags"])
50/80:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
50/81:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
50/82:
import json
df_tags_tech = pd.read_csv("./data/df_tags_use_app_15_11.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
50/83:
df_tech_new_tags.to_excel("./data/tags_gpt4.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_gpt4.csv", index=False)
50/84:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
        # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            # prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            # tags_dict,  row["Product category"], row["Product_Name"]
            # )
            # tags = general_gpt(prompt)
            # print("New tags: ", tags)
            # df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

# print(df_tech_new_tags["tags"])
50/85:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        if len(ant_tags) > 12:
        # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            # prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            # tags_dict,  row["Product category"], row["Product_Name"]
            # )
            # tags = general_gpt(prompt)
            # print("New tags: ", tags)
            # df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

# print(df_tech_new_tags["tags"])
50/86:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        print(ant_tags)
        if len(ant_tags) > 12:
        # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            # prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            # tags_dict,  row["Product category"], row["Product_Name"]
            # )
            # tags = general_gpt(prompt)
            # print("New tags: ", tags)
            # df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

# print(df_tech_new_tags["tags"])
50/87:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/tags_gpt4.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
50/88:
import json
df_tags_tech = pd.read_csv("./data/tags_gpt4.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
50/89:
# one culumn of my df is called tags and contains a dict with tags for each product with keys tag. I want to extract the values of the keys tag and put them in a list for each row. How can I do this?
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
print(df['tags'])
df['tag_values'] = df['tags'].apply(lambda x: x['tag'] if isinstance(x, dict) and 'tag' in x else [])
print(df['tag_values'])
50/90:
# one culumn of my df is called tags and contains a dict with tags for each product with keys tag. I want to extract the values of the keys tag and put them in a list for each row. How can I do this?
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
print(df['tags'])
# make the df['tags'] to a dict
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x))
print(df['tags'])
df['tag_values'] = df['tags'].apply(lambda x: x['tag'] if isinstance(x, dict) and 'tag' in x else [])
print(df['tag_values'])
50/91:
# one culumn of my df is called tags and contains a dict with tags for each product with keys tag. I want to extract the values of the keys tag and put them in a list for each row. How can I do this?
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
print(df['tags'])
# make the df['tags'] to a dict
df['tags'] = df['tags'].strip().apply(lambda x: ast.literal_eval(x))
print(df['tags'])
df['tag_values'] = df['tags'].apply(lambda x: x['tag'] if isinstance(x, dict) and 'tag' in x else [])
print(df['tag_values'])
50/92:
# one culumn of my df is called tags and contains a dict with tags for each product with keys tag. I want to extract the values of the keys tag and put them in a list for each row. How can I do this?
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
print(df['tags'])
# make the df['tags'] to a dict
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x))
print(df['tags'])
df['tag_values'] = df['tags'].apply(lambda x: x['tag'] if isinstance(x, dict) and 'tag' in x else [])
print(df['tag_values'])
50/93:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Print the 'tags' column after conversion
print(df['tags'])

# Extract the values of the key 'tag' and put them in a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tag'] if isinstance(x, dict) and 'tag' in x else [])

# Print the 'tag_values' column
print(df['tag_values'])
50/94:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Print the 'tags' column after conversion
print(df['tags'])

# Extract the values of the key 'tag' and put them in a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tag'] if isinstance(x, dict) and 'tag' in x else [])

# Print the 'tag_values' column
print(df['tag_values'])
50/95:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x else [])

# Print the 'tag_values' column
print(df['tag_values'])

# Print the 'tag_values' column
print(df['tag_values'])
50/96:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x['tags'], str) else x['tags'] if isinstance(x, dict) and 'tags' in x else [])

# Print the 'tag_values' column
print(df['tag_values'])

# Print the 'tag_values' column
print(df['tag_values'])
50/97:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])
print(df.columns)

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x['tags'], str) else x['tags'] if isinstance(x, dict) and 'tags' in x else [])

# Print the 'tag_values' column
print(df['tag_values'])

# Print the 'tag_values' column
print(df['tag_values'])
50/98:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df.columns)
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x['tags'], str) else x['tags'] if isinstance(x, dict) and 'tags' in x else [])

# Print the 'tag_values' column
print(df['tag_values'])

# Print the 'tag_values' column
print(df['tag_values'])
50/99:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df.columns)
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x.split(', ') if isinstance(x, str) else x if isinstance(x, list) else [])

# Print the 'tag_values' column
print(df['tag_values'])
50/100:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df.columns)
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
print(df['tag_values'])
50/101:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df.columns)
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
print(df['tag_values'])
50/102:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
print(df.columns)
print(df['tags'].unique())
# Print the 'tags' column
print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
print(df['tag_values'][10:20])
50/103:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(df[df['tag_values'].apply(lambda x: len(x) == 0)])
50/104:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")
# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/105:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty
print(len(df[df['tags'].apply(lambda x: len(x) == 0)]))

# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/106:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))



# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/107:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))
print(len(df[df['tags'] == ""]))



# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/108:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))
print(len(df[df['tags'] == ""]))
print(len(df[df['tags'] == "[]"]))



# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/109:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))
print(len(df[df['tags'] == ""]))
print(len(df[df['tags'] == "[]"]))
print(len(df[df['tags'] == "['']"]))



# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/110:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))
print(len(df[df['tags'] == ""]))
print(len(df[df['tags'] == "[]"]))
print(len(df[df['tags'] == "['']"]))
#print len of tags that are empty dicts
print(len(df[df['tags'] == "{}"]))



# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/111:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))

print(len(df[df['tags'].apply(lambda x: len(x) == 0)]))

# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/112:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))
#remove if tags is null
df = df[df['tags'].notnull()]
print(len(df[df['tags'].apply(lambda x: len(x) == 0)]))

# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
50/113:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))
#remove if tags is null
df = df[df['tags'].notnull()]
print(len(df[df['tags'].apply(lambda x: len(x) == 0)]))

# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
52/1:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
53/1:
from openai._client import OpenAI
import streamlit as st

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
53/2:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
)
53/3:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.csv", "rb"),
  purpose='assistants'
)
53/4:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.csv", "rb"),
  purpose='assistants'
)
53/5:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.csv", "rb"),
  purpose='assistants'
)
53/6:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
53/7:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
json_file = csv_file.to_json()
53/8:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
53/9:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
json_file = csv_file.to_json()
53/10: print(json_file)
53/11:
# pretty print json 
parsed = json.loads(json_file)
print(json.dumps(parsed, indent=4, sort_keys=True))
53/12:
file = client.files.create(
  file=json_file,
  purpose='assistants'
)
53/13:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# save to json file for later use
csv_file.to_json("data/good_data/tags_gpt4.json")
53/14:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
53/15:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# save to json file for later use
csv_file.to_json("data/good_data/tags_gpt4.json")
53/16:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
53/17:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
53/18:
thread = client.beta.threads.create(
  messages=[
    {
      "role": "user",
      "content": "Can you write new categories for the products?",
      "file_ids": [file.id]
    }
  ]
)
53/19:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id
)
53/20:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
53/21:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

print(messages)
53/22:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)
53/23:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
53/24:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# save to json file for later use
csv_file.to_json("data/good_data/tags_gpt4.json")
53/25:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
53/26:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
53/27:
thread = client.beta.threads.create(
  messages=[
    {
      "role": "user",
      "content": "Can you write new categories for the products?",
      "file_ids": [file.id]
    }
  ]
)
53/28:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id
)
53/29:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)
53/30:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)
53/31:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)
53/32:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
53/33:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# save to json file for later use
csv_file.to_json("data/good_data/tags_gpt4.json")
53/34:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
53/35:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
53/36:
thread = client.beta.threads.create(
  messages=[
    {
      "role": "user",
      "content": "Can you write new categories for the products?",
      "file_ids": [file.id]
    }
  ]
)
53/37:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id
)
53/38:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)
53/39:
thread = client.beta.threads.create(
  messages=[
    {
      "role": "user",
      "content": "Can you write new categories for the products?",
      "file_ids": [file.id]
    }
  ]
)
53/40:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id
)
53/41:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)
53/42:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)
53/43:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

# for message in messages.data:
#     for content in message.content:
#         print(content.text.value)

print(messages.data[0].content[0].text.value)
53/44:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

# for message in messages.data:
#     for content in message.content:
#         print(content.text.value)

print(messages.data[0].content[0].text.value)
53/45:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

# for message in messages.data:
#     for content in message.content:
#         print(content.text.value)

print(messages.data[0].content[0].text)
53/46:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id
  instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
)
53/47:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
)
53/48:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

# for message in messages.data:
#     for content in message.content:
#         print(content.text.value)

print(messages.data[0].content[0].text.value)
53/49:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
53/50:
# in the same thread, ask a question
question = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
   messages=[{
      "role": "user",
      "content": "Place the products in the new categories and return json file.",
      "file_ids": [file.id]
    }]
)
53/51:
# in the same thread, ask a question
question = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
   message=[{
      "role": "user",
      "content": "Place the products in the new categories and return json file.",
      "file_ids": [file.id]
    }]
)
53/52:
# in the same thread, ask a question
question = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  messages=[{
      "role": "user",
      "content": "Place the products in the new categories and return json file.",
      "file_ids": [file.id]
    }]
)
53/53:
# in the same thread, ask a question
question = client.beta.threads.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  messages=[{
      "role": "user",
      "content": "Place the products in the new categories and return json file.",
      "file_ids": [file.id]
    }]
)
53/54:
thread = client.beta.threads.create(
)
53/55:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
)
53/56:
thread = client.beta.threads.create(
)
53/57:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
53/58:
thread = client.beta.threads.create(
)
53/59:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
53/60:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
53/61:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
)
53/62:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
53/63:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
53/64:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# save to json file for later use
csv_file.to_json("data/good_data/tags_gpt4.json")
53/65:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
53/66:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
53/67:
thread = client.beta.threads.create(
)
53/68:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
53/69:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
)
53/70:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
53/71:
# # in the same thread, ask a question
# question = client.beta.threads.runs.create(
#   thread_id=thread.id,
#   assistant_id=assistant.id,
#    messages=[{
#       "role": "user",
#       "content": "Place the products in the new categories and return json file.",
#       "file_ids": [file.id]
#     }]
# )
53/72: # get the answer
53/73:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
53/74:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# save to json file for later use
csv_file.to_json("data/good_data/tags_gpt4.json")
53/75:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
53/76:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
53/77:
thread = client.beta.threads.create(
)
53/78:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
53/79:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="Answer the question as accurately as possible."

)
53/80:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
53/81:
# # in the same thread, ask a question
# question = client.beta.threads.runs.create(
#   thread_id=thread.id,
#   assistant_id=assistant.id,
#    messages=[{
#       "role": "user",
#       "content": "Place the products in the new categories and return json file.",
#       "file_ids": [file.id]
#     }]
# )
53/82: # get the answer
53/83:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="Answer the questions in the message"

)
53/84:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
53/85:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
53/86:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# save to json file for later use
csv_file.to_json("data/good_data/tags_gpt4.json")
53/87:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
53/88:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
53/89:
thread = client.beta.threads.create(
)
53/90:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
53/91:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="Answer the questions in the message"

)
53/92:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
53/93:
# # in the same thread, ask a question
# question = client.beta.threads.runs.create(
#   thread_id=thread.id,
#   assistant_id=assistant.id,
#    messages=[{
#       "role": "user",
#       "content": "Place the products in the new categories and return json file.",
#       "file_ids": [file.id]
#     }]
# )
53/94: # get the answer
54/1:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
54/2:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# save to json file for later use
csv_file.to_json("data/good_data/tags_gpt4.json")
54/3:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
54/4:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
54/5:
thread = client.beta.threads.create(
)
54/6:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
54/7:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="Answer the questions in the message"

)
54/8:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/9:
# # in the same thread, ask a question
# question = client.beta.threads.runs.create(
#   thread_id=thread.id,
#   assistant_id=assistant.id,
#    messages=[{
#       "role": "user",
#       "content": "Place the products in the new categories and return json file.",
#       "file_ids": [file.id]
#     }]
# )

# output as a saved csv file with file.id accessible
54/10: # get the answer
54/11:
file_id = client.beta.threads.messages.list(thread_id=thread.id).data[0].file_id
client.files.retrieve_content(file_id)
54/12:
file_id = client.beta.threads.messages.list(thread_id=thread.id).data[0].file_ids[0]
client.files.retrieve_content(file_id)
54/13:
file_id = client.beta.threads.messages.list(thread_id=thread.id).data[0].file_ids
client.files.retrieve_content(file_id)
54/14:
file_id = client.beta.threads.messages.list(thread_id=thread.id).data[0].file_ids[0]
client.files.content(file_id)
54/15:
messages = client.beta.threads.messages.list(thread_id=thread.id).data
if messages and messages[0].file_ids:
    file_id = messages[0].file_ids[0]
    client.files.retrieve_content(file_id)
else:
    print("No files associated with the first message in the thread.")
54/16:
messages = client.beta.threads.messages.list(thread_id=thread.id).data
if messages and messages[0].file_ids:
    file_id = messages[0].file_ids[0]
    client.files.content(file_id)
else:
    print("No files associated with the first message in the thread.")
54/17:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv", encoding='utf-8')

# save to json file for later use
with open("data/good_data/tags_gpt4.json", 'w', encoding='utf-8') as file:
    csv_file.to_json(file, force_ascii=False, orient='records')
54/18:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv", encoding='utf-8')

# save to json file for later use
with open("data/good_data/tags_gpt4.json", 'w', encoding='utf-8') as file:
    csv_file.to_json(file, force_ascii=False, orient='records')
54/19:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
54/20:
# # open data file and make into json with pandas
# csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# # save to json file for later use
# csv_file.to_json("data/good_data/tags_gpt4.json")
54/21:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv", encoding='utf-8')

# save to json file for later use
with open("data/good_data/tags_gpt4.json", 'w', encoding='utf-8') as file:
    csv_file.to_json(file, force_ascii=False, orient='records')
54/22:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
54/23:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
54/24:
thread = client.beta.threads.create(
)
54/25:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
54/26:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="Answer the questions in the message"

)
54/27:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/28:
messages = client.beta.threads.messages.list(thread_id=thread.id).data
if messages and messages[0].file_ids:
    file_id = messages[0].file_ids[0]
    client.files.content(file_id)
else:
    print("No files associated with the first message in the thread.")
54/29:
from openai._client import OpenAI
import streamlit as st
import pandas as pd
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
54/30:
# # open data file and make into json with pandas
# csv_file = pd.read_csv("data/good_data/tags_gpt4.csv")
# # save to json file for later use
# csv_file.to_json("data/good_data/tags_gpt4.json")
54/31:
# open data file and make into json with pandas
csv_file = pd.read_csv("data/good_data/tags_gpt4.csv", encoding='utf-8')

# save to json file for later use
with open("data/good_data/tags_gpt4.json", 'w', encoding='utf-8') as file:
    csv_file.to_json(file, force_ascii=False, orient='records')
54/32:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.json", "rb"),
  purpose='assistants'
)
54/33:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
54/34:
thread = client.beta.threads.create(
)
54/35:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
54/36:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
)
54/37:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/38:
messages = client.beta.threads.messages.list(thread_id=thread.id).data
if messages and messages[0].file_ids:
    file_id = messages[0].file_ids[0]
    client.files.content(file_id)
else:
    print("No files associated with the first message in the thread.")
54/39:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
54/40:
client.beta.threads.messages.create(
    assistant_id=assistant.id,
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
54/41:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
54/42:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
)
54/43:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/44:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

# Read the CSV file
df = pd.read_csv("data/good_data/tags_gpt4.csv", encoding='utf-8')

# Create a PdfPages object
pdf_pages = PdfPages('data/good_data/tags_gpt4.pdf')
54/45:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

# Read the CSV file
df = pd.read_csv("data/good_data/tags_gpt4.csv", encoding='utf-8')

# Create a PdfPages object
pdf_pages = PdfPages('data/good_data/tags_gpt4.pdf')
# Create a new figure and set the size of the figure
fig, ax =plt.subplots(figsize=(12,4))

# Remove the plot frame lines
ax.axis('tight')
ax.axis('off')

# Create a table and add it to the plot
the_table = ax.table(cellText=df.values,colLabels=df.columns,loc='center')

# Save the figure to the PdfPages object
pdf_pages.savefig(fig, bbox_inches='tight')

# Close the PdfPages object
pdf_pages.close()
54/46:
file = client.files.create(
  file=open("data/good_data/tags_gpt4.pdf", "rb"),
  purpose='assistants'
)
54/47:
assistant = client.beta.assistants.create(
    name="Content and tags creator",
    instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
    model="gpt-4-1106-preview",
    tools=[{"type": "retrieval"}],
    file_ids=[file.id]
)
54/48:
thread = client.beta.threads.create(
)
54/49:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
54/50:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
)
54/51:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/52:
messages = client.beta.threads.messages.list(thread_id=thread.id).data
if messages and messages[0].file_ids:
    file_id = messages[0].file_ids[0]
    client.files.content(file_id)
else:
    print("No files associated with the first message in the thread.")
54/53:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
)
54/54:
thread = client.beta.threads.create(
)
54/55:
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Can you write new categories for the products?",
    file_ids=[file.id]
)
54/56:
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="You are a content creator, and you wan to find out how to create tags for your products. The products are advanced maritime technology. Use your knowledge from file to answer the question.",
)
54/57:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/58:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/59:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/60:
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)
print(messages)

for message in messages.data:
    for content in message.content:
        print(content.text.value)

print(messages.data[0].content[0].text.value)
54/61:
messages = client.beta.threads.messages.list(thread_id=thread.id).data
if messages and messages[0].file_ids:
    file_id = messages[0].file_ids[0]
    client.files.content(file_id)
else:
    print("No files associated with the first message in the thread.")
52/2: print(df['tag_values'])
55/1:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import os
import json
55/2:
# open tags_gpt4.csv with pandas
tags = pd.read_csv('data/good_data/tags_gpt4.csv')
55/3:
# open tags_gpt4.csv with pandas
tags = pd.read_csv('data/good_data/tags_gpt4.csv')
# only use column Product_Name and url
tags = tags[['Product_Name','url']]
# drop duplicates
tags = tags.drop_duplicates()

print(tags.head())
55/4:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
55/5:
prompt = "Create new categories for the products in the tags pandas dataframe based on the URL structure. Note that one product can be in several categories."response = client.chat.completions.create(
    model="gpt-3.5-turbo-1106",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
        {"role": "user", "content": "{}".format(prompt)}
        ],
    max_tokens=200,
    response_format={ "type": "json_object" }
)
tags = response.choices[0].message.content.strip()
55/6:
# Make product and url in to dict from tags pandas detaframe
product_url_dict = tags.set_index('Product_Name')['url'].to_dict()
print(product_url_dict)
55/7:
prompt = "Create new categories for the products and url in the dictionary: {} based on the URL structure. Note that one product can be in several categories.".format(product_url_dict)
response = client.chat.completions.create(
    model="gpt-3.5-turbo-1106",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
        {"role": "user", "content": "{}".format(prompt)}
        ],
    max_tokens=200,
    response_format={ "type": "json_object" }
)
tags = response.choices[0].message.content.strip()
55/8: print(tags)
55/9:
prompt = "Create new categories for the products and url in the dictionary: {} based on the URL structure. Note that one product can be in several categories.".format(product_url_dict)
response = client.chat.completions.create(
    model="gpt-3.5-turbo-1106",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that makes new categories. Your responses should be in JSON format."},
        {"role": "user", "content": "{}".format(prompt)}
        ],
    max_tokens=200,
    response_format={ "type": "json_object" }
)
tags = response.choices[0].message.content.strip()
55/10: print(tags)
55/11:
prompt = "Create new categories for the products and url in the dictionary: {} based on the URL structure. Make 6 categores and put all the products into these categories. Note that one product can be in several categories.".format(product_url_dict)
response = client.chat.completions.create(
    model="gpt-3.5-turbo-1106",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that makes new categories. Your responses should be in JSON format."},
        {"role": "user", "content": "{}".format(prompt)}
        ],
    max_tokens=200,
    response_format={ "type": "json_object" }
)
tags = response.choices[0].message.content.strip()
55/12: print(tags)
55/13:
prompt = "Create new categories for the products and url in the dictionary: {} based on the URL structure. Make 6 categores. return these categories. Note that one product can be in several categories.".format(product_url_dict)
response = client.chat.completions.create(
    model="gpt-3.5-turbo-1106",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that makes new categories. Your responses should be in JSON format."},
        {"role": "user", "content": "{}".format(prompt)}
        ],
    max_tokens=200,
    response_format={ "type": "json_object" }
)
tags = response.choices[0].message.content.strip()
55/14: print(tags)
55/15:
prompt = "Create new categories for the products and url in the dictionary: {} based on the URL structure. Make 6 categores. return these categories. Note that one product can be in several categories.".format(product_url_dict)
response = client.chat.completions.create(
    model="gpt-3.5-turbo-1106",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that makes new categories. Your responses should be in JSON format."},
        {"role": "user", "content": "{}".format(prompt)}
        ],
    max_tokens=1500,
    response_format={ "type": "json_object" }
)
tags = response.choices[0].message.content.strip()
55/16: print(tags)
55/17:
prompt = "Create new categories for the products and url in the dictionary: {} based on the URL structure. return these categories. Note that one product can be in several categories.".format(product_url_dict)
response = client.chat.completions.create(
    model="gpt-3.5-turbo-1106",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that makes new categories. Your responses should be in JSON format."},
        {"role": "user", "content": "{}".format(prompt)}
        ],
    max_tokens=1500,
    response_format={ "type": "json_object" }
)
tags = response.choices[0].message.content.strip()
55/18: print(tags)
52/3:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
52/4:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
52/5:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
52/6:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
52/7:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
52/8:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
52/9:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    print(index)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
52/10:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    print(index, "/", len(df_tags)
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
52/11:
from bs4 import BeautifulSoup
import requests
# iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
df_tags = pd.read_csv("./data/df_tags.csv")
# only use name and url columnss
df_tags = df_tags[["Product_Name", "url", "Product category"]]  


def extract_bullet_points(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the link
    link = soup.find('a', href='#technicalInformation')

    bullet_points = []

    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]

    return bullet_points


url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_bullet_points(url)
    print(index, "/", len(df_tags))
    url_text_dict[url] = text

# save the dict as a json file
print(url_text_dict)
52/12:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
52/13:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.head(10).iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text
    print(index, "/", len(df_tags))


# save the dict as a json file
print(url_text_dict)
52/14:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
52/15:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
52/16:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
52/17:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
52/18:
# from bs4 import BeautifulSoup
# import requests
# # iterate over the rows of the dataframe, and find the text for each url and save it as a dict with the url as key and the text as value
# df_tags = pd.read_csv("./data/df_tags.csv")
# # only use name and url columnss
# df_tags = df_tags[["Product_Name", "url", "Product category"]]  


# def extract_bullet_points(url):
#     # Get the HTML of the page
#     response = requests.get(url)
#     html = response.text

#     # Parse the HTML with BeautifulSoup
#     soup = BeautifulSoup(html, 'html.parser')

#     # Find the link
#     link = soup.find('a', href='#technicalInformation')

#     bullet_points = []

#     # If the link was found, find the element it links to
#     if link is not None:
#         target_id = link['href'].lstrip('#')
#         target_element = soup.find(id=target_id)

#         # If the target element was found, get its text
#         if target_element is not None:
#             # Find all the list items in the target element
#             list_items = target_element.find_all('li')

#             # Extract the text of each list item
#             bullet_points = [li.get_text(strip=True) for li in list_items]

#     return bullet_points


# url_text_dict = {}
# for index, row in df_tags.iterrows():
#     url = row["url"]
#     text = extract_bullet_points(url)
#     print(index, "/", len(df_tags))
#     url_text_dict[url] = text

# # save the dict as a json file
# print(url_text_dict)
52/19:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text
    print(index, "/", len(df_tags))


# save the dict as a json file
print(url_text_dict)
52/20: print(url_text_dict)
52/21:
# save the dict as pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
    pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
52/22:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
52/23:
# def general_gpt(prompt: str):
#     # make chat gpt completion function with streamlit
#     openai.api_key = st.secrets["openai"]["api_key"]
#     openai.api_type = "azure"
#     openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
#     openai.api_version = "2023-03-15-preview"
#     # gpt_model = "gpt-35-turbo"
#     gpt_model = "gpt-35-turbo-16k"
#     # gpt_model = "gpt-4"
#     completion = openai.ChatCompletion.create(
#         deployment_id=gpt_model,
#         messages=[
#             {
#                 "role": "user",
#                 "content": "{}".format(prompt),
#             }
#         ],
#         temperature=0.3,
#         max_tokens=1500,
#         top_p=1.0,
#         frequency_penalty=0.1,
#         presence_penalty=0.1,
#     )
#     return str(completion.choices[0].message.content)
52/24:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                        ],
                    max_tokens=200,
                    response_format={ "type": "json_object" }
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                print(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                print(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/25:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                        ],
                    max_tokens=200,
                    response_format={ "type": "json_object" }
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                print(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                print(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/26:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")
        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
        while True:
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                        ],
                    max_tokens=200,
                    response_format={ "type": "json_object" }
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(type(e).__name__)
                print(str(e))
                match = re.search(r"retry after (\d+) seconds", str(e))
                if match:
                    wait_time = int(match.group(1))
                print(
                    "Rate limit exceeded. Waiting for {} seconds before retrying...".format(
                        wait_time
                    )
                )

                # show a bar that shows the progress in 30 second
                my_bar = st.progress(0)
                print(
                    " stopped at index {}, of total index of :{} ".format(
                        index, len(df_tags)
                    )
                )
                for percent_complete in range(wait_time):
                    time.sleep(1)
                    my_bar.progress((percent_complete + 1) / wait_time)
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/27:
import re
def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt
      
        response = client.chat.completions.create(
            model="gpt-3.5-turbo-1106",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                {"role": "user", "content": "{}".format(prompt)}
                ],
            max_tokens=200,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        print(tags)
        # Save the generated tags in the 'new_tags' column
        df_tags.loc[index, "new_tags"] = tags
                
    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/28:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/29:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/30:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
56/1: gpt4_tags = pd.read_csv('data/good_data/tags_gpt4.csv')
56/2:
import numpy as np
import pandas as pd
import streamlit as st
import requests
import json
import re
import os
56/3: gpt4_tags = pd.read_csv('data/good_data/tags_gpt4.csv')
56/4:
url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA440/"

# Get the HTML content of the URL
html_content = requests.get(url).text

# Check if the string exists in html_content
if '<a href="#relatedProducts">Related products</a>' in html_content:
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('a', {'href': '#relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_next('div')
        print(related_products)
56/5:
import numpy as np
import pandas as pd
import streamlit as st
import requests
import json
import re
import os
from bs4 import BeautifulSoup
56/6:
url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA440/"

# Get the HTML content of the URL
html_content = requests.get(url).text

# Check if the string exists in html_content
if '<a href="#relatedProducts">Related products</a>' in html_content:
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('a', {'href': '#relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_next('div')
        print(related_products)
52/31:
import ast

tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict_str = row.new_tags
    try:
        tags_dict = ast.literal_eval(tags_dict_str)
    except (SyntaxError, ValueError):
        print(f"Error parsing string to dict: {tags_dict_str}")
        continue

    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
print(len(tag_list))
52/32:
import ast

tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict_str = row.new_tags
    try:
        tags_dict = ast.literal_eval(tags_dict_str)
    except (SyntaxError, ValueError):
        print(f"Error parsing string to dict: {tags_dict_str}")
        continue

    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
print(len(tag_list))
52/33:

# tag_list = []
# for row in df_tags_tech.itertuples():
#     tags = row.new_tags
#     tags = tags.split(",")
#     tags = [tag.strip() for tag in tags]
#     tags = [tag.rstrip(".") for tag in tags]
#     # remove duplicates
#     # add all tags into one big list
#     for tag in tags:
#         tag_split = tag.split(" ")
#         # make tag_split to dict
#         print(tag.values())
#         tag_list.append(tag.values)
    

# # for i in tag_list:
# #     print(i)
# print((tag_list))
52/34:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

categories = [category.strip() for category in categories]
52/35:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
52/36:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict_temp = ast.literal_eval(tags_dict_str)
            for tags in tags_dict_temp.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
52/37: print(tags_dict)
52/38:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/39:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/40:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
52/41:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
52/42:
df_tech_new_tags.to_excel("./data/tags_gpt4.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_gpt4.csv", index=False)
52/43:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
52/44:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
52/45: df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")
52/46:
import re
ant = 0
for index, row in df_tech_new_tags.iterrows():
    print(index)
    if not pd.isna(row["tags"]):
        # ant_tags = row["tags"].split(",")
        ant_tags = re.split(',|:', row["tags"])
        print(ant_tags)
        if len(ant_tags) > 12:
        # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
            ant += 1
            print(index)
            print(row["tags"])
            print(len(ant_tags))
            print("found more than 10 tags")
            print("------------------")
            # print(row["tags_dict"])
            print("updating tags")
            # prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
            # tags_dict,  row["Product category"], row["Product_Name"]
            # )
            # tags = general_gpt(prompt)
            # print("New tags: ", tags)
            # df_tech_new_tags.loc[index, "tags"] = tags
            print("------------------")
            # update the tags column with only 8 tags


print("ant, ",ant)

# print(df_tech_new_tags["tags"])
52/47:
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
52/48:
# Read the dataframe from the CSV file
df_tags_tech = pd.read_csv("./data/tags_gpt4.csv")

# Group the dataframe by "Product category" and aggregate the "tags" column
grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# Create a dictionary to store the tags for each category
all_tags_by_category = {}

for category, tags_list in grouped_tags.items():
    # Flatten the list of tags
    tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
    # Remove duplicates
    tags = list(set(tags))
    # Add the tags to the dictionary
    all_tags_by_category[category] = ",".join(tags)

print(all_tags_by_category)
52/49:
import json
df_tags_tech = pd.read_csv("./data/tags_gpt4.csv")
# Create a set of all product tags
product_tags = set()
for category, group in df_tags_tech.groupby("Product category"):
    category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
    product_tags.update(category_tags)

non_matching_tags = []

# Iterate over each tag in dict_tags
for index, row in df_tags_tech.iterrows():
    tags_dict_str = row["tags_dict"]
    if pd.notna(tags_dict_str):
        tags_dict_str = tags_dict_str.replace("'", '"')
        print(tags_dict_str)  # Add this line
        try:
            tags_dict = json.loads(tags_dict_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            continue

        for tag in tags_dict.values():
            for t in tag:
                # Strip leading/trailing whitespace and quotes from t
                t = t.strip("' ").strip()
                if t not in product_tags:
                    non_matching_tags.append(t)

# Print non-matching tags
if non_matching_tags:
    print("The following tags do not have any corresponding tags in their category:")
    for tag in set(non_matching_tags):
        print(tag)
52/50:
# Load the data
df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# print len of tags that are empty or null
print(len(df[df['tags'].isnull()]))
#remove if tags is null
df = df[df['tags'].notnull()]
print(len(df[df['tags'].apply(lambda x: len(x) == 0)]))

# Print unique values in the 'tags' column
# print(df.columns)
# print(df['tags'].unique())
# # Print the 'tags' column
# print(df['tags'])

# Convert the 'tags' column to a dictionary
df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# Extract the values of the key 'tags' and split them into a list for each row
df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# Print the 'tag_values' column
# print(df['tag_values'])

# print all the empty list of tags
print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
56/7:
url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA440/"
from bs4 import BeautifulSoup
# Get the HTML content of the URL
html_content = requests.get(url).text
# Parse the HTML
soup = BeautifulSoup(html_content, 'html.parser')

# Find the related products section
related_products_section = soup.find('section', {'id': 'relatedProducts'})

# Get the related products
if related_products_section:
    related_products = related_products_section.find_all('h3', {'class': 'ProductCatalogItemCard__title'})

    # Print the related products
    for product in related_products:
        print(product.text)


# # Check if the string exists in html_content
# if '<a href="#relatedProducts">Related products</a>' in html_content:
#     # Parse the HTML
#     soup = BeautifulSoup(html_content, 'html.parser')

#     # Find the related products section
#     related_products_section = soup.find('a', {'href': '#relatedProducts'})

#     # Get the related products
#     if related_products_section:
#         related_products = related_products_section.find_next('div')
#         print(related_products)
56/8:
# url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA440/"
url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/ea-applications/"
from bs4 import BeautifulSoup
# Get the HTML content of the URL
html_content = requests.get(url).text
# Parse the HTML
soup = BeautifulSoup(html_content, 'html.parser')

# Find the related products section
related_products_section = soup.find('section', {'id': 'relatedProducts'})

# Get the related products
if related_products_section:
    related_products = related_products_section.find_all('h3', {'class': 'ProductCatalogItemCard__title'})

    # Print the related products
    for product in related_products:
        print(product.text)


# # Check if the string exists in html_content
# if '<a href="#relatedProducts">Related products</a>' in html_content:
#     # Parse the HTML
#     soup = BeautifulSoup(html_content, 'html.parser')

#     # Find the related products section
#     related_products_section = soup.find('a', {'href': '#relatedProducts'})

#     # Get the related products
#     if related_products_section:
#         related_products = related_products_section.find_next('div')
#         print(related_products)
56/9:
import numpy as np
import pandas as pd
import streamlit as st
import requests
import json
import re
import os
from bs4 import BeautifulSoup
56/10: gpt4_tags = pd.read_csv('data/good_data/tags_gpt4.csv')
56/11:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/12:
url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/ea-applications/"
from bs4 import BeautifulSoup

def find_related_products(url:str): 
    related_products = []
    # Get the HTML content of the URL
    html_content = requests.get(url).text
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('section', {'id': 'relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_all('h3', {'class': 'ProductCatalogItemCard__title'})

        # Print the related products
        for product in related_products:
            related_products.append(product.text)
            print(product.text)
    else :
        print("No related products found")
    return related_products
56/13:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/14:
# url = "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/ea-applications/"


def find_related_products(url:str): 
    related_products_list = []
    # Get the HTML content of the URL
    html_content = requests.get(url).text
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('section', {'id': 'relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_all('h3', {'class': 'ProductCatalogItemCard__title'})

        # Print the related products
        for product in related_products:
            related_products_list.append(product.text)
            print(product.text)
    else :
        print("No related products found")
    return related_products_list
56/15:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/16:
from bs4 import BeautifulSoup
import requests

def find_related_products(url:str): 
    related_products_list = []
    # Get the HTML content of the URL
    html_content = requests.get(url).text
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('section', {'id': 'relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_all('div', {'class': 'ProductCatalogItemCard ProductCatalogItemCard--productItem'})

        # Print the related products
        for product in related_products:
            product_title = product.find('h3', {'class': 'ProductCatalogItemCard__title'}).text
            product_link = product.find('a', {'class': 'ProductCatalogItemCard__link'})['href']
            related_products_list.append((product_title, product_link))
            print(product_title, product_link)
    else :
        print("No related products found")
    return related_products_list
56/17:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/18:
from bs4 import BeautifulSoup
import requests

def find_related_products(url:str): 
    base_url = "https://www.kongsberg.com"

    related_products_list = []
    # Get the HTML content of the URL
    html_content = requests.get(url).text
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('section', {'id': 'relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_all('div', {'class': 'ProductCatalogItemCard ProductCatalogItemCard--productItem'})

        # Print the related products
        for product in related_products:
            product_title = product.find('h3', {'class': 'ProductCatalogItemCard__title'}).text
            product_link = product.find('a', {'class': 'ProductCatalogItemCard__link'})['href']
            full_product_link = base_url + product_link
            related_products_list.append((product_title, full_product_link))
            print(product_title, product_link)
    else :
        print("No related products found")
    return related_products_list
56/19:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/20:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/21:
from bs4 import BeautifulSoup
import requests

def find_related_products(url:str): 
    base_url = "https://www.kongsberg.com"

    related_products_list = []
    # Get the HTML content of the URL
    html_content = requests.get(url).text
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('section', {'id': 'relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_all('div', {'class': 'ProductCatalogItemCard ProductCatalogItemCard--productItem'})

        # Print the related products
        for product in related_products:
            product_title = product.find('h3', {'class': 'ProductCatalogItemCard__title'}).text
            product_link = product.find('a', {'class': 'ProductCatalogItemCard__link'})['href']
            full_product_link = base_url + product_link
            related_products_list.append((product_title, full_product_link))
            print(product_title, full_product_link)
    else :
        print("No related products found")
    return related_products_list
56/22:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/23:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products, product_link = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/24:
from bs4 import BeautifulSoup
import requests

def find_related_products(url:str): 
    base_url = "https://www.kongsberg.com"

    related_products_list = []
    product_title_link = {}
    # Get the HTML content of the URL
    html_content = requests.get(url).text
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('section', {'id': 'relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_all('div', {'class': 'ProductCatalogItemCard ProductCatalogItemCard--productItem'})

        # Print the related products
        for product in related_products:
            product_title = product.find('h3', {'class': 'ProductCatalogItemCard__title'}).text
            product_link = product.find('a', {'class': 'ProductCatalogItemCard__link'})['href']
            full_product_link = base_url + product_link
            product_title_link[product_title] = full_product_link

            print(product_title_link)
    else :
        print("No related products found")
    return product_title_link
56/25:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products, product_link = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/26:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/27:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products = find_related_products(url)
    print(related_products)
    # gpt4_tags.at[index, 'related_products'] = related_products
56/28:
# gpt4_tags = pd.read_csv('data/tags_gpt4.csv')
gpt4_tags = pd.read_csv('data/df_tags_technology_15_11.csv')
56/29:
for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    product_title_link = find_related_products(url)
    print(product_title_link)
    gpt4_tags.at[index, 'related_products'] = product_title_link
56/30:
if 'related_products' not in gpt4_tags.columns:
    gpt4_tags['related_products'] = None

for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    product_title_link = find_related_products(url)
    print(product_title_link)
    gpt4_tags.at[index, 'related_products'] = product_title_link
56/31:
if 'related_products' not in gpt4_tags.columns:
    gpt4_tags['related_products'] = [{}]*len(gpt4_tags)

for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    product_title_link = find_related_products(url)
    print(product_title_link)
    gpt4_tags.at[index, 'related_products'] = product_title_link
56/32:
# Check if 'related_products' column exists, if not create it
if 'related_products' not in gpt4_tags.columns:
    gpt4_tags['related_products'] = [{} for _ in range(len(gpt4_tags))]

for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    product_title_link = find_related_products(url)
    print(product_title_link)
    gpt4_tags.at[index, 'related_products'] = product_title_link
56/33:
from bs4 import BeautifulSoup
import requests

def find_related_products(url:str): 
    base_url = "https://www.kongsberg.com"

    related_products_list = []
    related_products_url_list = []
    product_title_link = {}
    # Get the HTML content of the URL
    html_content = requests.get(url).text
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('section', {'id': 'relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_all('div', {'class': 'ProductCatalogItemCard ProductCatalogItemCard--productItem'})

        # Print the related products
        for product in related_products:
            product_title = product.find('h3', {'class': 'ProductCatalogItemCard__title'}).text
            product_link = product.find('a', {'class': 'ProductCatalogItemCard__link'})['href']
            full_product_link = base_url + product_link
            product_title_link[product_title] = full_product_link
            related_products_list.append(product_title)
            related_products_url_list.append(full_product_link)

            print(product_title_link)
    else :
        print("No related products found")
    return related_products_list, related_products_url_list
56/34:


for index, url in enumerate(gpt4_tags['url']):
    print(index, url)
    related_products_list, related_products_url_list = find_related_products(url)
    # print(product_title_link)
    gpt4_tags.at[index, 'related_products'] = related_products_list
    gpt4_tags.at[index, 'related_products_url'] = related_products_url_list
56/35: gpt4_tags["related_products"] = gpt4_tags["url"].apply(find_related_products)
56/36:
def wrapper_func_title(url):
    output1, output2 = find_related_products(url)
    return output1  # or return output2 if you need the second output
def wrapper_func_url(url):
    output1, output2 = find_related_products(url)
    return output2  # or return output2 if you need the second output

gpt4_tags["related_products"] = gpt4_tags["url"].apply(wrapper_func_title)
gpt4_tags["related_products_url"] = gpt4_tags["url"].apply(wrapper_func_url)
56/37: print(gpt4_tags.head())
56/38:
gpt4_tags = pd.read_csv('data/tags_gpt4.csv')
# gpt4_tags = pd.read_csv('data/df_tags_technology_15_11.csv')
56/39:
import numpy as np
import pandas as pd
import streamlit as st
import requests
import json
import re
import os
from bs4 import BeautifulSoup
56/40:
gpt4_tags = pd.read_csv('data/tags_gpt4.csv')
# gpt4_tags = pd.read_csv('data/df_tags_technology_15_11.csv')
56/41:
from bs4 import BeautifulSoup
import requests

def find_related_products(url:str): 
    base_url = "https://www.kongsberg.com"

    related_products_list = []
    related_products_url_list = []
    product_title_link = {}
    # Get the HTML content of the URL
    html_content = requests.get(url).text
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the related products section
    related_products_section = soup.find('section', {'id': 'relatedProducts'})

    # Get the related products
    if related_products_section:
        related_products = related_products_section.find_all('div', {'class': 'ProductCatalogItemCard ProductCatalogItemCard--productItem'})

        # Print the related products
        for product in related_products:
            product_title = product.find('h3', {'class': 'ProductCatalogItemCard__title'}).text
            product_link = product.find('a', {'class': 'ProductCatalogItemCard__link'})['href']
            full_product_link = base_url + product_link
            product_title_link[product_title] = full_product_link
            related_products_list.append(product_title)
            related_products_url_list.append(full_product_link)


    else :
        print("No related products found")
    return related_products_list, related_products_url_list
56/42:
def wrapper_func_title(url):
    output1, output2 = find_related_products(url)
    return output1  # or return output2 if you need the second output
def wrapper_func_url(url):
    output1, output2 = find_related_products(url)
    return output2  # or return output2 if you need the second output

gpt4_tags["related_products"] = gpt4_tags["url"].apply(wrapper_func_title)
gpt4_tags["related_products_url"] = gpt4_tags["url"].apply(wrapper_func_url)
56/43: print(gpt4_tags.head())
56/44:
# save to csv and xlsx
gpt4_tags.to_csv('data/tags_gpt4.csv', index=False)
gpt4_tags.to_excel('data/tags_gpt4.xlsx', index=False)
55/19: print(len(tags))
55/20:
mapping = {
  "Mapping Systems": [
    "K-Sync Synchronization unit",
    "Multibeam sonar, M3 Sonar",
    "Portable Hydrographic System, M3 Sonar",
    "Multibeam Sonar, Flexview",
    "HIGH RESOLUTION 1171 SONAR HEADS",
    "EM 2040 PHS Portable Hydrographic System",
    "GeoSwath Compact Survey Vessel",
    "EM 304 Multibeam echosounder, Max. 8000 m",
    "Portable Shallow Water Multibeam & Side Scan",
    "Shallow Water Multibeam & Side Scan",
    "EM 124 Multibeam echosounder, Max. 11000 m",
    "EM 712 Multibeam echosounder, Max. 3600 m",
    "GeoSwath 4 Multibeam echosounder, Shallow Water",
    "GeoSwath 4R Multibeam echosounder, Shallow water, Compact",
    "EM 2040 MKII Multibeam echosounder",
    "EM 2040C MKII Multibeam echosounder, Max. 500 m",
    "EM 2040P MKII Multibeam echosounder, Max. 550 m",
    "EM 2040C Compact Multibeam echosounder",
    "EM 2040 Multibeam echosounder",
    "EM 2000 Multibeam echosounder",
    "EM 2040P Portable multibeam echosounder",
    "EM 1002 Multibeam echosounder",
    "EM 120 Multibeam echosounder",
    "EM 122 Multibeam echosounder",
    "EM 300 Multibeam echosounder",
    "EM 3002 Multibeam echosounder",
    "EM 302 Multibeam echosounder",
    "EM 710 Multibeam echosounder",
    "EM 304 MKII Multibeam echosounder",
    "Side scan echo sounder, EA400",
    "EA440 - shallow and medium waters",
    "Single beam echo sounder, EA600 - To 11.000 m",
    "Echo sounder sweep system, MCU",
    "Single beam echosounder, EA400",
    "Survey single beam echo sounder, EA400",
    "Portable single beam echo sounder, EA400SP"
  ],
  "Acoustic Positioning Systems": [
    "cNODE Midi - Transponder",
    "cNODE - Transponder, Embedable",
    "cNODE MiniS - Transponder",
    "cNODE Maxi - Transponder",
    "cNODE Micro - Transponder",
    "cNODE IQAM - Intelligent data analysis and monitoring",
    "cNODE - Transponder, for explosive atmosphere",
    "TTC 30 & TTC 10 - Transponders test and configuration units",
    "cNODE Mini - Transponder",
    "cNODE Explorer - Transponder",
    "APOS Survey - Surveyor's independent Operator Station for HiPAP",
    "HAIN Reference for DP",
    "Acoustic control system for BOP operation",
    "ACS400 - Acoustic control system for BOP operation",
    "cPAP - ROV positioning in Long Base Line transponder array",
    "MPT 341 - Shorty series multifunction positioning transponder",
    "MPT Transponders",
    "MST Transponders",
    "RPT Transponders",
    "TTC 400 Transponder Test and Configuration unit",
    "MPT - HiPAP Transponders",
    "cPAP Portable tranceivers",
    "SPT Transponders",
    "SPT, HiPAP, Transponders"
  ],
  "Marine Robotics": [
    "Autonomous Underwater Vehicle, REMUS 600",
    "Autonomous Underwater Vehicle, Seaglider",
    "Autonomous Underwater Vehicle, REMUS 100",
    "Autonomous Underwater Vehicle, HUGIN",
    "Underwater Intervention Vehicle, Eelume",
    "Autonomous Underwater Vehicle, HUGIN Superior",
    "Autonomous Underwater Vehicle, REMUS 6000",
    "Autonomous Underwater Vehicle, HUGIN Endurance",
    "Autonomous Underwater Vehicle, HUGIN Edge",
    "Unmanned Surface Vehicle, GeoSwath 4R USV",
    "Uncrewed Surface Vehicle, Sounder",
    "EK Mission Planner",
    "Multibeam echosounder, ROV / AUV",
    "650M Mini Sonar Head",
    "EA applications",
    "SIS 5 - Multibeam echosounder software",
    "pcNODE - Modem MiniS",
    "cNODE - Modem Explorer",
    "cNODE - Modem Embedable",
    "cNode - Modem Mantis",
    "ROV Power / Telemetry 6-Port Interface Unit",
    "NavLab - Generic simulation and post-processing navigation software",
    "cPAP - ROV positioning in Long Base Line transponder array",
    "MPT 341 - Shorty series multifunction positioning transponder",
    "MPT Transponders",
    "MST Transponders",
    "RPT Transponders",
    "TTC 400 Transponder Test and Configuration unit",
    "MPT - HiPAP Transponders",
    "cPAP Portable tranceivers",
    "SPT Transponders",
    "SPT, HiPAP, Transponders",
    "cPAP Portable tranceivers"
  ],
  "Ocean Science": [
    "Sensor Fusion",
    "Data forwarder",
    "Sensor remote",
    "Analytics",
    "Core",
    "Vessel Motion Monitor",
    "Helideck Monitoring System",
    "Motion Reference Unit",
    "Aquaculture monitoring system",
    "Fish farm information transfer system",
    "Thermal Printer, GeoPrinter",
    "PI Portable hydrophone",
    "PI Purse seine hydrophone",
    "PI Trawl hydrophone",
    "Simrad EK15",
    "Simrad EK60",
    "Simrad EY60",
    "Simrad ES18"
  ]
}
55/21:
product_to_category = {product: category for category, products in mapping.items() for product in products}

# Create the new column
data_tags["new_categories"] = data_tags["product"].map(product_to_category)
55/22:
# open tags_gpt4.csv with pandas
data_tags = pd.read_csv('data/good_data/tags_gpt4.csv')
# only use column Product_Name and url
tags = data_tags[['Product_Name','url']]
# drop duplicates
tags = tags.drop_duplicates()
55/23:
product_to_category = {product: category for category, products in mapping.items() for product in products}

# Create the new column
data_tags["new_categories"] = data_tags["product"].map(product_to_category)
55/24:
product_to_category = {product: category for category, products in mapping.items() for product in products}

# Create the new column
data_tags["new_categories"] = data_tags["product_tags"].map(product_to_category)
55/25: print(data_tags["new_categories"].head(10))
55/26: print(data_tags["new_categories"].head(100))
55/27:
print(data_tags["new_categories"].head(100))
print(list(product_to_category.keys())[:5])
57/1:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
57/2:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/3:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")

    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/4:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/5:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
58/1:
import time
import pandas as pd
import numpy as np
import streamlit as st
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
58/2:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
58/3:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value. return boolean value True if it's a software or False if it's a hardware. "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags
df_tags = pd.read_csv("./data/tags_gpt4.csv")

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
# df_tags_tech.to_csv("./data/tags_gpt4.csv", index=False)
58/4:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value. return boolean value True if it's a software or False if it's a hardware. "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags
df_tags = pd.read_csv("./data/tags_gpt4.csv")

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
# df_tags_tech.to_csv("./data/tags_gpt4.csv", index=False)
58/5:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags
df_tags = pd.read_csv("./data/tags_gpt4.csv")

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
# df_tags_tech.to_csv("./data/tags_gpt4.csv", index=False)
58/6:
import time
import pandas as pd
import numpy as np
import streamlit as st
from openai._client import OpenAI
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
58/7:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                is_software_dict = json.loads(is_software)
                # Access the value of the 'is_software' key
                is_software_value = is_software_dict['is_software']
                print(is_software_value)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags
df_tags = pd.read_csv("./data/tags_gpt4.csv")

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
# df_tags_tech.to_csv("./data/tags_gpt4.csv", index=False)
58/8:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                is_software_dict = json.loads(is_software)
                # Access the value of the 'is_software' key
                is_software_value = is_software_dict['is_software']
                print(is_software_value)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software_value
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags
df_tags = pd.read_csv("./data/tags_gpt4.csv")

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
# df_tags_tech.to_csv("./data/tags_gpt4.csv", index=False)
58/9:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                is_software_dict = json.loads(is_software)
                # Access the value of the 'is_software' key
                is_software_value = is_software_dict['is_software']
                print(is_software_value)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software_value
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags
df_tags = pd.read_csv("./data/tags_gpt4.csv")

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_gpt4.csv", index=False)
57/6:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/7:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/8:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/9:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/10:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/11:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/12:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
57/13:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/14:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/15:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(
        df_url[["Product Name", "url"]], on="Product Name", how="inner"
    )

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/16:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/17:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/18:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/19:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    df_tags.columns = df_tags.iloc[0]
    df_url = df_url.drop_duplicates(subset=['Product Name'])

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/20:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
58/10:
df_tags = pd.read_csv("./data/tags_gpt4.csv")
print(len(df_tags))
df_tags = df_tags.drop_duplicates(subset=['Product_Name'])
print(len(df_tags))
57/21:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    df_tags.columns = df_tags.iloc[0]
    df_url = df_url.drop_duplicates(subset=['Product Name'])

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/22:
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/23:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    print(df_tags.iloc[0])
    df_tags.columns = df_tags.iloc[0]
    df_url = df_url.drop_duplicates(subset=['Product Name'])

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
57/24:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    print(df_tags.iloc[0])
    df_tags.columns = df_tags.iloc[0]
    df_url = df_url.drop_duplicates(subset=['Product Name'])

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/25:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    print(df_tags.iloc[0])
    df_tags.columns = df_tags.iloc[1]
    df_url = df_url.drop_duplicates(subset=['Product Name'])

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/26:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags (1).xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    print(df_tags.iloc[0])
    df_tags.columns = df_tags.iloc[0]
    df_url = df_url.drop_duplicates(subset=['Product Name'])

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/27:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    print(df_tags.iloc[0])
    df_tags.columns = df_tags.iloc[0]
    df_url = df_url.drop_duplicates(subset=['Product Name'])

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/28:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
57/29:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
57/30:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))
    df_url = df_url.dropna(axis=0, how="all")
    df_tags = df_tags.dropna(axis=0, how="all")
    print(len(df_url))
    print(len(df_tags))
    # set row to as header
    print(df_tags.iloc[0])
    df_tags.columns = df_tags.iloc[0]
    df_url = df_url.drop_duplicates(subset=['Product Name'])

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/31:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
57/32:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
for index, row in df_tags.iterrows():
    url = row["url"]
    text = extract_text(url)
    url_text_dict[url] = text
    print(index, "/", len(df_tags))


# save the dict as a json file
print(url_text_dict)
57/33:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
57/34: print(url_text_dict)
57/35:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
57/36:
# def general_gpt(prompt: str):
#     # make chat gpt completion function with streamlit
#     openai.api_key = st.secrets["openai"]["api_key"]
#     openai.api_type = "azure"
#     openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
#     openai.api_version = "2023-03-15-preview"
#     # gpt_model = "gpt-35-turbo"
#     gpt_model = "gpt-35-turbo-16k"
#     # gpt_model = "gpt-4"
#     completion = openai.ChatCompletion.create(
#         deployment_id=gpt_model,
#         messages=[
#             {
#                 "role": "user",
#                 "content": "{}".format(prompt),
#             }
#         ],
#         temperature=0.3,
#         max_tokens=1500,
#         top_p=1.0,
#         frequency_penalty=0.1,
#         presence_penalty=0.1,
#     )
#     return str(completion.choices[0].message.content)
57/37:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 5. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
58/11:
df_tags = pd.read_csv("./data/tags_gpt4.csv")
print(len(df_tags))
df_tags = df_tags.drop_duplicates(subset=['url'])
print(len(df_tags))
58/12:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
58/13:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                is_software_dict = json.loads(is_software)
                # Access the value of the 'is_software' key
                is_software_value = is_software_dict['is_software']
                print(is_software_value)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software_value
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags
df_tags = pd.read_csv("./data/tags_gpt4.csv")

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_gpt4_issoftware.csv", index=False)
58/14:
import time
import pandas as pd
import numpy as np
import streamlit as st
from openai._client import OpenAI
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
58/15:
df_tags = pd.read_csv("./data/tags_gpt4.csv")
print(len(df_tags))
df_tags = df_tags.drop_duplicates(subset=['url'])
print(len(df_tags))
58/16:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
58/17:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                is_software_dict = json.loads(is_software)
                # Access the value of the 'is_software' key
                is_software_value = is_software_dict['is_software']
                print(is_software_value)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software_value
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_gpt4_issoftware.csv", index=False)
57/38:
import ast

tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict_str = row.new_tags
    try:
        tags_dict = ast.literal_eval(tags_dict_str)
    except (SyntaxError, ValueError):
        print(f"Error parsing string to dict: {tags_dict_str}")
        continue

    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
print(len(tag_list))
57/39:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

categories = [category.strip() for category in categories]
57/40:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
57/41:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict_temp = ast.literal_eval(tags_dict_str)
            for tags in tags_dict_temp.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
57/42: print(tags_dict)
57/43:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
57/44:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
57/45:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
57/46:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/47: df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/48:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/49:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/50:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):

    if pd.isna(url) or 'http' not in url:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/51:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):

    if pd.isna(url) or 'http' not in url:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/52:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    if url == "nan":
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    if pd.isna(url) or 'http' not in url:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/53:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    print(url)
    if url == "nan":
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    if pd.isna(url) or 'http' not in url:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/54:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    print(url)
    if url == "nan":
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    if pd.isna(url) or 'http' not in url:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/55:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    print(url)
    if url == "www.kongsberg.comn":
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    if pd.isna(url) or 'http' not in url:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/56:
import requests
from bs4 import BeautifulSoup

def extract_image_url(url):
    print(url)
    if url == "https://www.kongsberg.comn":
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    if pd.isna(url) or 'http' not in url:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
    # Get the HTML of the page
    response = requests.get(url)

    # If the response status code is 404, return the dummy image URL
    if response.status_code == 404:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the image
    img = soup.find('img')

    # If the image was found, return its source URL
    if img is not None:
        img_url = "https://www.kongsberg.com"+img['src']
        return img_url
    else:
        return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
57/57:
df_tech_new_tags.to_excel("./data/df_tags_use_app_22_11.xlsx", index=False)
df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
58/18: print(len(df_tags_tech))
58/19: df_tags_tech.to_excel("./data/tags_gpt4_22_11.csv", index=False)
58/20: df_tags_tech.to_excel("./data/tags_gpt4_22_11.xlsx", index=False)
58/21:
import time
import pandas as pd
import numpy as np
import streamlit as st
from openai._client import OpenAI
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
58/22:
df_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
print(len(df_tags))
df_tags = df_tags.drop_duplicates(subset=['url'])
print(len(df_tags))
58/23:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
58/24:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                is_software_dict = json.loads(is_software)
                # Access the value of the 'is_software' key
                is_software_value = is_software_dict['is_software']
                print(is_software_value)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software_value
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/25: df_tags_tech.to_excel("./data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/26:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                is_software_dict = json.loads(is_software)
                # Access the value of the 'is_software' key
                is_software_value = is_software_dict['is_software']
                print(is_software_value)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software_value
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
58/27: df_tags_tech.to_excel("./data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
58/28:
df_tags = pd.read_csv("./data/df_tags_use_app_22_11_issoftware.csv")
df_tags_tech.to_excel("./data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
58/29:
import time
import pandas as pd
import numpy as np
import streamlit as st
from openai._client import OpenAI
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
58/30:
df_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
print(len(df_tags))
df_tags = df_tags.drop_duplicates(subset=['url'])
print(len(df_tags))
58/31:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
58/32:


def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["is_software"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the website text: '{website_text}', and the product name: '{product_name}', is the product a software or a hardware? If it is a combination, then it is hardware. If its only software, then return software.  Return true if it's a software or false if it's a hardware."

        # Call the general_gpt function with this prompt
        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that determines whether a product is software or hardware. Your responses should be in JSON format and consist of a boolean value on the format: {'is_software': true/false} "}, 
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=50,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                is_software = response.choices[0].message.content.strip()
                print(product_name)
                print(is_software)
                is_software_dict = json.loads(is_software)
                # Access the value of the 'is_software' key
                is_software_value = is_software_dict['is_software']
                print(is_software_value)
                print("_________________________________")
                # Save the determination in the 'is_software' column
                df_tags.loc[index, "is_software"] = is_software_value
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
            else:
                print(f"Skipping index {index} after 3 failed attempts")
                continue  # Skip the current index if the API call fails 3 times

    return df_tags

#read csv to df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/33:

df_tags_tech.to_excel("./data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
58/34:
df = pd.read_csv("data/tags_gpt4_issoftware.csv")
df.to_excel("data/tags_gpt4_issoftware.xlsx")
58/35:
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")
df.to_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
58/36:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/37:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/38:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/39:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/40:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/41:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/42:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
57/58:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))

    # set row to as header
    print(df_tags.iloc[0])
    df_tags.columns = df_tags.iloc[0]

    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))
    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
57/59:

# load the dataframes
def load_dataframes():
    csv_url_path = "./data/Products and Categories.xlsx"
    tags_url_path = "./data/dummy - Product category and tags.xlsx"
    df_url = pd.read_excel(csv_url_path)
    df_tags = pd.read_excel(tags_url_path)
    return df_url, df_tags


def process_dataframes(df_url, df_tags):
    print(len(df_url))
    print(len(df_tags))

    # set row to as header
    print(df_tags.iloc[0])
    df_tags.columns = df_tags.iloc[0]
    df_tags.to_excel("test_before_merge.xlsx", index=False)
    # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
    df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")
    df_tags.to_excel("test_after_merge.xlsx", index=False)
    # drop columns nan, Technology, General, Related products, Varient products
    df_tags = df_tags.drop(
        columns=[
            "Technology",
            "General",
            "Related products",
            "Variant products",
            "Contains / made from products",
        ],
        axis=1,
    )
    print(len(df_tags))


    # add the string "https://www.kongsberg.com/" to the url column for each url
    df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
    # rename the column Product Name to Product_Name
    df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

    return df_tags
df_url, df_tags = load_dataframes()
df_tags = process_dataframes(df_url, df_tags)
58/43:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
58/44:
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())
58/45:
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    print(index)
    print(row["Product category"])
    if "," in row["Product category"]:
        print("comma found")

58/46:
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    print(index)
    print(row["Product category"])
    if "," in row["Product category"]:
        print("comma found")
58/47:
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    print(index)
    if "," in row["Product category"]:
        print("comma found")
58/48:
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if "," in row["Product category"]:
        print(index)
        print("comma found")
59/1:
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if "," in row["Product category"]:
        category_list = row["Product category"].split(",")
        print(category_list)
59/2:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if "," in row["Product category"]:
        category_list = row["Product category"].split(",")
        print(category_list)
59/3:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = row["Product category"].split(",")
            print(category_list)
59/4:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
# df["Product category"].str.contains(",").sum()
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if "," in row["Product category"]:
        print(row["Product category"])
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = row["Product category"].split(",")
            print(category_list)
59/5:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        print(row["Product category"])
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = row["Product category"].split(",")
            print(category_list)
59/6:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = row["Product category"].split(",")
            print(category_list)
59/7:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = row["Product category"].split(",")
            print(category_list)
            print(row["Product_Name"])
59/8:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = row["Product category"].split(",").strip()
            print(category_list)
            print(row["Product_Name"])
59/9:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            print(category_list)
            print(row["Product_Name"])
57/60: print(tags_dict)
59/10:
import json

tags_dict = {
    {'Naval': '{\n    "Product Type": ["marine coating", "acoustic control system", "leak detection"],\n    "Technology": ["anti-fouling", "Hydroacoustic Positioning", "acoustic telemetry", "Inertial Navigation System"],\n    "Application": ["hull protection", "BOP operation", "early warning", "infrastructure safety"]\n}', 'Fish finding': '{\n    "Product Type": [\n        "portable sonar",\n        "echo sounder",\n        "sonar system",\n        "navigation software"\n    ],\n    "Technology": [\n        "chirp technology",\n        "split-beam technology",\n        "wideband chirp",\n        "multi-frequency echo sounder"\n    ],\n    "Application": [\n        "fish finding",\n        "fish stock assessment",\n        "maritime navigation",\n        "catch monitoring"\n    ]\n}', 'Surveillance & monitoring': '{\n    "Product Type": ["sonar system", "surveillance system"],\n    "Technology": ["subsea technology", "underwater surveillance"],\n    "Application": ["security", "naval defense"]\n}', 'Underwater navigation & positioning': '{\n    "Product Type": [\n        "Underwater Altimeters",\n        "Transponder",\n        "Acoustic Positioning System",\n        "Modem"\n    ],\n    "Technology": [\n        "HiPAP",\n        "Hydroacoustic",\n        "GNSS technology",\n        "Dynamic Positioning"\n    ],\n    "Application": [\n        "ROV navigation",\n        "Deep water operation",\n        "Subsea navigation",\n        "Position tracking"\n    ]\n}', 'Autonomous and uncrewed solutions': '{\n    "Product Type": [\n        "transponder",\n        "Autonomous Echo Sounders Programming",\n        "marine survey equipment",\n        "Software"\n    ],\n    "Technology": [\n        "acoustic communication",\n        "3D sonar sensing",\n        "sensor fusion",\n        "machine learning"\n    ],\n    "Application": [\n        "underwater_navigation",\n        "marine research",\n        "Data Management",\n        "environmental monitoring"\n    ]\n}', 'Product category': '{\n    "Product Type": ["Software", "Electronics", "Apparel"],\n    "Technology": ["AI", "Blockchain", "IoT"],\n    "Application": ["Data Analysis", "Payment Processing", "Wearable Tracking"]\n}', 'Geophysical survey': '{\n    "Product Type": [\n        "acoustic doppler profiler",\n        "sub-bottom profiler",\n        "marine survey equipment",\n        "oceanography equipment"\n    ],\n    "Technology": [\n        "narrow beam",\n        "multibeam echo sounder integration",\n        "parametric sub-bottom profiler",\n        "low frequency transducers"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "sub-bottom imaging",\n        "sediment layer mapping",\n        "seabed penetration"\n    ]\n}', 'Surface navigation & positioning': '{\n    "Product Type": [\n        "scientific transceiver",\n        "echo sounder",\n        "marine transceiver",\n        "navigation echo sounder"\n    ],\n    "Technology": [\n        "underwater acoustics",\n        "wideband transceiver",\n        "multiplexing",\n        "dual power input"\n    ],\n    "Application": [\n        "fisheries science",\n        "ocean research",\n        "marine navigation",\n        "underwater vehicle navigation"\n    ]\n}', 'Seafloor mapping': '{\n    "Product Type": [\n        "portable hydrographic system",\n        "multibeam echosounder",\n        "side scan sonar",\n        "echo sounder"\n    ],\n    "Technology": [\n        "multibeam sonar",\n        "sidescan sonar",\n        "sonar_signal_processing",\n        "multifrequency"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "ocean floor mapping",\n        "marine survey",\n        "bathymetric mapping"\n    ]\n}', 'Fishery research': '{\n    "Product Type": [\n        "Aquaculture monitoring system",\n        "Information transfer system",\n        "Analyser",\n        "Sensor"\n    ],\n    "Technology": [\n        "Environmental sensing",\n        "Maritime broadband radio",\n        "Submersible technology",\n        "Sensor fusion"\n    ],\n    "Application": [\n        "Biomass measurement",\n        "Fish farming efficiency",\n        "Seawater analysis",\n        "Environmental monitoring"\n    ]\n}', 'Maritime communications': '{\n    "Product Type": ["thermal_printer", "newsletter", "AIS transponder", "sonar software", "AIS base station", "integrated bridge system", "marine communication", "software update", "satellite communication", "AIS system", "electronic chart display", "communication equipment", "AIS Base Station management"],\n    "Technology": ["photographic_quality_output", "3D profiling", "transceiver", "maritime electronics", "IMO compliant", "data interpretation", "electromagnetic", "large_format_thermal_printing", "multi-head sonar support", "advanced_greyscale_capabilities", "high_resolution_imaging", "sdr_technology", "high-speed data transfer"],\n    "Application": ["vessel tracking", "collision avoidance", "maritime safety", "underwater survey", "vessel safety", "navigation aid tracking", "coastal surveillance", "maritime traffic monitoring", "remote operational coordination", "vessel data management", "space-based_ais", "offshore connectivity", "portable"]\n}'}
}

# Convert string values to dictionaries and remove newline characters and extra spaces
for key in tags_dict:
    tags_dict[key] = json.loads(tags_dict[key].replace('\n', '').replace('    ', ''))

print(tags_dict)
59/11:
import json

tags_dict = {
    {'Naval': '{\n    "Product Type": ["marine coating", "acoustic control system", "leak detection"],\n    "Technology": ["anti-fouling", "Hydroacoustic Positioning", "acoustic telemetry", "Inertial Navigation System"],\n    "Application": ["hull protection", "BOP operation", "early warning", "infrastructure safety"]\n}', 'Fish finding': '{\n    "Product Type": [\n        "portable sonar",\n        "echo sounder",\n        "sonar system",\n        "navigation software"\n    ],\n    "Technology": [\n        "chirp technology",\n        "split-beam technology",\n        "wideband chirp",\n        "multi-frequency echo sounder"\n    ],\n    "Application": [\n        "fish finding",\n        "fish stock assessment",\n        "maritime navigation",\n        "catch monitoring"\n    ]\n}', 'Surveillance & monitoring': '{\n    "Product Type": ["sonar system", "surveillance system"],\n    "Technology": ["subsea technology", "underwater surveillance"],\n    "Application": ["security", "naval defense"]\n}', 'Underwater navigation & positioning': '{\n    "Product Type": [\n        "Underwater Altimeters",\n        "Transponder",\n        "Acoustic Positioning System",\n        "Modem"\n    ],\n    "Technology": [\n        "HiPAP",\n        "Hydroacoustic",\n        "GNSS technology",\n        "Dynamic Positioning"\n    ],\n    "Application": [\n        "ROV navigation",\n        "Deep water operation",\n        "Subsea navigation",\n        "Position tracking"\n    ]\n}', 'Autonomous and uncrewed solutions': '{\n    "Product Type": [\n        "transponder",\n        "Autonomous Echo Sounders Programming",\n        "marine survey equipment",\n        "Software"\n    ],\n    "Technology": [\n        "acoustic communication",\n        "3D sonar sensing",\n        "sensor fusion",\n        "machine learning"\n    ],\n    "Application": [\n        "underwater_navigation",\n        "marine research",\n        "Data Management",\n        "environmental monitoring"\n    ]\n}', 'Product category': '{\n    "Product Type": ["Software", "Electronics", "Apparel"],\n    "Technology": ["AI", "Blockchain", "IoT"],\n    "Application": ["Data Analysis", "Payment Processing", "Wearable Tracking"]\n}', 'Geophysical survey': '{\n    "Product Type": [\n        "acoustic doppler profiler",\n        "sub-bottom profiler",\n        "marine survey equipment",\n        "oceanography equipment"\n    ],\n    "Technology": [\n        "narrow beam",\n        "multibeam echo sounder integration",\n        "parametric sub-bottom profiler",\n        "low frequency transducers"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "sub-bottom imaging",\n        "sediment layer mapping",\n        "seabed penetration"\n    ]\n}', 'Surface navigation & positioning': '{\n    "Product Type": [\n        "scientific transceiver",\n        "echo sounder",\n        "marine transceiver",\n        "navigation echo sounder"\n    ],\n    "Technology": [\n        "underwater acoustics",\n        "wideband transceiver",\n        "multiplexing",\n        "dual power input"\n    ],\n    "Application": [\n        "fisheries science",\n        "ocean research",\n        "marine navigation",\n        "underwater vehicle navigation"\n    ]\n}', 'Seafloor mapping': '{\n    "Product Type": [\n        "portable hydrographic system",\n        "multibeam echosounder",\n        "side scan sonar",\n        "echo sounder"\n    ],\n    "Technology": [\n        "multibeam sonar",\n        "sidescan sonar",\n        "sonar_signal_processing",\n        "multifrequency"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "ocean floor mapping",\n        "marine survey",\n        "bathymetric mapping"\n    ]\n}', 'Fishery research': '{\n    "Product Type": [\n        "Aquaculture monitoring system",\n        "Information transfer system",\n        "Analyser",\n        "Sensor"\n    ],\n    "Technology": [\n        "Environmental sensing",\n        "Maritime broadband radio",\n        "Submersible technology",\n        "Sensor fusion"\n    ],\n    "Application": [\n        "Biomass measurement",\n        "Fish farming efficiency",\n        "Seawater analysis",\n        "Environmental monitoring"\n    ]\n}', 'Maritime communications': '{\n    "Product Type": ["thermal_printer", "newsletter", "AIS transponder", "sonar software", "AIS base station", "integrated bridge system", "marine communication", "software update", "satellite communication", "AIS system", "electronic chart display", "communication equipment", "AIS Base Station management"],\n    "Technology": ["photographic_quality_output", "3D profiling", "transceiver", "maritime electronics", "IMO compliant", "data interpretation", "electromagnetic", "large_format_thermal_printing", "multi-head sonar support", "advanced_greyscale_capabilities", "high_resolution_imaging", "sdr_technology", "high-speed data transfer"],\n    "Application": ["vessel tracking", "collision avoidance", "maritime safety", "underwater survey", "vessel safety", "navigation aid tracking", "coastal surveillance", "maritime traffic monitoring", "remote operational coordination", "vessel data management", "space-based_ais", "offshore connectivity", "portable"]\n}'}
}

# Convert string values to dictionaries and remove newline characters and extra spaces
for key in tags_dict:
    tags_dict[key] = json.loads(tags_dict[key].replace('\n', '').replace('    ', ''))

print(tags_dict)
59/12:
import json

tags_dict = {
    {'Naval': '{\n    "Product Type": ["marine coating", "acoustic control system", "leak detection"],\n    "Technology": ["anti-fouling", "Hydroacoustic Positioning", "acoustic telemetry", "Inertial Navigation System"],\n    "Application": ["hull protection", "BOP operation", "early warning", "infrastructure safety"]\n}', 'Fish finding': '{\n    "Product Type": [\n        "portable sonar",\n        "echo sounder",\n        "sonar system",\n        "navigation software"\n    ],\n    "Technology": [\n        "chirp technology",\n        "split-beam technology",\n        "wideband chirp",\n        "multi-frequency echo sounder"\n    ],\n    "Application": [\n        "fish finding",\n        "fish stock assessment",\n        "maritime navigation",\n        "catch monitoring"\n    ]\n}', 'Surveillance & monitoring': '{\n    "Product Type": ["sonar system", "surveillance system"],\n    "Technology": ["subsea technology", "underwater surveillance"],\n    "Application": ["security", "naval defense"]\n}', 'Underwater navigation & positioning': '{\n    "Product Type": [\n        "Underwater Altimeters",\n        "Transponder",\n        "Acoustic Positioning System",\n        "Modem"\n    ],\n    "Technology": [\n        "HiPAP",\n        "Hydroacoustic",\n        "GNSS technology",\n        "Dynamic Positioning"\n    ],\n    "Application": [\n        "ROV navigation",\n        "Deep water operation",\n        "Subsea navigation",\n        "Position tracking"\n    ]\n}', 'Autonomous and uncrewed solutions': '{\n    "Product Type": [\n        "transponder",\n        "Autonomous Echo Sounders Programming",\n        "marine survey equipment",\n        "Software"\n    ],\n    "Technology": [\n        "acoustic communication",\n        "3D sonar sensing",\n        "sensor fusion",\n        "machine learning"\n    ],\n    "Application": [\n        "underwater_navigation",\n        "marine research",\n        "Data Management",\n        "environmental monitoring"\n    ]\n}', 'Product category': '{\n    "Product Type": ["Software", "Electronics", "Apparel"],\n    "Technology": ["AI", "Blockchain", "IoT"],\n    "Application": ["Data Analysis", "Payment Processing", "Wearable Tracking"]\n}', 'Geophysical survey': '{\n    "Product Type": [\n        "acoustic doppler profiler",\n        "sub-bottom profiler",\n        "marine survey equipment",\n        "oceanography equipment"\n    ],\n    "Technology": [\n        "narrow beam",\n        "multibeam echo sounder integration",\n        "parametric sub-bottom profiler",\n        "low frequency transducers"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "sub-bottom imaging",\n        "sediment layer mapping",\n        "seabed penetration"\n    ]\n}', 'Surface navigation & positioning': '{\n    "Product Type": [\n        "scientific transceiver",\n        "echo sounder",\n        "marine transceiver",\n        "navigation echo sounder"\n    ],\n    "Technology": [\n        "underwater acoustics",\n        "wideband transceiver",\n        "multiplexing",\n        "dual power input"\n    ],\n    "Application": [\n        "fisheries science",\n        "ocean research",\n        "marine navigation",\n        "underwater vehicle navigation"\n    ]\n}', 'Seafloor mapping': '{\n    "Product Type": [\n        "portable hydrographic system",\n        "multibeam echosounder",\n        "side scan sonar",\n        "echo sounder"\n    ],\n    "Technology": [\n        "multibeam sonar",\n        "sidescan sonar",\n        "sonar_signal_processing",\n        "multifrequency"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "ocean floor mapping",\n        "marine survey",\n        "bathymetric mapping"\n    ]\n}', 'Fishery research': '{\n    "Product Type": [\n        "Aquaculture monitoring system",\n        "Information transfer system",\n        "Analyser",\n        "Sensor"\n    ],\n    "Technology": [\n        "Environmental sensing",\n        "Maritime broadband radio",\n        "Submersible technology",\n        "Sensor fusion"\n    ],\n    "Application": [\n        "Biomass measurement",\n        "Fish farming efficiency",\n        "Seawater analysis",\n        "Environmental monitoring"\n    ]\n}', 'Maritime communications': '{\n    "Product Type": ["thermal_printer", "newsletter", "AIS transponder", "sonar software", "AIS base station", "integrated bridge system", "marine communication", "software update", "satellite communication", "AIS system", "electronic chart display", "communication equipment", "AIS Base Station management"],\n    "Technology": ["photographic_quality_output", "3D profiling", "transceiver", "maritime electronics", "IMO compliant", "data interpretation", "electromagnetic", "large_format_thermal_printing", "multi-head sonar support", "advanced_greyscale_capabilities", "high_resolution_imaging", "sdr_technology", "high-speed data transfer"],\n    "Application": ["vessel tracking", "collision avoidance", "maritime safety", "underwater survey", "vessel safety", "navigation aid tracking", "coastal surveillance", "maritime traffic monitoring", "remote operational coordination", "vessel data management", "space-based_ais", "offshore connectivity", "portable"]\n}'}
}

# Convert string values to dictionaries and remove newline characters and extra spaces
for key in tags_dict:
    tags_dict[key] = json.loads(tags_dict[key].replace('\n', '').replace('    ', ''))

print(tags_dict)
59/13:
import json

tags_dict = {
    {'Naval': '{\n    "Product Type": ["marine coating", "acoustic control system", "leak detection"],\n    "Technology": ["anti-fouling", "Hydroacoustic Positioning", "acoustic telemetry", "Inertial Navigation System"],\n    "Application": ["hull protection", "BOP operation", "early warning", "infrastructure safety"]\n}', 'Fish finding': '{\n    "Product Type": [\n        "portable sonar",\n        "echo sounder",\n        "sonar system",\n        "navigation software"\n    ],\n    "Technology": [\n        "chirp technology",\n        "split-beam technology",\n        "wideband chirp",\n        "multi-frequency echo sounder"\n    ],\n    "Application": [\n        "fish finding",\n        "fish stock assessment",\n        "maritime navigation",\n        "catch monitoring"\n    ]\n}', 'Surveillance & monitoring': '{\n    "Product Type": ["sonar system", "surveillance system"],\n    "Technology": ["subsea technology", "underwater surveillance"],\n    "Application": ["security", "naval defense"]\n}', 'Underwater navigation & positioning': '{\n    "Product Type": [\n        "Underwater Altimeters",\n        "Transponder",\n        "Acoustic Positioning System",\n        "Modem"\n    ],\n    "Technology": [\n        "HiPAP",\n        "Hydroacoustic",\n        "GNSS technology",\n        "Dynamic Positioning"\n    ],\n    "Application": [\n        "ROV navigation",\n        "Deep water operation",\n        "Subsea navigation",\n        "Position tracking"\n    ]\n}', 'Autonomous and uncrewed solutions': '{\n    "Product Type": [\n        "transponder",\n        "Autonomous Echo Sounders Programming",\n        "marine survey equipment",\n        "Software"\n    ],\n    "Technology": [\n        "acoustic communication",\n        "3D sonar sensing",\n        "sensor fusion",\n        "machine learning"\n    ],\n    "Application": [\n        "underwater_navigation",\n        "marine research",\n        "Data Management",\n        "environmental monitoring"\n    ]\n}', 'Product category': '{\n    "Product Type": ["Software", "Electronics", "Apparel"],\n    "Technology": ["AI", "Blockchain", "IoT"],\n    "Application": ["Data Analysis", "Payment Processing", "Wearable Tracking"]\n}', 'Geophysical survey': '{\n    "Product Type": [\n        "acoustic doppler profiler",\n        "sub-bottom profiler",\n        "marine survey equipment",\n        "oceanography equipment"\n    ],\n    "Technology": [\n        "narrow beam",\n        "multibeam echo sounder integration",\n        "parametric sub-bottom profiler",\n        "low frequency transducers"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "sub-bottom imaging",\n        "sediment layer mapping",\n        "seabed penetration"\n    ]\n}', 'Surface navigation & positioning': '{\n    "Product Type": [\n        "scientific transceiver",\n        "echo sounder",\n        "marine transceiver",\n        "navigation echo sounder"\n    ],\n    "Technology": [\n        "underwater acoustics",\n        "wideband transceiver",\n        "multiplexing",\n        "dual power input"\n    ],\n    "Application": [\n        "fisheries science",\n        "ocean research",\n        "marine navigation",\n        "underwater vehicle navigation"\n    ]\n}', 'Seafloor mapping': '{\n    "Product Type": [\n        "portable hydrographic system",\n        "multibeam echosounder",\n        "side scan sonar",\n        "echo sounder"\n    ],\n    "Technology": [\n        "multibeam sonar",\n        "sidescan sonar",\n        "sonar_signal_processing",\n        "multifrequency"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "ocean floor mapping",\n        "marine survey",\n        "bathymetric mapping"\n    ]\n}', 'Fishery research': '{\n    "Product Type": [\n        "Aquaculture monitoring system",\n        "Information transfer system",\n        "Analyser",\n        "Sensor"\n    ],\n    "Technology": [\n        "Environmental sensing",\n        "Maritime broadband radio",\n        "Submersible technology",\n        "Sensor fusion"\n    ],\n    "Application": [\n        "Biomass measurement",\n        "Fish farming efficiency",\n        "Seawater analysis",\n        "Environmental monitoring"\n    ]\n}', 'Maritime communications': '{\n    "Product Type": ["thermal_printer", "newsletter", "AIS transponder", "sonar software", "AIS base station", "integrated bridge system", "marine communication", "software update", "satellite communication", "AIS system", "electronic chart display", "communication equipment", "AIS Base Station management"],\n    "Technology": ["photographic_quality_output", "3D profiling", "transceiver", "maritime electronics", "IMO compliant", "data interpretation", "electromagnetic", "large_format_thermal_printing", "multi-head sonar support", "advanced_greyscale_capabilities", "high_resolution_imaging", "sdr_technology", "high-speed data transfer"],\n    "Application": ["vessel tracking", "collision avoidance", "maritime safety", "underwater survey", "vessel safety", "navigation aid tracking", "coastal surveillance", "maritime traffic monitoring", "remote operational coordination", "vessel data management", "space-based_ais", "offshore connectivity", "portable"]\n}'}
}

# Convert string values to dictionaries and remove newline characters and extra spaces
for key in tags_dict:
    tags_dict[key] = json.loads(tags_dict[key].replace('\n', '').replace('    ', ''))

print(tags_dict)
59/14:
import json

tags_dict = {'Naval': '{\n    "Product Type": ["marine coating", "acoustic control system", "leak detection"],\n    "Technology": ["anti-fouling", "Hydroacoustic Positioning", "acoustic telemetry", "Inertial Navigation System"],\n    "Application": ["hull protection", "BOP operation", "early warning", "infrastructure safety"]\n}', 'Fish finding': '{\n    "Product Type": [\n        "portable sonar",\n        "echo sounder",\n        "sonar system",\n        "navigation software"\n    ],\n    "Technology": [\n        "chirp technology",\n        "split-beam technology",\n        "wideband chirp",\n        "multi-frequency echo sounder"\n    ],\n    "Application": [\n        "fish finding",\n        "fish stock assessment",\n        "maritime navigation",\n        "catch monitoring"\n    ]\n}', 'Surveillance & monitoring': '{\n    "Product Type": ["sonar system", "surveillance system"],\n    "Technology": ["subsea technology", "underwater surveillance"],\n    "Application": ["security", "naval defense"]\n}', 'Underwater navigation & positioning': '{\n    "Product Type": [\n        "Underwater Altimeters",\n        "Transponder",\n        "Acoustic Positioning System",\n        "Modem"\n    ],\n    "Technology": [\n        "HiPAP",\n        "Hydroacoustic",\n        "GNSS technology",\n        "Dynamic Positioning"\n    ],\n    "Application": [\n        "ROV navigation",\n        "Deep water operation",\n        "Subsea navigation",\n        "Position tracking"\n    ]\n}', 'Autonomous and uncrewed solutions': '{\n    "Product Type": [\n        "transponder",\n        "Autonomous Echo Sounders Programming",\n        "marine survey equipment",\n        "Software"\n    ],\n    "Technology": [\n        "acoustic communication",\n        "3D sonar sensing",\n        "sensor fusion",\n        "machine learning"\n    ],\n    "Application": [\n        "underwater_navigation",\n        "marine research",\n        "Data Management",\n        "environmental monitoring"\n    ]\n}', 'Product category': '{\n    "Product Type": ["Software", "Electronics", "Apparel"],\n    "Technology": ["AI", "Blockchain", "IoT"],\n    "Application": ["Data Analysis", "Payment Processing", "Wearable Tracking"]\n}', 'Geophysical survey': '{\n    "Product Type": [\n        "acoustic doppler profiler",\n        "sub-bottom profiler",\n        "marine survey equipment",\n        "oceanography equipment"\n    ],\n    "Technology": [\n        "narrow beam",\n        "multibeam echo sounder integration",\n        "parametric sub-bottom profiler",\n        "low frequency transducers"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "sub-bottom imaging",\n        "sediment layer mapping",\n        "seabed penetration"\n    ]\n}', 'Surface navigation & positioning': '{\n    "Product Type": [\n        "scientific transceiver",\n        "echo sounder",\n        "marine transceiver",\n        "navigation echo sounder"\n    ],\n    "Technology": [\n        "underwater acoustics",\n        "wideband transceiver",\n        "multiplexing",\n        "dual power input"\n    ],\n    "Application": [\n        "fisheries science",\n        "ocean research",\n        "marine navigation",\n        "underwater vehicle navigation"\n    ]\n}', 'Seafloor mapping': '{\n    "Product Type": [\n        "portable hydrographic system",\n        "multibeam echosounder",\n        "side scan sonar",\n        "echo sounder"\n    ],\n    "Technology": [\n        "multibeam sonar",\n        "sidescan sonar",\n        "sonar_signal_processing",\n        "multifrequency"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "ocean floor mapping",\n        "marine survey",\n        "bathymetric mapping"\n    ]\n}', 'Fishery research': '{\n    "Product Type": [\n        "Aquaculture monitoring system",\n        "Information transfer system",\n        "Analyser",\n        "Sensor"\n    ],\n    "Technology": [\n        "Environmental sensing",\n        "Maritime broadband radio",\n        "Submersible technology",\n        "Sensor fusion"\n    ],\n    "Application": [\n        "Biomass measurement",\n        "Fish farming efficiency",\n        "Seawater analysis",\n        "Environmental monitoring"\n    ]\n}', 'Maritime communications': '{\n    "Product Type": ["thermal_printer", "newsletter", "AIS transponder", "sonar software", "AIS base station", "integrated bridge system", "marine communication", "software update", "satellite communication", "AIS system", "electronic chart display", "communication equipment", "AIS Base Station management"],\n    "Technology": ["photographic_quality_output", "3D profiling", "transceiver", "maritime electronics", "IMO compliant", "data interpretation", "electromagnetic", "large_format_thermal_printing", "multi-head sonar support", "advanced_greyscale_capabilities", "high_resolution_imaging", "sdr_technology", "high-speed data transfer"],\n    "Application": ["vessel tracking", "collision avoidance", "maritime safety", "underwater survey", "vessel safety", "navigation aid tracking", "coastal surveillance", "maritime traffic monitoring", "remote operational coordination", "vessel data management", "space-based_ais", "offshore connectivity", "portable"]\n}'}


# Convert string values to dictionaries and remove newline characters and extra spaces
for key in tags_dict:
    tags_dict[key] = json.loads(tags_dict[key].replace('\n', '').replace('    ', ''))

print(tags_dict)
59/15:
import json

tags_dict = {'Naval': '{\n    "Product Type": ["marine coating", "acoustic control system", "leak detection"],\n    "Technology": ["anti-fouling", "Hydroacoustic Positioning", "acoustic telemetry", "Inertial Navigation System"],\n    "Application": ["hull protection", "BOP operation", "early warning", "infrastructure safety"]\n}', 'Fish finding': '{\n    "Product Type": [\n        "portable sonar",\n        "echo sounder",\n        "sonar system",\n        "navigation software"\n    ],\n    "Technology": [\n        "chirp technology",\n        "split-beam technology",\n        "wideband chirp",\n        "multi-frequency echo sounder"\n    ],\n    "Application": [\n        "fish finding",\n        "fish stock assessment",\n        "maritime navigation",\n        "catch monitoring"\n    ]\n}', 'Surveillance & monitoring': '{\n    "Product Type": ["sonar system", "surveillance system"],\n    "Technology": ["subsea technology", "underwater surveillance"],\n    "Application": ["security", "naval defense"]\n}', 'Underwater navigation & positioning': '{\n    "Product Type": [\n        "Underwater Altimeters",\n        "Transponder",\n        "Acoustic Positioning System",\n        "Modem"\n    ],\n    "Technology": [\n        "HiPAP",\n        "Hydroacoustic",\n        "GNSS technology",\n        "Dynamic Positioning"\n    ],\n    "Application": [\n        "ROV navigation",\n        "Deep water operation",\n        "Subsea navigation",\n        "Position tracking"\n    ]\n}', 'Autonomous and uncrewed solutions': '{\n    "Product Type": [\n        "transponder",\n        "Autonomous Echo Sounders Programming",\n        "marine survey equipment",\n        "Software"\n    ],\n    "Technology": [\n        "acoustic communication",\n        "3D sonar sensing",\n        "sensor fusion",\n        "machine learning"\n    ],\n    "Application": [\n        "underwater_navigation",\n        "marine research",\n        "Data Management",\n        "environmental monitoring"\n    ]\n}', 'Product category': '{\n    "Product Type": ["Software", "Electronics", "Apparel"],\n    "Technology": ["AI", "Blockchain", "IoT"],\n    "Application": ["Data Analysis", "Payment Processing", "Wearable Tracking"]\n}', 'Geophysical survey': '{\n    "Product Type": [\n        "acoustic doppler profiler",\n        "sub-bottom profiler",\n        "marine survey equipment",\n        "oceanography equipment"\n    ],\n    "Technology": [\n        "narrow beam",\n        "multibeam echo sounder integration",\n        "parametric sub-bottom profiler",\n        "low frequency transducers"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "sub-bottom imaging",\n        "sediment layer mapping",\n        "seabed penetration"\n    ]\n}', 'Surface navigation & positioning': '{\n    "Product Type": [\n        "scientific transceiver",\n        "echo sounder",\n        "marine transceiver",\n        "navigation echo sounder"\n    ],\n    "Technology": [\n        "underwater acoustics",\n        "wideband transceiver",\n        "multiplexing",\n        "dual power input"\n    ],\n    "Application": [\n        "fisheries science",\n        "ocean research",\n        "marine navigation",\n        "underwater vehicle navigation"\n    ]\n}', 'Seafloor mapping': '{\n    "Product Type": [\n        "portable hydrographic system",\n        "multibeam echosounder",\n        "side scan sonar",\n        "echo sounder"\n    ],\n    "Technology": [\n        "multibeam sonar",\n        "sidescan sonar",\n        "sonar_signal_processing",\n        "multifrequency"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "ocean floor mapping",\n        "marine survey",\n        "bathymetric mapping"\n    ]\n}', 'Fishery research': '{\n    "Product Type": [\n        "Aquaculture monitoring system",\n        "Information transfer system",\n        "Analyser",\n        "Sensor"\n    ],\n    "Technology": [\n        "Environmental sensing",\n        "Maritime broadband radio",\n        "Submersible technology",\n        "Sensor fusion"\n    ],\n    "Application": [\n        "Biomass measurement",\n        "Fish farming efficiency",\n        "Seawater analysis",\n        "Environmental monitoring"\n    ]\n}', 'Maritime communications': '{\n    "Product Type": ["thermal_printer", "newsletter", "AIS transponder", "sonar software", "AIS base station", "integrated bridge system", "marine communication", "software update", "satellite communication", "AIS system", "electronic chart display", "communication equipment", "AIS Base Station management"],\n    "Technology": ["photographic_quality_output", "3D profiling", "transceiver", "maritime electronics", "IMO compliant", "data interpretation", "electromagnetic", "large_format_thermal_printing", "multi-head sonar support", "advanced_greyscale_capabilities", "high_resolution_imaging", "sdr_technology", "high-speed data transfer"],\n    "Application": ["vessel tracking", "collision avoidance", "maritime safety", "underwater survey", "vessel safety", "navigation aid tracking", "coastal surveillance", "maritime traffic monitoring", "remote operational coordination", "vessel data management", "space-based_ais", "offshore connectivity", "portable"]\n}'}


# Convert string values to dictionaries and remove newline characters and extra spaces
for key in tags_dict:
    tags_dict[key] = json.loads(tags_dict[key].replace('\n', '').replace('    ', ''))

print(tags_dict)

for key in tags_dict:
    print(key)
    print(tags_dict[key])
    print("_______________________________________")
59/16:
import json

tags_dict = {'Naval': '{\n    "Product Type": ["marine coating", "acoustic control system", "leak detection"],\n    "Technology": ["anti-fouling", "Hydroacoustic Positioning", "acoustic telemetry", "Inertial Navigation System"],\n    "Application": ["hull protection", "BOP operation", "early warning", "infrastructure safety"]\n}', 'Fish finding': '{\n    "Product Type": [\n        "portable sonar",\n        "echo sounder",\n        "sonar system",\n        "navigation software"\n    ],\n    "Technology": [\n        "chirp technology",\n        "split-beam technology",\n        "wideband chirp",\n        "multi-frequency echo sounder"\n    ],\n    "Application": [\n        "fish finding",\n        "fish stock assessment",\n        "maritime navigation",\n        "catch monitoring"\n    ]\n}', 'Surveillance & monitoring': '{\n    "Product Type": ["sonar system", "surveillance system"],\n    "Technology": ["subsea technology", "underwater surveillance"],\n    "Application": ["security", "naval defense"]\n}', 'Underwater navigation & positioning': '{\n    "Product Type": [\n        "Underwater Altimeters",\n        "Transponder",\n        "Acoustic Positioning System",\n        "Modem"\n    ],\n    "Technology": [\n        "HiPAP",\n        "Hydroacoustic",\n        "GNSS technology",\n        "Dynamic Positioning"\n    ],\n    "Application": [\n        "ROV navigation",\n        "Deep water operation",\n        "Subsea navigation",\n        "Position tracking"\n    ]\n}', 'Autonomous and uncrewed solutions': '{\n    "Product Type": [\n        "transponder",\n        "Autonomous Echo Sounders Programming",\n        "marine survey equipment",\n        "Software"\n    ],\n    "Technology": [\n        "acoustic communication",\n        "3D sonar sensing",\n        "sensor fusion",\n        "machine learning"\n    ],\n    "Application": [\n        "underwater_navigation",\n        "marine research",\n        "Data Management",\n        "environmental monitoring"\n    ]\n}', 'Product category': '{\n    "Product Type": ["Software", "Electronics", "Apparel"],\n    "Technology": ["AI", "Blockchain", "IoT"],\n    "Application": ["Data Analysis", "Payment Processing", "Wearable Tracking"]\n}', 'Geophysical survey': '{\n    "Product Type": [\n        "acoustic doppler profiler",\n        "sub-bottom profiler",\n        "marine survey equipment",\n        "oceanography equipment"\n    ],\n    "Technology": [\n        "narrow beam",\n        "multibeam echo sounder integration",\n        "parametric sub-bottom profiler",\n        "low frequency transducers"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "sub-bottom imaging",\n        "sediment layer mapping",\n        "seabed penetration"\n    ]\n}', 'Surface navigation & positioning': '{\n    "Product Type": [\n        "scientific transceiver",\n        "echo sounder",\n        "marine transceiver",\n        "navigation echo sounder"\n    ],\n    "Technology": [\n        "underwater acoustics",\n        "wideband transceiver",\n        "multiplexing",\n        "dual power input"\n    ],\n    "Application": [\n        "fisheries science",\n        "ocean research",\n        "marine navigation",\n        "underwater vehicle navigation"\n    ]\n}', 'Seafloor mapping': '{\n    "Product Type": [\n        "portable hydrographic system",\n        "multibeam echosounder",\n        "side scan sonar",\n        "echo sounder"\n    ],\n    "Technology": [\n        "multibeam sonar",\n        "sidescan sonar",\n        "sonar_signal_processing",\n        "multifrequency"\n    ],\n    "Application": [\n        "seafloor mapping",\n        "ocean floor mapping",\n        "marine survey",\n        "bathymetric mapping"\n    ]\n}', 'Fishery research': '{\n    "Product Type": [\n        "Aquaculture monitoring system",\n        "Information transfer system",\n        "Analyser",\n        "Sensor"\n    ],\n    "Technology": [\n        "Environmental sensing",\n        "Maritime broadband radio",\n        "Submersible technology",\n        "Sensor fusion"\n    ],\n    "Application": [\n        "Biomass measurement",\n        "Fish farming efficiency",\n        "Seawater analysis",\n        "Environmental monitoring"\n    ]\n}', 'Maritime communications': '{\n    "Product Type": ["thermal_printer", "newsletter", "AIS transponder", "sonar software", "AIS base station", "integrated bridge system", "marine communication", "software update", "satellite communication", "AIS system", "electronic chart display", "communication equipment", "AIS Base Station management"],\n    "Technology": ["photographic_quality_output", "3D profiling", "transceiver", "maritime electronics", "IMO compliant", "data interpretation", "electromagnetic", "large_format_thermal_printing", "multi-head sonar support", "advanced_greyscale_capabilities", "high_resolution_imaging", "sdr_technology", "high-speed data transfer"],\n    "Application": ["vessel tracking", "collision avoidance", "maritime safety", "underwater survey", "vessel safety", "navigation aid tracking", "coastal surveillance", "maritime traffic monitoring", "remote operational coordination", "vessel data management", "space-based_ais", "offshore connectivity", "portable"]\n}'}


# Convert string values to dictionaries and remove newline characters and extra spaces
for key in tags_dict:
    tags_dict[key] = json.loads(tags_dict[key].replace('\n', '').replace('    ', ''))

print(tags_dict)
59/17:
#column names
#print colmns name
print(df.columns)
59/18:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
59/19:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    messages = [
                        {
                            "role": "system",
                            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                        },
                        {
                            "role": "user",
                            "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                        }
                    ]
                    response = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        messages=messages,
                        response_format={ "type": "json_object" }
                    )
                    tags = response.choices[0].message.content.strip()
                    df.loc[index, "tags"] = tags
                    df.loc[index, "filter_tags"] = category_tags
59/20:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
59/21:
from openai._client import OpenAI
import streamlit as st
client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
59/22:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    messages = [
                        {
                            "role": "system",
                            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                        },
                        {
                            "role": "user",
                            "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                        }
                    ]
                    response = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        messages=messages,
                        response_format={ "type": "json_object" }
                    )
                    tags = response.choices[0].message.content.strip()
                    df.loc[index, "tags"] = tags
                    df.loc[index, "filter_tags"] = category_tags
59/23:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    messages = [
                        {
                            "role": "system",
                            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                        },
                        {
                            "role": "user",
                            "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                        }
                    ]
                    response = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        messages=messages,
                        response_format={ "type": "json_object" }
                    )
                    tags = response.choices[0].message.content.strip()
                    df.loc[index, "tags"] = tags
                    df.loc[index, "filter_tags"] = ""
59/24:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    # messages = [
                    #     {
                    #         "role": "system",
                    #         "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                    #     },
                    #     {
                    #         "role": "user",
                    #         "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                    #     }
                    # ]
                    # response = client.chat.completions.create(
                    #     model="gpt-4-1106-preview",
                    #     messages=messages,
                    #     response_format={ "type": "json_object" }
                    # )
                    # tags = response.choices[0].message.content.strip()
                    df.loc[index, "tags"] = ""
                    df.loc[index, "filter_tags"] = ""
59/25:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    messages = [
                        {
                            "role": "system",
                            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                        },
                        {
                            "role": "user",
                            "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                        }
                    ]
                    response = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        messages=messages,
                        response_format={ "type": "json_object" }
                    )
                    tags = response.choices[0].message.content.strip()
                    if pd.isna(row["tags"]):
                        # If not, write the new tags
                        df.loc[index, "tags"] = tags
                    else:
                        # If yes, append the new tags to the existing ones
                        df.loc[index, "tags"] += ", " + tags

                    df.loc[index, "filter_tags"] = ""
59/26:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    messages = [
                        {
                            "role": "system",
                            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                        },
                        {
                            "role": "user",
                            "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                        }
                    ]
                    response = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        messages=messages,
                        response_format={ "type": "json_object" }
                    )
                    tags = response.choices[0].message.content.strip()
                    if pd.isna(row["product_tags"]):
                        # If not, write the new tags
                        df.loc[index, "product_tags"] = tags
                    else:
                        # If yes, append the new tags to the existing ones
                        df.loc[index, "product_tags"] += ", " + tags

                    df.loc[index, "filter_tags"] = ""
59/27:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    messages = [
                        {
                            "role": "system",
                            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                        },
                        {
                            "role": "user",
                            "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                        }
                    ]
                    response = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        messages=messages,
                        response_format={ "type": "json_object" }
                    )
                    tags = response.choices[0].message.content.strip()
                    print(tags)
                    df.loc[index, "product_tags"] += ", " + tags
59/28:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    messages = [
                        {
                            "role": "system",
                            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                        },
                        {
                            "role": "user",
                            "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                        }
                    ]
                    response = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        messages=messages,
                        response_format={ "type": "json_object" }
                    )
                    tags = response.choices[0].message.content.strip()
                    print(tags)
                    if pd.isna(df.loc[index, "product_tags"]):
                        df.loc[index, "product_tags"] = tags
                    else:
                        df.loc[index, "product_tags"] += ", " + tags
59/29:
import pandas as pd
df = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")

# check if comma in product category
print(df["Product category"].str.contains(",").sum())

# iterate over rows and check if comma in product category
for index, row in df.iterrows():
    if isinstance(row["Product category"], str) and "," in row["Product category"]:
        # check if nan
        if not pd.isna(row["Product category"]):
            category_list = [category.strip() for category in row["Product category"].split(",")]

            for category in category_list:
                if category in tags_dict:
                    category_tags = tags_dict[category]  # Use a different variable
                    print(row["Product_Name"])
                    print(category_tags)
                    # choose 8 tags from the tags list for each product
                    messages = [
                        {
                            "role": "system",
                            "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                        },
                        {
                            "role": "user",
                            "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                        }
                    ]
                    response = client.chat.completions.create(
                        model="gpt-4-1106-preview",
                        messages=messages,
                        response_format={ "type": "json_object" }
                    )
                    tags = response.choices[0].message.content.strip()
                    if pd.isna(df.loc[index, "product_tags"]):
                        df.loc[index, "product_tags"] = tags
                        print(tags)
                    else:
                        df.loc[index, "product_tags"] += ", " + tags
                        print(tags)
59/30:
#sazve to csv
df.to_csv("./data/df_tags_use_app_22_11_issoftware.csv", index=False)
59/31:
#sazve to csv
df.to_csv("./data/df_tags_use_app_22_11_issoftware.csv", index=False)
df.to_excel("./data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
59/32:
#sazve to csv
csv_pd = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
csv_pd.to_excel("data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
59/33:
#sazve to csv
csv_pd = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv")
csv_pd.to_excel("data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
59/34:
# iterate over rows and check if product name matches
df = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
for index, row in df.iterrows():
    if row["Product_Name"] == "Multibeam echosounder, ROV / AUV":
        category = row["Product category"]
        if category in tags_dict:
            category_tags = tags_dict[category]  # Use a different variable
            print(row["Product_Name"])
            print(category_tags)
            # choose 8 tags from the tags list for each product
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                },
                {
                    "role": "user",
                    "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                }
            ]
            response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=messages,
                response_format={ "type": "json_object" }
            )
            tags = response.choices[0].message.content.strip()
            df.loc[index, "product_tags"] = tags
59/35: df.to_excel("data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
59/36:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
59/37:
# iterate over rows and check if product name matches
df = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
for index, row in df.iterrows():
    if row["Product_Name"] == "PX MultiSensor Charger":
        category = row["Product category"]
        if category in tags_dict:
            category_tags = tags_dict[category]  # Use a different variable
            print(row["Product_Name"])
            print(category_tags)
            # choose 8 tags from the tags list for each product
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
                },
                {
                    "role": "user",
                    "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
                }
            ]
            response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=messages,
                response_format={ "type": "json_object" }
            )
            tags = response.choices[0].message.content.strip()
            df.loc[index, "product_tags"] = tags
59/38: df.to_excel("data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
59/39:
df.to_excel("data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
df.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
60/1: import pandas as pd
59/40:
df.to_excel("data/df_tags_use_app_22_11_issoftware.xlsx", index=False)
df.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
60/2:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware.csv", index=False)
60/3:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware2.csv", index=False)
60/4:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware2.csv", index=False)
60/5:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware2.csv", index=False)
60/6:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware2.csv", index=False)
60/7:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware2.csv", index=False)
62/1:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
62/2: df_tags = pd.read_csv("df_tags_use_app_22_11_issoftwares.csv", usecols=["Product_Name","Product category", "url", "img_url", "is_software"])
62/3: df_tags = pd.read_csv("data/df_tags_use_app_22_11_issoftwares.csv", usecols=["Product_Name","Product category", "url", "img_url", "is_software"])
62/4: df_tags = pd.read_csv("/data/df_tags_use_app_22_11_issoftwares.csv", usecols=["Product_Name","Product category", "url", "img_url", "is_software"])
62/5: df_tags = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv", usecols=["Product_Name","Product category", "url", "img_url", "is_software"])
62/6: df_tags = pd.read_csv("data/df_tags_use_app_22_11_issoftware.csv", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
62/7:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
62/8:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
61/1:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware2.csv", index=False)
61/2: import pandas as pd
61/3:
df_new = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx")
df_new.to_csv("data/df_tags_use_app_22_11_issoftware2.csv", index=False)
62/9:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
62/10:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
print(df_tags["Product_Name"][3])
62/11: df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
62/12:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
print(df_tags["Product_Name"][3])
62/13:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
62/14: print(url_text_dict)
62/15:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
62/16:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', product category {product_category} and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 6. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
62/17:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', product category {product_category} and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 6. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_27_11.csv", index=False)
62/18:
import ast

tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict_str = row.new_tags
    try:
        tags_dict = ast.literal_eval(tags_dict_str)
    except (SyntaxError, ValueError):
        print(f"Error parsing string to dict: {tags_dict_str}")
        continue

    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
print(len(tag_list))
62/19:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

categories = [category.strip() for category in categories]
62/20:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
62/21:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict_temp = ast.literal_eval(tags_dict_str)
            for tags in tags_dict_temp.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
62/22: print(tags_dict)
62/23:
#save tags_dict as a colum with coresponding category
# df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
df_tags_tech["tags_dict"] = ""
for index, row in df_tags_tech.iterrows():
    print(row)
    category = row["Product category"]
    print(category)
    tags_dict_category = tags_dict.get(category)
    print(tags_dict_category)
    df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


df_tags_tech.to_csv("./data/tags_27_11.csv", index=False)
62/24:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/25: df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/26:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
62/27:
# import requests
# from bs4 import BeautifulSoup

# def extract_image_url(url):
#     print(url)
#     if url == "https://www.kongsberg.comn":
#         return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
#     if pd.isna(url) or 'http' not in url:
#         return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
#     # Get the HTML of the page
#     response = requests.get(url)

#     # If the response status code is 404, return the dummy image URL
#     if response.status_code == 404:
#         return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

#     html = response.text

#     # Parse the HTML with BeautifulSoup
#     soup = BeautifulSoup(html, 'html.parser')

#     # Find the image
#     img = soup.find('img')

#     # If the image was found, return its source URL
#     if img is not None:
#         img_url = "https://www.kongsberg.com"+img['src']
#         return img_url
#     else:
#         return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# # iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
# df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# # save the dataframe as csv
# df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
62/28:
df_tech_new_tags.to_excel("./data/tags_27_11.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/29:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
62/30:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
62/31: # df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv")
62/32:
# import re
# ant = 0
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     if not pd.isna(row["tags"]):
#         # ant_tags = row["tags"].split(",")
#         ant_tags = re.split(',|:', row["tags"])
#         print(ant_tags)
#         if len(ant_tags) > 12:
#         # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
#             ant += 1
#             print(index)
#             print(row["tags"])
#             print(len(ant_tags))
#             print("found more than 10 tags")
#             print("------------------")
#             # print(row["tags_dict"])
#             print("updating tags")
#             # prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
#             # tags_dict,  row["Product category"], row["Product_Name"]
#             # )
#             # tags = general_gpt(prompt)
#             # print("New tags: ", tags)
#             # df_tech_new_tags.loc[index, "tags"] = tags
#             print("------------------")
#             # update the tags column with only 8 tags


# print("ant, ",ant)

# # print(df_tech_new_tags["tags"])
62/33:
# df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
62/34:
# # Read the dataframe from the CSV file
# df_tags_tech = pd.read_csv("./data/tags_gpt4.csv")

# # Group the dataframe by "Product category" and aggregate the "tags" column
# grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# # Create a dictionary to store the tags for each category
# all_tags_by_category = {}

# for category, tags_list in grouped_tags.items():
#     # Flatten the list of tags
#     tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
#     # Remove duplicates
#     tags = list(set(tags))
#     # Add the tags to the dictionary
#     all_tags_by_category[category] = ",".join(tags)

# print(all_tags_by_category)
62/35:
# import json
# df_tags_tech = pd.read_csv("./data/tags_gpt4.csv")
# # Create a set of all product tags
# product_tags = set()
# for category, group in df_tags_tech.groupby("Product category"):
#     category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
#     product_tags.update(category_tags)

# non_matching_tags = []

# # Iterate over each tag in dict_tags
# for index, row in df_tags_tech.iterrows():
#     tags_dict_str = row["tags_dict"]
#     if pd.notna(tags_dict_str):
#         tags_dict_str = tags_dict_str.replace("'", '"')
#         print(tags_dict_str)  # Add this line
#         try:
#             tags_dict = json.loads(tags_dict_str)
#         except json.JSONDecodeError as e:
#             print(f"Error decoding JSON for row {index}: {e}")
#             continue

#         for tag in tags_dict.values():
#             for t in tag:
#                 # Strip leading/trailing whitespace and quotes from t
#                 t = t.strip("' ").strip()
#                 if t not in product_tags:
#                     non_matching_tags.append(t)

# # Print non-matching tags
# if non_matching_tags:
#     print("The following tags do not have any corresponding tags in their category:")
#     for tag in set(non_matching_tags):
#         print(tag)
62/36:
# # Load the data
# df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# # print len of tags that are empty or null
# print(len(df[df['tags'].isnull()]))
# #remove if tags is null
# df = df[df['tags'].notnull()]
# print(len(df[df['tags'].apply(lambda x: len(x) == 0)]))

# # Print unique values in the 'tags' column
# # print(df.columns)
# # print(df['tags'].unique())
# # # Print the 'tags' column
# # print(df['tags'])

# # Convert the 'tags' column to a dictionary
# df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# # Extract the values of the key 'tags' and split them into a list for each row
# df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# # Print the 'tag_values' column
# # print(df['tag_values'])

# # print all the empty list of tags
# print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
63/1:
df_new = pd.read_excel("data/tags_27_11.xlsx")
df_new.to_csv("data/tags_27_11.csv", index=False)
63/2:
import time
import pandas as pd
import numpy as np
import streamlit as st
from openai._client import OpenAI
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
63/3:
df_new = pd.read_excel("data/tags_27_11.xlsx")
df_new.to_csv("data/tags_27_11.csv", index=False)
63/4:
df_new = pd.read_excel("data/tags_27_11.xlsx")
df_new.to_csv("data/tags_27_11.csv", index=False)
63/5:
df_new = pd.read_excel("data/tags_27_11.xlsx")
df_new.to_csv("data/tags_27_11.csv", index=False)
62/37:
df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
62/38:
df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()

print(len(df_tags))
62/39:
df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
62/40:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
62/41:

# # load the dataframes
# def load_dataframes():
#     csv_url_path = "./data/Products and Categories.xlsx"
#     tags_url_path = "./data/dummy - Product category and tags.xlsx"
#     df_url = pd.read_excel(csv_url_path)
#     df_tags = pd.read_excel(tags_url_path)
#     return df_url, df_tags


# def process_dataframes(df_url, df_tags):
#     print(len(df_url))
#     print(len(df_tags))

#     # set row to as header
#     print(df_tags.iloc[0])
#     df_tags.columns = df_tags.iloc[0]
#     df_url = df_url.drop_duplicates(subset=['Product Name'])

#     # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
#     df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

#     # drop columns nan, Technology, General, Related products, Varient products
#     df_tags = df_tags.drop(
#         columns=[
#             "Technology",
#             "General",
#             "Related products",
#             "Variant products",
#             "Contains / made from products",
#         ],
#         axis=1,
#     )
#     print(len(df_tags))
#     # add the string "https://www.kongsberg.com/" to the url column for each url
#     df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
#     # rename the column Product Name to Product_Name
#     df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

#     return df_tags
# df_url, df_tags = load_dataframes()
# df_tags = process_dataframes(df_url, df_tags)
62/42:
df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
62/43:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
print(df_tags["Product_Name"][3])
62/44:
# def extract_text(url):
#     # Get the HTML of the page
#     response = requests.get(url)
#     html = response.text

#     # Parse the HTML with BeautifulSoup
#     soup = BeautifulSoup(html, 'html.parser')

#     # Find all elements with the specified class
#     elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

#     # Extract the text of each element
#     rich_text = [element.get_text(strip=True) for element in elements]
#     rich_text_string = ' '.join(rich_text)

#     link = soup.find('a', href='#technicalInformation')

#     bullet_points = []
#     bullet_points_string = ""
#     # If the link was found, find the element it links to
#     if link is not None:
#         target_id = link['href'].lstrip('#')
#         target_element = soup.find(id=target_id)

#         # If the target element was found, get its text
#         if target_element is not None:
#             # Find all the list items in the target element
#             list_items = target_element.find_all('li')

#             # Extract the text of each list item
#             bullet_points = [li.get_text(strip=True) for li in list_items]
#             bullet_points_string = '\n'.join(bullet_points)

        
#     text = rich_text_string +" \n "+ bullet_points_string
#     return text
# url_text_dict = {}
# for index, row in df_tags.iterrows():
#     url = row["url"]
#     text = extract_text(url)
#     url_text_dict[url] = text
#     print(index, "/", len(df_tags))


# # save the dict as a json file
# print(url_text_dict)
62/45:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
62/46: print(url_text_dict)
62/47:
# # save the dict as pickle file
# import pickle
# with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
#     pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
62/48:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
62/49:
# def general_gpt(prompt: str):
#     # make chat gpt completion function with streamlit
#     openai.api_key = st.secrets["openai"]["api_key"]
#     openai.api_type = "azure"
#     openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
#     openai.api_version = "2023-03-15-preview"
#     # gpt_model = "gpt-35-turbo"
#     gpt_model = "gpt-35-turbo-16k"
#     # gpt_model = "gpt-4"
#     completion = openai.ChatCompletion.create(
#         deployment_id=gpt_model,
#         messages=[
#             {
#                 "role": "user",
#                 "content": "{}".format(prompt),
#             }
#         ],
#         temperature=0.3,
#         max_tokens=1500,
#         top_p=1.0,
#         frequency_penalty=0.1,
#         presence_penalty=0.1,
#     )
#     return str(completion.choices[0].message.content)
62/50:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', product category {product_category} and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 6. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_27_11.csv", index=False)
62/51:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
62/52:

# # load the dataframes
# def load_dataframes():
#     csv_url_path = "./data/Products and Categories.xlsx"
#     tags_url_path = "./data/dummy - Product category and tags.xlsx"
#     df_url = pd.read_excel(csv_url_path)
#     df_tags = pd.read_excel(tags_url_path)
#     return df_url, df_tags


# def process_dataframes(df_url, df_tags):
#     print(len(df_url))
#     print(len(df_tags))

#     # set row to as header
#     print(df_tags.iloc[0])
#     df_tags.columns = df_tags.iloc[0]
#     df_url = df_url.drop_duplicates(subset=['Product Name'])

#     # Merge df_tags and df_url on 'Product_Name', only adding the 'url' column
#     df_tags = df_tags.merge(df_url[["Product Name", "url"]], on="Product Name", how="left")

#     # drop columns nan, Technology, General, Related products, Varient products
#     df_tags = df_tags.drop(
#         columns=[
#             "Technology",
#             "General",
#             "Related products",
#             "Variant products",
#             "Contains / made from products",
#         ],
#         axis=1,
#     )
#     print(len(df_tags))
#     # add the string "https://www.kongsberg.com/" to the url column for each url
#     df_tags["url"] = "https://www.kongsberg.com" + df_tags["url"].astype(str)
#     # rename the column Product Name to Product_Name
#     df_tags = df_tags.rename(columns={"Product Name": "Product_Name"})

#     return df_tags
# df_url, df_tags = load_dataframes()
# df_tags = process_dataframes(df_url, df_tags)
62/53:
df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
62/54:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
print(df_tags["Product_Name"][3])
62/55:
# def extract_text(url):
#     # Get the HTML of the page
#     response = requests.get(url)
#     html = response.text

#     # Parse the HTML with BeautifulSoup
#     soup = BeautifulSoup(html, 'html.parser')

#     # Find all elements with the specified class
#     elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

#     # Extract the text of each element
#     rich_text = [element.get_text(strip=True) for element in elements]
#     rich_text_string = ' '.join(rich_text)

#     link = soup.find('a', href='#technicalInformation')

#     bullet_points = []
#     bullet_points_string = ""
#     # If the link was found, find the element it links to
#     if link is not None:
#         target_id = link['href'].lstrip('#')
#         target_element = soup.find(id=target_id)

#         # If the target element was found, get its text
#         if target_element is not None:
#             # Find all the list items in the target element
#             list_items = target_element.find_all('li')

#             # Extract the text of each list item
#             bullet_points = [li.get_text(strip=True) for li in list_items]
#             bullet_points_string = '\n'.join(bullet_points)

        
#     text = rich_text_string +" \n "+ bullet_points_string
#     return text
# url_text_dict = {}
# for index, row in df_tags.iterrows():
#     url = row["url"]
#     text = extract_text(url)
#     url_text_dict[url] = text
#     print(index, "/", len(df_tags))


# # save the dict as a json file
# print(url_text_dict)
62/56:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
62/57: print(url_text_dict)
62/58:
# # save the dict as pickle file
# import pickle
# with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
#     pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
62/59:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
62/60:
# def general_gpt(prompt: str):
#     # make chat gpt completion function with streamlit
#     openai.api_key = st.secrets["openai"]["api_key"]
#     openai.api_type = "azure"
#     openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
#     openai.api_version = "2023-03-15-preview"
#     # gpt_model = "gpt-35-turbo"
#     gpt_model = "gpt-35-turbo-16k"
#     # gpt_model = "gpt-4"
#     completion = openai.ChatCompletion.create(
#         deployment_id=gpt_model,
#         messages=[
#             {
#                 "role": "user",
#                 "content": "{}".format(prompt),
#             }
#         ],
#         temperature=0.3,
#         max_tokens=1500,
#         top_p=1.0,
#         frequency_penalty=0.1,
#         presence_penalty=0.1,
#     )
#     return str(completion.choices[0].message.content)
62/61:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', product category {product_category} and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 6. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_27_11.csv", index=False)
62/62:
import ast

tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict_str = row.new_tags
    try:
        tags_dict = ast.literal_eval(tags_dict_str)
    except (SyntaxError, ValueError):
        print(f"Error parsing string to dict: {tags_dict_str}")
        continue

    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
print(len(tag_list))
62/63:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

categories = [category.strip() for category in categories]
62/64:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
62/65:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict_temp = ast.literal_eval(tags_dict_str)
            for tags in tags_dict_temp.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
62/66: print(tags_dict)
62/67:
# #save tags_dict as a colum with coresponding category
# # df_tag_tech_test = pd.read_csv("./data/df_tags_technology.csv")
# df_tags_tech["tags_dict"] = ""
# for index, row in df_tags_tech.iterrows():
#     print(row)
#     category = row["Product category"]
#     print(category)
#     tags_dict_category = tags_dict.get(category)
#     print(tags_dict_category)
#     df_tags_tech.loc[index, "tags_dict"] = tags_dict_category


# df_tags_tech.to_csv("./data/tags_27_11.csv", index=False)
62/68:

# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()

# Iterate over the rows of the dataframe
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_dict:
        category_tags = tags_dict[product_category]  # Use a different variable
        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_dict")


# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/69: df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/70:
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     product_category = row["Product category"]
#     # Get the tags for the product category
#     if product_category in tags_dict:
#         tags = tags_dict[product_category]
#         print(tags)

#         # Remove "High-Level Tag" from the tags
#         tags = [tag for tag in tags if tag != "High-Level Tag"]

#         # choose 8 tags from the tags list for each product
#         prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 8 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 8 tags from the given list.".format(
#             tags,  row["Product category"], row["Product_Name"]
#         )
#         tags = general_gpt(prompt)
#         df_tech_new_tags.loc[index, "tags"] = tags
#     else:
#         print(f"Category '{product_category}' not found in tags_dict")
62/71:
# import requests
# from bs4 import BeautifulSoup

# def extract_image_url(url):
#     print(url)
#     if url == "https://www.kongsberg.comn":
#         return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
#     if pd.isna(url) or 'http' not in url:
#         return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
#     # Get the HTML of the page
#     response = requests.get(url)

#     # If the response status code is 404, return the dummy image URL
#     if response.status_code == 404:
#         return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"

#     html = response.text

#     # Parse the HTML with BeautifulSoup
#     soup = BeautifulSoup(html, 'html.parser')

#     # Find the image
#     img = soup.find('img')

#     # If the image was found, return its source URL
#     if img is not None:
#         img_url = "https://www.kongsberg.com"+img['src']
#         return img_url
#     else:
#         return "https://www.feed-image-editor.com/sites/default/files/perm/wysiwyg/image_not_available.png"
# # iterate over the rows of the dataframe, and find the image URL for each url and save it in a new column
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_22_11.csv")
# df_tech_new_tags["image_url"] = df_tech_new_tags["url"].apply(extract_image_url)

# # save the dataframe as csv
# df_tech_new_tags.to_csv("./data/df_tags_use_app_22_11.csv", index=False)
62/72:
df_tech_new_tags.to_excel("./data/tags_27_11.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/73:
# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     # Check if the "tags" column starts with "product"
#     # Check if the "tags" column starts with "product" or "Product"
#     if "Product Type" in str(row["tags"]):
#     # if str(row["tags"]).startswith("product") or str(row["tags"]).startswith("Product") or str(row["tags"]).startswith("Product Type"):
#         print("found occurence")

#         product_category = row["Product category"]
#         # Get the tags for the product category
#         if product_category in tags_dict:
#             print("found category")
#             tags_dict = tags_dict[product_category]

#             # choose 5 tags from the tags list for each product
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"Category '{product_category}' not found in tags_dict")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
62/74:
# import ast
# df_tech_new_tags = pd.read_csv("./data/df_tags_use_app_15_11.csv")

# # Iterate over the rows of the dataframe
# for index, row in df_tech_new_tags.iterrows():
#     print(index)

#     product_category = row["Product category"]
#     tags_dict_str = row["tags_dict"]

#     if pd.notna(tags_dict_str):
#         # Convert the string representation of dictionary to a dictionary
#         tags_dict = ast.literal_eval(tags_dict_str)

#         # Check if "High-Level Tag" is in the values of tags_dict
#         if "High-Level Tag" in tags_dict.values():
#             # Repeat the process
#             prompt = "Given the list of tags: {} and the product category :{} and product: {}, please select 5 tags from the list of tags, that can be used as filters to best find this product on a website. Each secondlevel tag should correspond to minimun one product each. Your response should only be a comma-separated list of exactly 5 tags from the given list.".format(
#                 tags_dict,  row["Product category"], row["Product_Name"]
#             )
#             tags = general_gpt(prompt)
#             df_tech_new_tags.loc[index, "tags"] = tags
#             print(tags)
#         else:
#             print(f"'High-Level Tag' not found in tags_dict values")
#     else:
#         print("tags_dict_str is NaN")

# # save the dataframe as excel file
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
62/75: # df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv")
62/76:
# import re
# ant = 0
# for index, row in df_tech_new_tags.iterrows():
#     print(index)
#     if not pd.isna(row["tags"]):
#         # ant_tags = row["tags"].split(",")
#         ant_tags = re.split(',|:', row["tags"])
#         print(ant_tags)
#         if len(ant_tags) > 12:
#         # if "'Product Type'" in ant_tags or  "Product Type" in ant_tags or " Product Type" in ant_tags or "Product Type " in ant_tags or " Application" in ant_tags or " Technology" in ant_tags:
                
#             ant += 1
#             print(index)
#             print(row["tags"])
#             print(len(ant_tags))
#             print("found more than 10 tags")
#             print("------------------")
#             # print(row["tags_dict"])
#             print("updating tags")
#             # prompt = "Use this list of tags {}. And for this product: {}, please select 8 tags from the list of tags that can be used as filters to best find this product on a website. Your response should only be a comma-separated list of exactly 8 tags from the given list. Do not use the word 'tag' as a tag. Dont use Product Type, Technology or Application as tags either.".format(            
#             # tags_dict,  row["Product category"], row["Product_Name"]
#             # )
#             # tags = general_gpt(prompt)
#             # print("New tags: ", tags)
#             # df_tech_new_tags.loc[index, "tags"] = tags
#             print("------------------")
#             # update the tags column with only 8 tags


# print("ant, ",ant)

# # print(df_tech_new_tags["tags"])
62/77:
# df_tech_new_tags.to_csv("./data/df_tags_use_app_15_11.csv", index=False)
# df_tech_new_tags.to_excel("./data/df_tags_use_app_15_11.xlsx", index=False)
62/78:
# # Read the dataframe from the CSV file
# df_tags_tech = pd.read_csv("./data/tags_gpt4.csv")

# # Group the dataframe by "Product category" and aggregate the "tags" column
# grouped_tags = df_tags_tech.groupby("Product category")["tags"].agg(list)

# # Create a dictionary to store the tags for each category
# all_tags_by_category = {}

# for category, tags_list in grouped_tags.items():
#     # Flatten the list of tags
#     tags = [tag.strip("' ").strip() for tags in tags_list if isinstance(tags, str) for tag in tags.split(",")]
#     # Remove duplicates
#     tags = list(set(tags))
#     # Add the tags to the dictionary
#     all_tags_by_category[category] = ",".join(tags)

# print(all_tags_by_category)
62/79:
# import json
# df_tags_tech = pd.read_csv("./data/tags_gpt4.csv")
# # Create a set of all product tags
# product_tags = set()
# for category, group in df_tags_tech.groupby("Product category"):
#     category_tags = [tag.strip("' ").strip() for tag in all_tags_by_category[category].split(',')]
#     product_tags.update(category_tags)

# non_matching_tags = []

# # Iterate over each tag in dict_tags
# for index, row in df_tags_tech.iterrows():
#     tags_dict_str = row["tags_dict"]
#     if pd.notna(tags_dict_str):
#         tags_dict_str = tags_dict_str.replace("'", '"')
#         print(tags_dict_str)  # Add this line
#         try:
#             tags_dict = json.loads(tags_dict_str)
#         except json.JSONDecodeError as e:
#             print(f"Error decoding JSON for row {index}: {e}")
#             continue

#         for tag in tags_dict.values():
#             for t in tag:
#                 # Strip leading/trailing whitespace and quotes from t
#                 t = t.strip("' ").strip()
#                 if t not in product_tags:
#                     non_matching_tags.append(t)

# # Print non-matching tags
# if non_matching_tags:
#     print("The following tags do not have any corresponding tags in their category:")
#     for tag in set(non_matching_tags):
#         print(tag)
62/80:
# # Load the data
# df = pd.read_csv("./data/good_data/tags_gpt4.csv")

# # print len of tags that are empty or null
# print(len(df[df['tags'].isnull()]))
# #remove if tags is null
# df = df[df['tags'].notnull()]
# print(len(df[df['tags'].apply(lambda x: len(x) == 0)]))

# # Print unique values in the 'tags' column
# # print(df.columns)
# # print(df['tags'].unique())
# # # Print the 'tags' column
# # print(df['tags'])

# # Convert the 'tags' column to a dictionary
# df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})

# # Extract the values of the key 'tags' and split them into a list for each row
# df['tag_values'] = df['tags'].apply(lambda x: x['tags'].split(', ') if isinstance(x, dict) and 'tags' in x and isinstance(x['tags'], str) else [])

# # Print the 'tag_values' column
# # print(df['tag_values'])

# # print all the empty list of tags
# print(len(df[df['tag_values'].apply(lambda x: len(x) == 0)]))
63/6:
df_new = pd.read_excel("data/tags_27_11.xlsx")
df_new.to_csv("data/tags_27_11.csv", index=False)
63/7:
df_new = pd.read_excel("data/tags_27_11.xlsx")
#rename column dic_tags to filter_tags
df_new = df_new.rename(columns={"dic_tags": "filter_tags"})
df_new = df_new.rename(columns={"tags": "product_tags"})
df_new.to_csv("data/tags_27_11.csv", index=False)
63/8:
df_new = pd.read_excel("data/tags_27_11.xlsx")
#rename column dic_tags to filter_tags
df_new = df_new.rename(columns={"dict_tags": "filter_tags"})
df_new = df_new.rename(columns={"tags": "product_tags"})
df_new.to_csv("data/tags_27_11.csv", index=False)
62/81:
print(tags_dict)
import json

with open('tags_dict.json', 'w') as f:
    json.dump(tags_dict, f)
61/4:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({'product_tags': lambda x: list(x), 'Product category': lambda x: list(x), 'website url': 'first', 'image_url': 'first'}).reset_index()
61/5:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({'product_tags': lambda x: list(x), 'Product category': lambda x: list(x), 'url': 'first', 'image_url': 'first'}).reset_index()
61/6:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({'product_tags': lambda x: list(x), 'Product category': lambda x: list(x), 'url': 'first', 'image_url': 'first'}).reset_index()

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/7:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({'product_tags': lambda x: list(x), 'Product category': lambda x: list(x), 'url': 'first', 'image_url': 'first'}).reset_index()
print(len(df_with_many_tags))
# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/8:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))
# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/9:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].str.replace(r'{"tags":|}|"', '', regex=True)
print(len(df_with_many_tags))
# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/10:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].str.replace(r'{"tags":|}|"', '', regex=True)
print(len(df_with_many_tags))
# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/11:
import ast

def extract_tags(tags_str):
    tags_dict = ast.literal_eval(tags_str)
    return ', '.join(map(str.strip, tags_dict.values()))
61/12:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/13:
import ast

def extract_tags(tags_str):
    tags_dict = ast.literal_eval(tags_str)
    tags_values = []
    for value in tags_dict.values():
        if isinstance(value, list):
            tags_values.append(', '.join(map(str.strip, value)))
        else:
            tags_values.append(value.strip())
    return ', '.join(tags_values)
61/14:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/15:
import ast
def extract_tags(tags_str):
    tags_obj = ast.literal_eval(tags_str)
    if isinstance(tags_obj, dict):
        tags_values = tags_obj.values()
    elif isinstance(tags_obj, tuple):
        tags_values = tags_obj
    else:
        raise ValueError(f"Unexpected type {type(tags_obj)}: {tags_obj}")
    
    tags_list = []
    for value in tags_values:
        if isinstance(value, list):
            tags_list.append(', '.join(map(str.strip, value)))
        else:
            tags_list.append(value.strip())
    return ', '.join(tags_list)
61/16:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/17:
import ast
def extract_tags(tags_str):
    tags_obj = ast.literal_eval(tags_str)
    if isinstance(tags_obj, dict):
        tags_values = tags_obj.values()
    elif isinstance(tags_obj, tuple):
        tags_values = tags_obj
    else:
        raise ValueError(f"Unexpected type {type(tags_obj)}: {tags_obj}")
    
    tags_list = []
    for value in tags_values:
        if isinstance(value, list):
            tags_list.append(', '.join(map(str.strip, value)))
        elif isinstance(value, dict):
            tags_list.append(str(value))
        else:
            tags_list.append(value.strip())
    return ', '.join(tags_list)
61/18:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/19:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
61/20:
df_new = pd.read_excel("data/tags_27_11.xlsx")
df_new['product_tags'] = df_new['product_tags'].apply(extract_tags)

df_new.to_csv("data/tags_27_11.csv", index=False)
61/21:
df_new = pd.read_excel("data/tags_27_11.xlsx")
#rename columns tags to product_tags
df_new = df_new.rename(columns={"tags": "product_tags"})
df_new['product_tags'] = df_new['product_tags'].apply(extract_tags)

df_new.to_csv("data/tags_27_11.csv", index=False)
61/22:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
61/23:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(str.strip, x)),
    'Product category': lambda x: ', '.join(map(str.strip, x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
61/24:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
62/82:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
62/83:
df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
62/84:
df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
62/85:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
print(df_tags["Product_Name"][3])
62/86:
# def extract_text(url):
#     # Get the HTML of the page
#     response = requests.get(url)
#     html = response.text

#     # Parse the HTML with BeautifulSoup
#     soup = BeautifulSoup(html, 'html.parser')

#     # Find all elements with the specified class
#     elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

#     # Extract the text of each element
#     rich_text = [element.get_text(strip=True) for element in elements]
#     rich_text_string = ' '.join(rich_text)

#     link = soup.find('a', href='#technicalInformation')

#     bullet_points = []
#     bullet_points_string = ""
#     # If the link was found, find the element it links to
#     if link is not None:
#         target_id = link['href'].lstrip('#')
#         target_element = soup.find(id=target_id)

#         # If the target element was found, get its text
#         if target_element is not None:
#             # Find all the list items in the target element
#             list_items = target_element.find_all('li')

#             # Extract the text of each list item
#             bullet_points = [li.get_text(strip=True) for li in list_items]
#             bullet_points_string = '\n'.join(bullet_points)

        
#     text = rich_text_string +" \n "+ bullet_points_string
#     return text
# url_text_dict = {}
# for index, row in df_tags.iterrows():
#     url = row["url"]
#     text = extract_text(url)
#     url_text_dict[url] = text
#     print(index, "/", len(df_tags))


# # save the dict as a json file
# print(url_text_dict)
62/87:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
62/88: print(url_text_dict)
62/89:
# # save the dict as pickle file
# import pickle
# with open('./data/url_technical_text_dict.pkl', 'wb') as handle:
#     pickle.dump(url_text_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
62/90:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
62/91:
# def general_gpt(prompt: str):
#     # make chat gpt completion function with streamlit
#     openai.api_key = st.secrets["openai"]["api_key"]
#     openai.api_type = "azure"
#     openai.api_base = "https://cog-fxpoc-tonality-dev-01.openai.azure.com/"
#     openai.api_version = "2023-03-15-preview"
#     # gpt_model = "gpt-35-turbo"
#     gpt_model = "gpt-35-turbo-16k"
#     # gpt_model = "gpt-4"
#     completion = openai.ChatCompletion.create(
#         deployment_id=gpt_model,
#         messages=[
#             {
#                 "role": "user",
#                 "content": "{}".format(prompt),
#             }
#         ],
#         temperature=0.3,
#         max_tokens=1500,
#         top_p=1.0,
#         frequency_penalty=0.1,
#         presence_penalty=0.1,
#     )
#     return str(completion.choices[0].message.content)
62/92:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', product category {product_category} and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 6. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_27_11.csv", index=False)
62/93:
df_tags = pd.read_excel("data/df_tags_use_app_22_11_issoftware.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
62/94:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
print(df_tags["Product_Name"][3])
62/95:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
62/96: print(url_text_dict)
62/97:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
62/98:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', product category {product_category} and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 6. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_27_11.csv", index=False)
62/99:
# save the dict as a excel file
df = pd.DataFrame.from_dict(tags_dict, orient="index")
df.to_excel("filter_tags.xlsx")
62/100:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
62/101:
import time

def generate_tags_and_handle_rate_limit(df_tags):
    df_tags["new_tags"] = ""

    # Iterate over the rows of the dataframe
    for index, row in df_tags.iterrows():
        print(index)
        # Create a prompt using the Product_Name, category, and the text from the website
        url = row["url"]
        product_name = row["Product_Name"]
        product_category = row["Product category"]

        website_text = url_text_dict.get(url, "")

        prompt = f"Given the product description: '{website_text}', product category {product_category} and the product name: '{product_name}', make an appropiate number of tags per product, but no more than 6. These tags should be derived from the product description and be applicable to this and similar products. Please avoid using the product name directly as a tag. Your response should be a comma-separated list of the number of tags of your choice."
        # Call the general_gpt function with this prompt

        for _ in range(3):  # Retry up to 3 times
            try:
                response = client.chat.completions.create(
                    # model="gpt-3.5-turbo-1106",
                    model="gpt-4-1106-preview",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags. return json with following format: {'tags': ['tag1', 'tag2', 'tag3, etc...']}"},
                        {"role": "user", "content": "{}".format(prompt)}
                    ],
                    max_tokens=200,
                    response_format={ "type": "json_object" },
                    timeout=10  # Add a timeout
                )
                tags = response.choices[0].message.content.strip()
                print(tags)
                # Save the generated tags in the 'new_tags' column
                df_tags.loc[index, "new_tags"] = tags
                break
            except Exception as e:
                print(f"Error at index {index}: {e}")
                time.sleep(5)  # Wait for 5 seconds before retrying
        else:
            print(f"Skipping index {index} after 3 failed attempts")
            continue  # Skip the current index if the API call fails 3 times

    return df_tags

df_tags_tech = generate_tags_and_handle_rate_limit(df_tags)
df_tags_tech.to_csv("./data/tags_27_11.csv", index=False)
62/102:
import ast

tag_list = []

for row in df_tags_tech.itertuples():
    tags_dict_str = row.new_tags
    try:
        tags_dict = ast.literal_eval(tags_dict_str)
    except (SyntaxError, ValueError):
        print(f"Error parsing string to dict: {tags_dict_str}")
        continue

    for tag_category, tags in tags_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            tag_list.append(tag)

print(tag_list)
print(len(tag_list))
62/103:
categories = df_tags_tech["Product category"].dropna().unique()

# Split the categories that have multiple categories with comma
categories = [category.split(",") for category in categories]

# Make the list into without duplicates
categories = list(set([item for sublist in categories for item in sublist]))

categories = [category.strip() for category in categories]
62/104:
unique_categories = set(categories)

for category in unique_categories:
    print(category)
62/105:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict_temp = ast.literal_eval(tags_dict_str)
            for tags in tags_dict_temp.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
62/106:
print(tags_dict)
import json

with open('tags_dict.json', 'w') as f:
    json.dump(tags_dict, f)
62/107:
# save the dict as a excel file
df = pd.DataFrame.from_dict(tags_dict, orient="index")
df.to_excel("filter_tags.xlsx")
62/108:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict_temp = ast.literal_eval(tags_dict_str)
            for tags in tags_dict_temp.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Product type is for example Software"""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
62/109:
tags = tag_list

# Create a dictionary to store the tags
tags_dict = {}

for category in set(categories):
    # Get the products in the current category
    products = df_tags_tech[df_tags_tech["Product category"] == category]["Product_Name"]
    # Get the tags in the current category
    # Assuming 'df' is your DataFrame and 'category' is the current category
    tags_in_category = df_tags_tech[df_tags_tech['Product category'] == category]['new_tags']

    all_tags_in_category = []
    for tags_dict_str in tags_in_category:
        try:
            tags_dict_temp = ast.literal_eval(tags_dict_str)
            for tags in tags_dict_temp.values():
                all_tags_in_category.extend(tags)
        except (SyntaxError, ValueError):
            print(f"Error parsing string to dict: {tags_dict_str}")

    # Remove duplicates
    all_tags_in_category = list(set(all_tags_in_category))

    messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a dictionary where the keys are the top-level tags and the values are lists of the sub-tags."
    },
    {
        "role": "user",
        "content": f"""Given the list of sub-tags: {all_tags_in_category} and the products: {products} in category {category}: Use these top-level tags:
        ['Product Type', 'Technology', 'Application']. Product type is what type of product it is. Technology is what technology the product uses. Application is what the product is used for. The technical should be more technical than the application and prouduct type.
         For each top-level tag, select specific and descriptive sub-tags from the list of sub-tags. Use as many subtags as you think is necessary to describe the products in the category. But no more than 4. Try to have unique sub-tags for each top-level tag. 
         Use tags from the list of sub-tags only once. Do not use the same sub-tag for multiple top-level tags. Choose the sub-level tags that are most relevant for the top-level tag. Do not use Product Names as tags. Only use 4 tags per top-level tag."""
    }
    ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    category_tags = response.choices[0].message.content.strip()
    print(category_tags)

    # Save the tags in the dictionary
    tags_dict[category] = category_tags
print(tags_dict)
62/110:
print(tags_dict)
import json

with open('tags_dict.json', 'w') as f:
    json.dump(tags_dict, f)
62/111:
# save the dict as a excel file
df = pd.DataFrame.from_dict(tags_dict, orient="index")
df.to_excel("filter_tags.xlsx")
62/112:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'YourColumnName'}, inplace=True)
print(df)
62/113:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'Category'}, inplace=True)
print(df)
62/114:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'Category'}, inplace=True)
print(df.columns)
# Convert the dictionary in the second column to separate columns
df = df['YourColumnName'].apply(pd.Series)

# Join the new columns with the original DataFrame
df = pd.concat([df, df['YourColumnName']], axis=1)

# Drop the original column
df.drop(columns=['YourColumnName'], inplace=True)
print(df)
62/115:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'Category'}, inplace=True)
df.rename(columns={'0': 'value'}, inplace=True)

print(df.columns)
# Convert the dictionary in the second column to separate columns
df = df['YourColumnName'].apply(pd.Series)

# Join the new columns with the original DataFrame
df = pd.concat([df, df['YourColumnName']], axis=1)

# Drop the original column
df.drop(columns=['YourColumnName'], inplace=True)
print(df)
62/116:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'Category'}, inplace=True)
df.rename(columns={0: 'value'}, inplace=True)

print(df.columns)
# Convert the dictionary in the second column to separate columns
df = df['YourColumnName'].apply(pd.Series)

# Join the new columns with the original DataFrame
df = pd.concat([df, df['YourColumnName']], axis=1)

# Drop the original column
df.drop(columns=['YourColumnName'], inplace=True)
print(df)
62/117:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'Category'}, inplace=True)
df.rename(columns={0: 'value'}, inplace=True)

print(df.columns)
# Convert the dictionary in the second column to separate columns
df = df['YourColumnName'].apply(pd.Series)

# Join the new columns with the original DataFrame
df = pd.concat([df, df['value']], axis=1)

# Drop the original column
df.drop(columns=['value'], inplace=True)
print(df)
62/118:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'Category'}, inplace=True)
df.rename(columns={0: 'value'}, inplace=True)

print(df.columns)
# Convert the dictionary in the second column to separate columns
df = df['value'].apply(pd.Series)

# Join the new columns with the original DataFrame
df = pd.concat([df, df['value']], axis=1)

# Drop the original column
df.drop(columns=['value'], inplace=True)
print(df)
62/119:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'Category'}, inplace=True)
df.rename(columns={0: 'value'}, inplace=True)

print(df.columns)
# Convert the dictionary in the second column to separate columns
df = df['value'].apply(pd.Series)

# Join the new columns with the original DataFrame
# df = pd.concat([df, df['YourColumnName']], axis=1)

# # Drop the original column
# df.drop(columns=['YourColumnName'], inplace=True)
print(df)
62/120:
# Convert the nested dictionary to a DataFrame
df = pd.DataFrame.from_dict(tags_dict, orient='index')

# Reset the index
df.reset_index(inplace=True)

# Rename the index column
df.rename(columns={'index': 'Category'}, inplace=True)
df.rename(columns={0: 'value'}, inplace=True)

print(df.columns)
# Convert the dictionary in the second column to separate columns
df = df['value'].apply(pd.Series)

# Join the new columns with the original DataFrame
# df = pd.concat([df, df['YourColumnName']], axis=1)

# # Drop the original column
# df.drop(columns=['YourColumnName'], inplace=True)
print(df)
df.to_excel("test_fitler.xlsx")
62/121:
for category, tags in tags_dict.items():
    print(category)
    print(tags)
    print("\n")
62/122:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category
    for tag_category, tags in filters_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            new_df_tags_filter = new_df_tags_filter.append({"category": category, tag_category: tag}, ignore_index=True)
print(new_df_tags_filter)
62/123:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category
    for tag_category, tags in filters_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
            new_row = {"category": category if tag_category == "category" else None,
                       "product_type": tag if tag_category == "product_type" else None,
                       "technology": tag if tag_category == "technology" else None,
                       "application": tag if tag_category == "application" else None}
            new_df_tags_filter = new_df_tags_filter.append(new_row, ignore_index=True)
            
print(new_df_tags_filter)
62/124:
import pandas as pd

new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    filters_dict = ast.literal_eval(filters)
    for tag_category, tags in filters_dict.items():
        for tag in tags:
            new_row = {"category": category if tag_category == "category" else None,
                       "product_type": tag if tag_category == "product_type" else None,
                       "technology": tag if tag_category == "technology" else None,
                       "application": tag if tag_category == "application" else None}
            new_df_tags_filter = new_df_tags_filter.append(new_row, ignore_index=True)
            
print(new_df_tags_filter)
62/125:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category
    for tag_category, tags in filters_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
            new_row = {"category": category if tag_category == "category" else None,
                       "product_type": tag if tag_category == "product_type" else None,
                       "technology": tag if tag_category == "technology" else None,
                       "application": tag if tag_category == "application" else None}
            new_df_tags_filter = new_df_tags_filter.concat(new_row, ignore_index=True)
            
print(new_df_tags_filter)
62/126:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category
    for tag_category, tags in filters_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
            new_row = {"category": category if tag_category == "category" else None,
                       "product_type": tag if tag_category == "product_type" else None,
                       "technology": tag if tag_category == "technology" else None,
                       "application": tag if tag_category == "application" else None}
            new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/127:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category
    for tag_category, tags in filters_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            print("category", category, "tag_category", tag_category, "tag", tag )
            # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
            new_row = {"category": category if tag_category == "category" else None,
                       "product_type": tag if tag_category == "product_type" else None,
                       "technology": tag if tag_category == "technology" else None,
                       "application": tag if tag_category == "application" else None}
            new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/128:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category
    for tag_category, tags in filters_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            print("category", category, ", tag_category", tag_category, ", tag", tag )
            # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
            new_row = {"category": category if tag_category == "category" else None,
                       "product_type": tag if tag_category == "product_type" else None,
                       "technology": tag if tag_category == "technology" else None,
                       "application": tag if tag_category == "application" else None}
            new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/129:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category
    for tag_category, tags in filters_dict.items():
        # Assuming tags is a list of strings
        for tag in tags:
            print("category", category, ", tag_category", tag_category, ", tag", tags )
            # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
            new_row = {"category": category if tag_category == "category" else None,
                       "product_type": tag if tag_category == "product_type" else None,
                       "technology": tag if tag_category == "technology" else None,
                       "application": tag if tag_category == "application" else None}
            new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/130:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category
    for tag_category, tags in filters_dict.items():
        # Assuming tags is a list of strings
       
        print("category", category, ", tag_category", tag_category, ", tag", tags )
        # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
        new_row = {"category": category if tag_category == "category" else None,
                    "product_type": tag if tag_category == "product_type" else None,
                    "technology": tag if tag_category == "technology" else None,
                    "application": tag if tag_category == "application" else None}
        new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/131:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    
    print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    new_row = {"category": category, "product_type": 
                "product_type": tag if tag_category == "product_type" else None,
                "technology": tag if tag_category == "technology" else None,
                "application": tag if tag_category == "application" else None}
    new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/132:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    
    print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/133:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    if "Product Type" in filters_dict.keys():
        product_type = filters_dict["Product Type"]
    elif "technology" in filters_dict.keys():
        technology = filters_dict["technology"]
    elif "application" in filters_dict.keys():
        application = filters_dict["application"]
    
    print("product_type", product_type, ", technology", technology, ", application", application)
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/134:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    if "Product Type" in filters_dict.keys():
        product_type = filters_dict["Product Type"]
    elif "technology" in filters_dict.keys():
        technology = filters_dict["Technology"]
    elif "application" in filters_dict.keys():
        application = filters_dict["Application"]
    
    print("product_type", product_type, ", technology", technology, ", application", application)
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/135:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    if "Product Type" in filters_dict.keys():
        product_type = filters_dict["Product Type"]
    elif "Technology" in filters_dict.keys():
        technology = filters_dict["Technology"]
    elif "Application" in filters_dict.keys():
        application = filters_dict["Application"]
    
    print("product_type", product_type, ", technology", technology, ", application", application)
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/136:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    for k in filters_dict.keys():
        if "Product Type" in k:
            product_type = filters_dict["Product Type"]
        elif "Technology" in k:
            technology = filters_dict["Technology"]
        elif "Application" in k:
            application = filters_dict["Application"]
    
    print("product_type", product_type, ", technology", technology, ", application", application)
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/137:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    # print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    for k in filters_dict.keys():
        if "Product Type" in k:
            product_type = filters_dict["Product Type"]
        elif "Technology" in k:
            technology = filters_dict["Technology"]
        elif "Application" in k:
            application = filters_dict["Application"]
    
    print("product_type", product_type, ", technology", technology, ", application", application)
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/138:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    # print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    for k in filters_dict.keys():
        if "Product Type" in k:
            product_type = filters_dict["Product Type"]
        elif "Technology" in k:
            technology = filters_dict["Technology"]
        elif "Application" in k:
            application = filters_dict["Application"]
    
    print("product_type", product_type, ", technology", technology, ", application", application)
    new_row = {"category": category, "product_type": product_type, "technology": technology, "application": application}
    new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
62/139:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    # print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    for k in filters_dict.keys():
        if "Product Type" in k:
            product_type = filters_dict["Product Type"]
        elif "Technology" in k:
            technology = filters_dict["Technology"]
        elif "Application" in k:
            application = filters_dict["Application"]
    
    print("product_type", product_type, ", technology", technology, ", application", application)
    new_row = {"category": category, "product_type": product_type, "technology": technology, "application": application}
    new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
new_df_tags_filter.to_excel("filter_tags_test.xlsx")
62/140:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    # print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    for k in filters_dict.keys():
        if "Product Type" in k:
            product_type = filters_dict["Product Type"]
        elif "Technology" in k:
            technology = filters_dict["Technology"]
        elif "Application" in k:
            application = filters_dict["Application"]
    
    print("product_type", ', '.join(product_type), ", technology", ', '.join(technology), ", application", ', '.join(application))
    new_row = {"category": category, "product_type": ', '.join(product_type), "technology": ', '.join(technology), "application": ', '.join(application)}    
    new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
new_df_tags_filter.to_excel("filter_tags_test.xlsx")
62/141:
new_df_tags_filter = pd.DataFrame(columns=["category", "product_type", "technology", "application"])
for category, filters in tags_dict.items():
    # make tags into a dict
    filters_dict = ast.literal_eval(filters)
    # get the tags for each category

    
    # print("category", category, ", tag_category", filters_dict.keys(), ", tag", filters_dict.values() )
    for k in filters_dict.keys():
        if "Product Type" in k:
            product_type = filters_dict["Product Type"]
        elif "Technology" in k:
            technology = filters_dict["Technology"]
        elif "Application" in k:
            application = filters_dict["Application"]
    
    print("product_type", ', '.join(product_type), ", technology", ', '.join(technology), ", application", ', '.join(application))
    new_row = {"category": category, "product_type": ', '.join(product_type), "technology": ', '.join(technology), "application": ', '.join(application)}    
    new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!
    
    # add the category in columns category, tags in columns product_type if tag_category is product_type etc.
    # new_row = {"category": category, "product_type": 
    #             "product_type": tag if tag_category == "product_type" else None,
    #             "technology": tag if tag_category == "technology" else None,
    #             "application": tag if tag_category == "application" else None}
    # new_df_tags_filter.loc[len(new_df_tags_filter)] = new_row # only use with a RangeIndex!


            
print(new_df_tags_filter)
new_df_tags_filter.to_excel("filter_tags.xlsx")
62/142:
import openpyxl

# Load the Excel file
workbook = openpyxl.load_workbook('filter_tags.xlsx')

# Assuming you want to modify the first sheet, change the index accordingly
sheet = workbook.active

# Specify the cell containing the text (assuming it's A1 for this example)
cell = sheet['A1']

# Split the text into bullet points
text_lines = cell.value.split('\t')  # Assuming your data is tab-separated
bullet_points = '\n'.join([' ' + line.strip() for line in text_lines])

# Write the modified text back to the cell
cell.value = bullet_points

# Save the modified workbook
workbook.save('filter_tags.xlsx')
62/143:
import openpyxl

# Load the Excel file
workbook = openpyxl.load_workbook('filter_tags.xlsx')

# Assuming you want to modify the first sheet, change the index accordingly
sheet = workbook.active

# Specify the cell containing the text (assuming it's A1 for this example)
cell = sheet['product_type']

# Split the text into bullet points
text_lines = cell.value.split('\t')  # Assuming your data is tab-separated
bullet_points = '\n'.join([' ' + line.strip() for line in text_lines])

# Write the modified text back to the cell
cell.value = bullet_points

# Save the modified workbook
workbook.save('filter_tags.xlsx')
62/144:
import openpyxl

# Load the Excel file
workbook = openpyxl.load_workbook('filter_tags.xlsx')

# Assuming you want to modify the first sheet, change the index accordingly
sheet = workbook.active

# Iterate over the rows in the 'product_type' column
for row in range(1, sheet.max_row + 1):
    cell = sheet['product_type' + str(row)]
    
    # Check if cell has a value
    if cell.value:
        # Split the text into bullet points
        text_lines = cell.value.split('\t')  # Assuming your data is tab-separated
        bullet_points = '\n'.join([' ' + line.strip() for line in text_lines])

        # Write the modified text back to the cell
        cell.value = bullet_points

# Save the modified workbook
workbook.save('filter_tags.xlsx')
62/145:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
62/146:
# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        print(row["Product_Name"])
        print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/147:
# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(category_tags)
        # choose 8 tags from the tags list for each product
        # messages = [
        #     {
        #         "role": "system",
        #         "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
        #     },
        #     {
        #         "role": "user",
        #         "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
        #     }
        # ]
        # response = client.chat.completions.create(
        #     model="gpt-4-1106-preview",
        #     messages=messages,
        #     response_format={ "type": "json_object" }
        # )
        # tags = response.choices[0].message.content.strip()
        # df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        # print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/148:
# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        print(category_tags)
        # choose 8 tags from the tags list for each product
        # messages = [
        #     {
        #         "role": "system",
        #         "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
        #     },
        #     {
        #         "role": "user",
        #         "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
        #     }
        # ]
        # response = client.chat.completions.create(
        #     model="gpt-4-1106-preview",
        #     messages=messages,
        #     response_format={ "type": "json_object" }
        # )
        # tags = response.choices[0].message.content.strip()
        # df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        # print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/149:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
62/150:
# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        print(category_tags)
        # choose 8 tags from the tags list for each product
        # messages = [
        #     {
        #         "role": "system",
        #         "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
        #     },
        #     {
        #         "role": "user",
        #         "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
        #     }
        # ]
        # response = client.chat.completions.create(
        #     model="gpt-4-1106-preview",
        #     messages=messages,
        #     response_format={ "type": "json_object" }
        # )
        # tags = response.choices[0].message.content.strip()
        # df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        # print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/151:
# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        print(category_tags)
        # choose 8 tags from the tags list for each product
        # messages = [
        #     {
        #         "role": "system",
        #         "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
        #     },
        #     {
        #         "role": "user",
        #         "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
        #     }
        # ]
        # response = client.chat.completions.create(
        #     model="gpt-4-1106-preview",
        #     messages=messages,
        #     response_format={ "type": "json_object" }
        # )
        # tags = response.choices[0].message.content.strip()
        # df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        # print(tags)
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
62/152:
# Create a new column to store the tags for each product
df_tech_new_tags = df_tags_tech.copy()
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
64/1:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
64/2: import pandas as pd
64/3:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
64/4:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
65/1:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
65/2:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
65/3:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
65/4:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
65/5: print(url_text_dict)
65/6:
# Create a new column to store the tags for each product
df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv", columns=["Product_Name", "Product category", "url", "image_url", "is_software"])
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, and the text about the product: {website_text} select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/7:
# Create a new column to store the tags for each product
df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv", usecols=["Product_Name", "Product category", "url", "image_url", "is_software"])
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, and the text about the product: {website_text} select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/8:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
65/9:
# Create a new column to store the tags for each product
df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv", usecols=["Product_Name", "Product category", "url", "image_url", "is_software"])
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, and the text about the product: {website_text} select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/10:
df_tech_new_tags.to_excel("./data/tags_27_11.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
64/5:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
66/1:
df_new = pd.read_excel("data/tags_27_11.xlsx")
#rename column dic_tags to filter_tags
# df_new = df_new.rename(columns={"dict_tags": "filter_tags"})
# df_new = df_new.rename(columns={"tags": "product_tags"})
df_new.to_csv("data/tags_27_11.csv", index=False)
66/2:
import time
import pandas as pd
import numpy as np
import streamlit as st
from openai._client import OpenAI
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
66/3:
df_new = pd.read_excel("data/tags_27_11.xlsx")
#rename column dic_tags to filter_tags
# df_new = df_new.rename(columns={"dict_tags": "filter_tags"})
# df_new = df_new.rename(columns={"tags": "product_tags"})
df_new.to_csv("data/tags_27_11.csv", index=False)
66/4:
df_new = pd.read_excel("data/tags_27_11.xlsx")
#rename column dic_tags to filter_tags
# df_new = df_new.rename(columns={"dict_tags": "filter_tags"})
# df_new = df_new.rename(columns={"tags": "product_tags"})
df_new.to_csv("data/tags_27_11.csv", index=False)
64/6:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
65/11:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
65/12:
# Create a new column to store the tags for each product
df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv", usecols=["Product_Name", "Product category", "url", "image_url", "is_software"])
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"Please refer to this list of tags: {category_tags}. For the product: {row['Product_Name']}, and the text about the product: {website_text} select the tags that best describe it. Your response should be a comma-separated list of tags from the provided list. Avoid using 'tag' as a tag, and do not use 'Product Type', 'Technology', or 'Application' as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/13:
df_tech_new_tags.to_excel("./data/tags_27_11.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/14:
df_tech_new_tags.to_excel("./data/tags_27_11.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
64/7:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
65/15:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
65/16:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
66/5:
filter_tags_test = pd.read_csv("filter_tags.csv")

prompt= "Help me determine if the list of filter tags for kongsberg maritime is correct, or if I should change some of the filter tags, to better make a filter to search for products. Here is my excel file: {}".format(filter_tags_test)

response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"}, 
                    {"role": "user", "content": "{}".format(prompt)}
                ],
                max_tokens=50,
                response_format={ "type": "json_object" },
                timeout=10  # Add a timeout
            )
is_software = response.choices[0].message.content.strip()

print(is_software)
66/6:
filter_tags_test = pd.read_csv("filter_tags.csv")

prompt= "Help me determine if the list of filter tags for kongsberg maritime is correct, or if I should change some of the filter tags, to better make a filter to search for products. Here is my excel file: {}".format(filter_tags_test)

response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"}, 
                    {"role": "user", "content": "{}".format(prompt)}
                ],
                max_tokens=150,
                timeout=10  # Add a timeout
            )
is_software = response.choices[0].message.content.strip()

print(is_software)
66/7:
filter_tags_test = pd.read_csv("filter_tags.csv")

prompt= "Help me determine if the list of filter tags for kongsberg maritime is correct, or if I should change some of the filter tags, to better make a filter to search for products. Here is my excel file: {}. Please state what changes you recommend and how to fix them".format(filter_tags_test)

response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"}, 
                    {"role": "user", "content": "{}".format(prompt)}
                ],
                max_tokens=150,
                timeout=10  # Add a timeout
            )
is_software = response.choices[0].message.content.strip()

print(is_software)
66/8:
filter_tags_test = pd.read_csv("filter_tags.csv")

prompt= "Help me determine if the list of filter tags for kongsberg maritime is correct, or if I should change some of the filter tags, to better make a filter to search for products. Here is my excel file: {}. Please state what changes you recommend and how to fix them".format(filter_tags_test)

response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"}, 
                    {"role": "user", "content": "{}".format(prompt)}
                ],
                max_tokens=1500,
                timeout=10  # Add a timeout
            )
is_software = response.choices[0].message.content.strip()

print(is_software)
65/17:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save it as json
import json
with open('filter_tags.json', 'w') as f:
    json.dump(tags_dict, f)
65/18:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save it as json
import json
with open('filter_tags.json', 'w') as f:
    json.dump(new_df, f)
65/19:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save dataframe as json file
new_df.to_json("filter_tags.json", orient="records")
65/20:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save dataframe as json file


#dump it as a json file
with open('filter_tags.json', 'w') as f:
    f.write(new_df.to_json(orient='records', lines=True))
66/9:
filter_tags_test = pd.read_csv("filter_tags.csv")

prompt= "Help me determine if the list of filter tags for kongsberg maritime is correct, or if I should change some of the filter tags, to better make a filter to search for products. Here is my excel file: {}. Please state what changes you recommend and how to fix them".format(filter_tags_test)

response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"}, 
                    {"role": "user", "content": "{}".format(prompt)}
                ],
                max_tokens=1500,
                timeout=10  # Add a timeout
            )
is_software = response.choices[0].message.content.strip()

print(is_software)
66/10:
filter_tags_test = pd.read_csv("filter_tags.csv")

prompt= "Help me determine if the list of filter tags for kongsberg maritime is correct, or if I should change some of the filter tags, to better make a filter to search for products. Here is my excel file: {}. Please state what changes you recommend and how to fix them".format(filter_tags_test)

response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"}, 
                    {"role": "user", "content": "{}".format(prompt)}
                ],
                max_tokens=1500,
                timeout=10  # Add a timeout
            )
is_software = response.choices[0].message.content.strip()

print(is_software)
66/11:
filter_tags_test = pd.read_csv("filter_tags.csv")

prompt= "Help me determine if the list of filter tags for kongsberg maritime is correct, or if I should change some of the filter tags, to better make a filter to search for products. Here is my excel file: {}. Please state what changes you recommend and how to fix them".format(filter_tags_test)

response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"}, 
                    {"role": "user", "content": "{}".format(prompt)}
                ],
                max_tokens=1500,
            )
is_software = response.choices[0].message.content.strip()

print(is_software)
65/21:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save dataframe as json file
65/22:
# Create a new column to store the tags for each product
df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv", usecols=["Product_Name", "Product category", "url", "image_url", "is_software"])
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/23:
# Create a new column to store the tags for each product
df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv", usecols=["Product_Name", "Product category", "url", "image_url", "is_software"])
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be in JSON format and consist of a comma-separated list of tags."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/24:
df_tech_new_tags.to_excel("./data/tags_27_11.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/25:
import ast
def extract_tags(tags_str):
    tags_obj = ast.literal_eval(tags_str)
    if isinstance(tags_obj, dict):
        tags_values = tags_obj.values()
    elif isinstance(tags_obj, tuple):
        tags_values = tags_obj
    else:
        raise ValueError(f"Unexpected type {type(tags_obj)}: {tags_obj}")
    
    tags_list = []
    for value in tags_values:
        if isinstance(value, list):
            tags_list.append(', '.join(map(str.strip, value)))
        elif isinstance(value, dict):
            tags_list.append(str(value))
        else:
            tags_list.append(value.strip())
    return ', '.join(tags_list)
65/26:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
    
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
65/27:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save dataframe as json file
66/12:
df_new = pd.read_excel("data/tags_27_11.xlsx")
#rename column dic_tags to filter_tags
# df_new = df_new.rename(columns={"dict_tags": "filter_tags"})
# df_new = df_new.rename(columns={"tags": "product_tags"})
df_new.to_csv("data/tags_27_11.csv", index=False)
66/13:
import time
import pandas as pd
import numpy as np
import streamlit as st
from openai._client import OpenAI
import json

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
65/28:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
65/29:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save dataframe as json file
65/30: print(new_df.columns)
65/31: print(new_df["category"])
65/32:
# Create a new column to store the tags for each product
df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv", usecols=["Product_Name", "Product category", "url", "image_url", "is_software"])
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/33:
# Create a new column to store the tags for each product
df_tech_new_tags = pd.read_csv("./data/tags_27_11.csv", usecols=["Product_Name", "Product category", "url", "image_url", "is_software"])
tags_df = pd.read_csv("filter_tags.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        # choose 8 tags from the tags list for each product
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/34:
df_tech_new_tags.to_excel("./data/tags_27_11.xlsx", index=False)
df_tech_new_tags.to_csv("./data/tags_27_11.csv", index=False)
65/35:
import ast
def extract_tags(tags_str):
    tags_obj = ast.literal_eval(tags_str)
    if isinstance(tags_obj, dict):
        tags_values = tags_obj.values()
    elif isinstance(tags_obj, tuple):
        tags_values = tags_obj
    else:
        raise ValueError(f"Unexpected type {type(tags_obj)}: {tags_obj}")
    
    tags_list = []
    for value in tags_values:
        if isinstance(value, list):
            tags_list.append(', '.join(map(str.strip, value)))
        elif isinstance(value, dict):
            tags_list.append(str(value))
        else:
            tags_list.append(value.strip())
    return ', '.join(tags_list)
65/36:
df_with_many_tags = pd.read_csv("data/tags_27_11.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
    
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/df_with_many_tags_27_11.xlsx", index=False)
df_with_many_tags.to_csv("data/df_with_many_tags_27_11.csv", index=False)
64/8:
df_30_11 = pd.read_excel("data/tags_30_11.xlsx")
df_30_11.to_csv("data/tags_30_11.csv", index=False)
64/9:
df_tags_splitted = df_30_11.assign(
    **{"Product category": df_30_11["Product category"].str.split(",")}
).explode("Product category")
64/10:
print(len(df_tags_splitted))
print(len(df_30_11))
64/11:
df_tags_splitted = df_30_11.assign(
    **{"Product category": df_30_11["Product category"].str.split(",")}
).explode("Product category")
df_tags_splitted.to_excel("data/df_tags_splitted_30_11.xlsx", index=False)
df_tags_splitted.to_csv("data/df_tags_splitted_30_11.csv", index=False)
64/12:
print(len(df_tags_splitted))
print(len(df_30_11))
65/37:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save dataframe as json file
65/38:
new_df = pd.read_excel("filter_tags.xlsx")
new_df.to_csv("filter_tags.csv", index=False)
#save dataframe as json file

# order the dataframe to have alfpabetical order on category
new_df = new_df.sort_values(by=['category'])
65/39: print(new_df["category"])
65/40:
new_df = pd.read_excel("filter_tags.xlsx")
new_df = new_df.sort_values(by=['category'])
new_df.to_csv("filter_tags.csv", index=False)
new_df.to_excel("filter_tags.xlsx", index=False)
#save dataframe as json file

# order the dataframe to have alfpabetical order on category
65/41: print(new_df["category"])
65/42: print(new_df["technology"])
65/43:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file

# sort the column technology by alphabet
test_df = new_df.sort_values(by=["technology"], ascending=True)
65/44:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file

# sort the column technology by alphabet
test_df = new_df.sort_values(by=["technology"], ascending=True)
65/45:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file

# sort the column technology by alphabet
test_df = new_df.sort_values(by=["technology"], ascending=True)
65/46: print(test_df["technology"])
65/47: print(test_df["category"])
65/48: print(new_df["category"])
65/49:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file

# Sort each cell in the dataframe alphabetically
test_df = new_df.apply(lambda x: x.str.strip().str.lower().str.split(',').apply(sorted).str.join(','))
65/50: print(test_df["category"])
65/51:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file

# Sort each cell in the dataframe alphabetically
test_df = new_df.apply(lambda x: x.str.strip().str.split(',').apply(sorted).str.join(','))
65/52: print(test_df["category"])
65/53: print(test_df["technology"][0])
65/54: print(test_df["technology"][0])
65/55:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ','.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)
65/56: print(test_df["technology"][0])
65/57: print(test_df["technology"][0])
65/58: print(test_df["technology"][0])
65/59:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ','.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)

print(test_df["technology"][0])
65/60:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ','.join(sorted([i.strip().title() for i in x.split(',')])) if isinstance(x, str) else x)
print(test_df["technology"][0])
65/61:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ','.join(sorted([i.strip().title() for i in x.split(',')])) if isinstance(x, str) else x)
print(test_df["technology"][0])
65/62: print(test_df["technology"][0])
65/63:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ','.join(sorted([' '.join([word[0].upper() + word[1:] for word in i.split()]) for i in x.split(',')])) if isinstance(x, str) else x)
print(test_df["technology"][0])
65/64:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ','.join(sorted([' '.join([word[0].upper() + word[1:] for word in i.split()]) for i in x.split(',')])) if isinstance(x, str) else x)
print(test_df["technology"][0])
65/65: print(test_df["technology"][0])
65/66: print(test_df["technology"][1])
65/67: print(test_df["technology"][1])
65/68:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ','.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)

print(test_df["technology"][0])
65/69: print(test_df["technology"][1])
65/70: print(test_df["technology"][1])
65/71:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ','.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)

print(test_df["technology"][1])
65/72: print(test_df["technology"][0])
65/73:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ', '.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)
print(test_df["technology"][1])
65/74:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ', '.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)
print(test_df["technology"][1])
65/75: print(test_df["technology"][0])
65/76: print(test_df["technology"][1])
65/77: print(test_df["technology"][1])
65/78:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ', '.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)
print(test_df["technology"][1])
65/79: print(test_df["technology"][1])
65/80:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

#save dataframe as json file
test_df = new_df.apply(lambda x: ', '.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)
print(test_df["technology"][1])
65/81:
new_df = pd.read_excel("filter_tags.xlsx")

new_df.to_csv("filter_tags.csv", index=False)

# #save dataframe as json file
# test_df = new_df.apply(lambda x: ', '.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)
# print(test_df["technology"][1])
65/82:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][1])
65/83:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][1])
65/84:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/85:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/86:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/87:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/88:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/89:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/90:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip()for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/91:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag if tag.isupper() else tag.strip().title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/92:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag if tag.isupper() else tag.strip().title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/93:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag if any(char.isupper() for char in tag) else tag.strip().title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/94:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag if any(char.isupper() for char in tag) else tag.strip().title() for tag in tags_list]
        # Sort the list of tags
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/95:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() if not any(char.isupper() for char in tag) else tag for tag in tags_list]
        # Sort the list of tags, keeping the original order for tags that contain acronyms
        tags_list.sort(key=lambda tag: (not any(char.isupper() for char in tag), tag))
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/96:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() if not any(char.isupper() for char in tag) else tag for tag in tags_list]
        # Sort the list of tags, keeping the original order for tags that contain acronyms
        tags_list.sort(key=lambda tag: (not any(char.isupper() for char in tag), tag))
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/97:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() if not any(char.isupper() for char in tag) else tag for tag in tags_list]
        # Sort the list of tags, ignoring case
        tags_list.sort(key=lambda tag: tag.lower())
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/98:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() if not any(char.isupper() for char in tag) else tag for tag in tags_list]
        # Sort the list of tags, ignoring case
        tags_list.sort(key=lambda tag: tag.lower())
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/99:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        tags_list = tags.split(',')
        # Strip leading/trailing spaces and capitalize first letter of each tag
        tags_list = [tag.strip().title() if not any(char.isupper() for char in tag) else tag for tag in tags_list]
        # Sort the list of tags, ignoring case
        tags_list.sort()
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/100:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Sort the list of lowercase tags
        lowercase_tags_list.sort()
        # Create a mapping from lowercase tags to original tags
        tag_mapping = dict(zip(lowercase_tags_list, original_tags_list))
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag_mapping[tag] for tag in lowercase_tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/101:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Sort the list of lowercase tags
        lowercase_tags_list.sort()
        # Create a mapping from lowercase tags to original tags
        tag_mapping = dict(zip(lowercase_tags_list, original_tags_list))
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag_mapping[tag] for tag in lowercase_tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/102:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1] for tag in sorted_tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/103:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1] for tag in sorted_tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/104:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1] for tag in sorted_tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][6])
65/105:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1] for tag in sorted_tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/106:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1] for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag
        tags_list = [tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/107:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1] for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/108:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1] for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/109:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.applymap(sort_tags)

print(test_df["technology"][2])
65/110:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][2])
65/111:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
new_df = new_df.map(sort_tags)

new_df.to_csv("filter_tags.csv", index=False)
new_df.to_excel("filter_tags.xlsx", index=False)
65/112:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][1])
65/113:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df["technology"][0])
65/114:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
test_df = new_df.map(sort_tags)

print(test_df.head(10))
65/115:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
new_df = new_df.map(sort_tags)

new_df.to_csv("filter_tags.csv", index=False)
new_df.to_excel("filter_tags.xlsx", index=False)
65/116:
df_tags = pd.read_excel("data/df_30_11.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
65/117:
df_tags = pd.read_excel("data/tags_30_11.xlsx", usecols=["Product_Name","Product category", "url", "image_url", "is_software"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
65/118:
df_tags = pd.read_excel("data/tags_30_11.xlsx", usecols=["Product_Name","Product category", "url", "image_url"])
print(len(df_tags))
# Split the 'Product category' column into multiple rows
df_tags = df_tags.assign(
    **{"Product category": df_tags["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df_tags["Product category"] = df_tags["Product category"].str.strip()


print(len(df_tags))
65/119:
print(df_tags["url"][3])
print(df_tags["Product category"][3])
print(df_tags["Product_Name"][3])
65/120:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
65/121: print(url_text_dict)
65/122:
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
65/123:
new_df = pd.read_excel("data/filter_tags - RAO.xlsx")

new_df.to_csv("data/filter_tags - RAO.csv", index=False)

# #save dataframe as json file
# test_df = new_df.apply(lambda x: ', '.join(sorted([i.strip() for i in x.split(',')])) if isinstance(x, str) else x)
# print(test_df["technology"][1])
65/124:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
new_df = new_df.map(sort_tags)

new_df.to_csv("filter_tags.csv", index=False)
new_df.to_excel("filter_tags.xlsx", index=False)
65/125:
def sort_tags(tags):
    if isinstance(tags, str):
        # Split the string into a list of tags
        original_tags_list = tags.split(',')
        # Strip leading/trailing spaces and convert all tags to lowercase
        lowercase_tags_list = [tag.strip().lower() for tag in original_tags_list]
        # Create a mapping from lowercase tags to original tags
        tag_mapping = list(zip(lowercase_tags_list, original_tags_list))
        # Sort the list of lowercase tags
        sorted_tags_list = sorted(tag_mapping)
        # Convert the sorted lowercase tags back to the original case
        tags_list = [tag[1].strip() for tag in sorted_tags_list]
        # Capitalize the first letter of each word in a tag if the tag does not contain any uppercase characters
        tags_list = [tag if any(char.isupper() for char in tag) else tag.title() for tag in tags_list]
        # Join the sorted tags back into a string
        tags = ', '.join(tags_list)
    return tags

# Apply the function to each cell in the DataFrame
new_df = new_df.map(sort_tags)

new_df.to_csv("data/filter_tags - RAO.csv", index=False)
new_df.to_excel("data/filter_tags - RAO.xlsx", index=False)
65/126:
df_filter_tags_1_12 = pd.read_excel("data/filter_tags.xlsx")
df_filter_tags_1_12.to_csv("filter_tags.csv", index=False)
65/127:
df_filter_tags_1_12 = pd.read_excel("filter_tags.xlsx")
df_filter_tags_1_12.to_csv("filter_tags.csv", index=False)
65/128:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df_tags
tags_df = pd.read_csv("data/filter_tags - RAO.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_runar_01_12.csv", index=False)
65/129:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df_tags
tags_df = pd.read_csv("data/filter_tags - RAO.csv")

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_runar_01_12.csv", index=False)
65/130:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df_tags
tags_df = pd.read_csv("data/filter_tags - RAO.csv", usecols=["category", "product_type", "technology", "application"])

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_runar_01_12.csv", index=False)
65/131:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df_tags
tags_df = pd.read_csv("data/filter_tags - RAO.csv", usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")

        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_runar_01_12.csv", index=False)
65/132:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df_tags
tags_df = pd.read_csv("data/filter_tags - RAO.csv", usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")
        print(row["Product category"])
        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_runar_01_12.csv", index=False)
65/133: print(df_tags.head(10))
65/134: print(df_tags.head(20))
65/135:
#reset index
df_tags_new_index = df_tags.reset_index(drop=True)
print(df_tags_new_index.head(20))
65/136:
#reset index
df_tags = df_tags.reset_index(drop=True)
print(df_tags.head(20))
65/137:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df_tags
tags_df = pd.read_csv("data/filter_tags - RAO.csv", usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of tags_df for easier lookup
tags_df.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in tags_df.index:
        category_tags = tags_df.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")
        print(row["Product category"])
        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in tags_df")

# save the dataframe as csv
df_tech_new_tags.to_csv("./data/tags_runar_01_12.csv", index=False)
65/138:
df_with_many_tags = pd.read_csv("data/tags_runar_01_12_splitted.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
    
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/tags_runar_01_12_grouped.xlsx", index=False)
df_with_many_tags.to_csv("data/tags_runar_01_12_grouped.csv", index=False)
65/139:
df_with_many_tags = pd.read_csv("./data/tags_runar_01_12_splitted.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
    
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/tags_runar_01_12_grouped.xlsx", index=False)
df_with_many_tags.to_csv("data/tags_runar_01_12_grouped.csv", index=False)
65/140:
df_runar = pd.read_csv("data/tags_runar_01_12.csv")
df_runar.to_csv("data/tags_runar_01_12_splitted.csv", index=False)
df_runar.to_excel("data/tags_runar_01_12_splitted.xlsx", index=False)
65/141:
df_with_many_tags = pd.read_csv("data/tags_runar_01_12_splitted.csv")
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
    
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel("data/tags_runar_01_12_grouped.xlsx", index=False)
df_with_many_tags.to_csv("data/tags_runar_01_12_grouped.csv", index=False)
67/1:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json
67/2:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json

from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
67/3:
# let todays data save to a file_name 
today = time.strftime("%Y-%m-%d")
print(today)
67/4:
# let todays data save to a file_name 
today = time.strftime("%m-%d")
print(today)
67/5:
# let todays data save to a file_name 
today = time.strftime("%m_%d")
print(today)
67/6:
# let todays data save to a file_name 
# add hour 

today = time.strftime("%m_%d_%H")
print(today)
67/7:
# let todays data save to a file_name 
# add hour 

today = time.strftime("%m_%d_%H:%M")
print(today)
67/8:
file_path_original_dataframe = "data/tags_30_11.xlsx"
file_path_filter_dataframe = "filter_tags.xlsx"

new_file_path_splitted_dataframe_excel = "data/splitted_tags_" + today + ".xlsx"
new_file_path_splitted_dataframe_csv = "data/splitted_tags_" + today + ".csv"

new_file_path_grouped_dataframe_excel = "data/grouped_tags_" + today + ".xlsx"
new_file_path_grouped_dataframe_csv = "data/grouped_tags_" + today + ".csv"
67/9:

today = time.strftime("%m_%d_%H:%M")

# Make a folder for today's data
if not os.path.exists(today):
    os.makedirs(today)

# save the csv data in todays folder if it exists
67/10:

today = time.strftime("%m_%d_%H:%M")

# Make a folder for today's data inside the data folder
if not os.path.exists("data/" + today):
    os.mkdir("data/" + today)

# save the csv data in todays folder if it exists
67/11:

today = time.strftime("%m_%d")
today_hour = time.strftime("%m_%d_%H:%M")

# Make a folder for today's data inside the data folder
if not os.path.exists("data/" + today):
    os.mkdir("data/" + today)

# save the csv data in todays folder if it exists
67/12:

file_path_original_dataframe = "data/tags_30_11.xlsx"
file_path_filter_dataframe = "filter_tags.xlsx"

new_file_path_splitted_dataframe_excel = "data/"+today+"/splitted_tags_" + today_hour + ".xlsx"
new_file_path_splitted_dataframe_csv = "data/"+today+"/splitted_tags_" + today_hour + ".csv"

new_file_path_grouped_dataframe_excel = "data/"+today+"/grouped_tags_" + today_hour + ".xlsx"
new_file_path_grouped_dataframe_csv = "data/"+today+"/grouped_tags_" + today_hour + ".csv"
67/13:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json

from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
67/14:

today = time.strftime("%m_%d")
today_hour = time.strftime("%m_%d_%H:%M")

# Make a folder for today's data inside the data folder
if not os.path.exists("data/" + today):
    os.mkdir("data/" + today)
67/15:

file_path_original_dataframe_csv = "data/tags_30_11.csv"
file_path_original_dataframe_excel = "data/tags_30_11.xlsx"

file_path_filter_dataframe_csv = "filter_tags.csv"
file_path_filter_dataframe_excel = "filter_tags.xlsx"

new_file_path_splitted_dataframe_excel = "data/"+today+"/splitted_tags_" + today_hour + ".xlsx"
new_file_path_splitted_dataframe_csv = "data/"+today+"/splitted_tags_" + today_hour + ".csv"

new_file_path_grouped_dataframe_excel = "data/"+today+"/grouped_tags_" + today_hour + ".xlsx"
new_file_path_grouped_dataframe_csv = "data/"+today+"/grouped_tags_" + today_hour + ".csv"
67/16:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
67/17:
df = pd.read_excel(file_path_original_dataframe_excel, usecols=["Product_Name","Product category", "url", "image_url"])
print(len(df))

# Split the 'Product category' column into multiple rows
df = df.assign(
    **{"Product category": df["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df["Product category"] = df["Product category"].str.strip()


print(len(df))
67/18:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of filter_dataframe for easier lookup
filter_dataframe.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in filter_dataframe.index:
        category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")
        print(row["Product category"])
        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/19:
import ast
def extract_tags(tags_str):
    tags_obj = ast.literal_eval(tags_str)
    if isinstance(tags_obj, dict):
        tags_values = tags_obj.values()
    elif isinstance(tags_obj, tuple):
        tags_values = tags_obj
    else:
        raise ValueError(f"Unexpected type {type(tags_obj)}: {tags_obj}")
    
    tags_list = []
    for value in tags_values:
        if isinstance(value, list):
            tags_list.append(', '.join(map(str.strip, value)))
        elif isinstance(value, dict):
            tags_list.append(str(value))
        else:
            tags_list.append(value.strip())
    return ', '.join(tags_list)
67/20:
df_with_many_tags = pd.read_csv(new_file_path_splitted_dataframe_csv)
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
    
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel(new_file_path_grouped_dataframe_excel, index=False)
df_with_many_tags.to_csv(new_file_path_grouped_dataframe_csv, index=False)
67/21:
# if dataframe do not have column "is_software" then merge the column from software dataframe to the dataframe on the product name column
df_grouped_tags = pd.read_csv(new_file_path_grouped_dataframe_csv)
#check if the column is_software exists
if "is_software" not in df_grouped_tags.columns:
    # read the software dataframe
    df_software = pd.read_csv("data/tags_27_11.csv")
    # merge the column is_software to the dataframe on the product name column
    df_grouped_tags = pd.merge(df_grouped_tags, df_software[["Product_Name", "is_software"]], on="Product_Name", how="left")
    # save the dataframe
    print(df_grouped_tags.head())
65/142:
df = pd.read_excel("data/filter_tags.xlsx")
df.to_csv("data/filter_tags.csv", index=False)
65/143:
df = pd.read_excel("filter_tags.xlsx")
df.to_csv("data/filter_tags.csv", index=False)
65/144:
df = pd.read_excel("filter_tags.xlsx")
df.to_csv("filter_tags.csv", index=False)
65/145:
df = pd.read_excel("filter_tags.xlsx")
df.to_csv("filter_tags.csv", index=False)
65/146:
df = pd.read_excel("filter_tags.xlsx")
df.to_csv("filter_tags.csv", index=False)
65/147:
df = pd.read_excel("filter_tags.xlsx")
df.to_csv("filter_tags.csv", index=False)
   1:
import requests
from bs4 import BeautifulSoup
   2:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string +" \n "+ bullet_points_string
    return text
url_text_dict = {}
   3: extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
   4: extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
   5: extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
   6: extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
   7: extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
   8:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)

    link = soup.find('a', href='#technicalInformation')

    bullet_points = []
    bullet_points_string = ""
    # If the link was found, find the element it links to
    if link is not None:
        target_id = link['href'].lstrip('#')
        target_element = soup.find(id=target_id)

        # If the target element was found, get its text
        if target_element is not None:
            # Find all the list items in the target element
            list_items = target_element.find_all('li')

            # Extract the text of each list item
            bullet_points = [li.get_text(strip=True) for li in list_items]
            bullet_points_string = '\n'.join(bullet_points)

        
    text = rich_text_string
    return text
url_text_dict = {}
   9: extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
  10:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element and stop when encountering a heading
    rich_text = []
    for element in elements:
        if element.find_next(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            break
        rich_text.append(element.get_text(strip=True))
    rich_text_string = ' '.join(rich_text)
        
    text = rich_text_string
    return text

url_text_dict = {}
  11: extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
  12:
def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find all elements with the specified class
    elements = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each element
    rich_text = [element.get_text(strip=True) for element in elements]
    rich_text_string = ' '.join(rich_text)
        
    text = rich_text_string
    return text
url_text_dict = {}
  13: extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
  14:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name in ['h2', 'h3', 'ul']:
            break
        else:
            text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string

print(extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/"))
  15:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name in ['ul']:
            break
        else:
            text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string

print(extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/"))
  16:
import requests
from bs4 import BeautifulSoup
import pandas as pd
  17:
df = pd.read_csv("data/tags_30_11.csv")

print(extract_text(df["url"][0]))
  18:
df = pd.read_csv("data/tags_30_11.csv")

print(extract_text(df["url"][0]))
print(df["url"][0])
  19:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    img_url = None
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name == 'img':
            img_url = child['src']
        elif child.name == 'ul':
            break
        else:
            text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string, img_url

text, img_url = extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
print(text)
print(img_url)
  20:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    img_url = None
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name == 'picture':
            img_url = child['src']
        elif child.name == 'ul':
            break
        else:
            text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string, img_url

text, img_url = extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
print(text)
print(img_url)
  21:
df = pd.read_csv("data/tags_30_11.csv")

print(extract_text(df["url"][0]))
print(df["url"][0])
  22:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    img_url = None
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name == 'picture':
            img = child.find('img')
            if img:
                img_url = img['src']
        elif child.name == 'ul':
            break
        else:
            text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string, img_url

text, img_url = extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
print(text)
print(img_url)
  23:
df = pd.read_csv("data/tags_30_11.csv")

print(extract_text(df["url"][0]))
print(df["url"][0])
  24:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    img_url = None
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name == 'picture':
            img = child.find('img')
            if img:
                img_url = img['src']
        elif child.name in ['h2', 'h3', 'ul']:
            break
        else:
            text.append(child.get_text(strip=True))
            # Check for picture tag within other tags
            picture = child.find('picture')
            if picture:
                img = picture.find('img')
                if img and not img_url:  # Only set img_url if it hasn't been set yet
                    img_url = img['src']
    text_string = ' '.join(text)
        
    return text_string, img_url

text, img_url = extract_text(df["url"][0])
print(text)
print(img_url)
  25:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    img_url = None
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name == 'picture':
            img = child.find('img')
            if img:
                img_url = img['src']
        elif child.name in ['h2', 'h3', 'ul']:
            break
        else:
            text.append(child.get_text(strip=True))
            # Check for picture tag within other tags
            picture = child.find('picture')
            if picture:
                img = picture.find('img')
                if img and not img_url:  # Only set img_url if it hasn't been set yet
                    img_url = img['src']
    text_string = ' '.join(text)
        
    return text_string, img_url

text, img_url = extract_text(df["url"][0])
print(text)
print(img_url)
  26:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    img_url = None
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name == 'picture':
            img = child.find('img')
            if img:
                img_url = img['src']
        elif child.name in ['h2', 'h3', 'ul']:
            break
        else:
            text.append(child.get_text(strip=True))
            # Check for picture tag within other tags
            picture = child.find('picture')
            if picture:
                img = picture.find('img')
                if img and not img_url:  # Only set img_url if it hasn't been set yet
                    img_url = img['src']
    text_string = ' '.join(text)
        
    return text_string, img_url

text, img_url = extract_text(df["url"][0])
print(text)
print(img_url)
  27:
df = pd.read_csv("data/tags_30_11.csv")

print(extract_text(df["url"][0]))
print(df["url"][0])
  28:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name in ['ul']:
            break
        else:
            text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string

print(extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/"))
  29:
df = pd.read_csv("data/tags_30_11.csv")

print(extract_text(df["url"][0]))
print(df["url"][0])
  30:
df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(extract_text(df["url"][number]))
print(df["url"][number])
  31:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a ul tag is encountered
    text = []
    img_url = None
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name == 'picture':
            img = child.find('img')
            if img:
                img_url = img['src']
        elif child.name == 'ul':
            break
        else:
            text.append(str(child))  # Append the string representation of the child, including HTML tags
    text_string = ''.join(text)
        
    return text_string, img_url

text, img_url = extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/")
print(text)
print(img_url)
  32:
df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(extract_text(df["url"][number]))
print(df["url"][number])
  33:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h3 tag is encountered
    text = []
    img_url = None
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name == 'picture':
                img = child.find('img')
                if img:
                    img_url = img['src']
            elif child.name == 'h3':
                break
            else:
                text.append(str(child))  # Append the string representation of the child, including HTML tags
    text_string = ''.join(text)
        
    return text_string, img_url

df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(extract_text(df["url"][number]))
print(df["url"][number])
  34:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h3 tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name == 'h3':
                break
            elif child.name != 'picture':  # Exclude the picture tag
                text.append(str(child))  # Append the string representation of the child, including HTML tags
    text_string = ''.join(text)
        
    return text_string

df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(extract_text(df["url"][number]))
print(df["url"][number])
  35:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h3 tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child):
                text.append(child.strip())
            elif child.name == 'ul':
                break
            elif child.name != 'picture':  # Exclude the picture tag
                text.append(str(child))  # Append the string representation of the child, including HTML tags
    text_string = ''.join(text)
        
    return text_string

df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(extract_text(df["url"][number]))
print(df["url"][number])
  36:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    div = soup.find(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    for child in div.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name in ['ul']:
            break
        else:
            text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string

print(extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/"))
  37:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the specified class
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered
    text = []
    for child in divs.children:
        if isinstance(child, NavigableString):
            text.append(child.strip())
        elif child.name in ['ul']:
            break
        else:
            text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string

print(extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/"))
  38:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string

print(extract_text("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/"))
  39:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(extract_text(df["url"][number]))
print(df["url"][number])
  40:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        children = list(div.children)
        for i in range(len(children)):
            child = children[i]
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            elif i < len(children) - 1 and children[i + 1].name == 'ul':
                continue  # Skip the heading before the ul tag
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string

df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(extract_text(df["url"][number]))
print(df["url"][number])
  41:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(extract_text(df["url"][number]))
print(df["url"][number])
  42:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 2

print(extract_text(df["url"][number]))
print(df["url"][number])
  43:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 3

print(extract_text(df["url"][number]))

print(df["url"][number])
  44:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 4

print(extract_text(df["url"][number]))

print(df["url"][number])
  45:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 5

print(extract_text(df["url"][number]))

print(df["url"][number])
  46:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 6

print(extract_text(df["url"][number]))

print(df["url"][number])
  47:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 2

print(extract_text(df["url"][number]))

print(df["url"][number])
  48:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 3

print(extract_text(df["url"][number]))

print(df["url"][number])
  49:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 4

print(extract_text(df["url"][number]))

print(df["url"][number])
  50:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 5

print(extract_text(df["url"][number]))

print(df["url"][number])
  51:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 8

print(extract_text(df["url"][number]))

print(df["url"][number])
  52:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 21

print(extract_text(df["url"][number]))

print(df["url"][number])
  53:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 22

print(extract_text(df["url"][number]))

print(df["url"][number])
  54:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_=["RichtextArea ProductPage__richtext text-wrapper", "layout--2col wrapper"])

    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered
    text = []
    for div in divs:
        for child in div.children:
            if isinstance(child, NavigableString):
                text.append(child.strip())
            elif child.name in ['ul']:
                break
            else:
                text.append(child.get_text(strip=True))
    text_string = ' '.join(text)
        
    return text_string
df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(extract_text(df["url"][number]))

print(df["url"][number])
  55:
from bs4 import BeautifulSoup
import requests

def find_datasheet(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the span with the specified class and text
    datasheet_span = soup.find('span', class_="Downloads__itemName", text="Datasheet")

    # If the span is found, return the parent link's href attribute
    if datasheet_span:
        datasheet_link = datasheet_span.parent.get('href')
        return datasheet_link

    # If the span is not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(find_datasheet(df["url"][number]))
print(df["url"][number])
  56:
from bs4 import BeautifulSoup
import requests

def find_datasheet(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the span with the text "Datasheet" within this div
    if downloads_div:
        datasheet_span = downloads_div.find('span', class_="Downloads__itemName", text="Datasheet")

        # If the span is found, return the parent link's href attribute
        if datasheet_span:
            datasheet_link = datasheet_span.parent.get('href')
            return datasheet_link

    # If the div or the span is not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(find_datasheet(df["url"][number]))
print(df["url"][number])
  57:
from bs4 import BeautifulSoup
import requests

def find_datasheet(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the span with the text "Datasheet" within this div
    if downloads_div:
        datasheet_span = downloads_div.find('span', class_="Downloads__itemName", text="Datasheet")

        # If the span is found, return the parent link's href attribute
        if datasheet_span:
            datasheet_link = datasheet_span.parent.get('href')
            return datasheet_link

    # If the div or the span is not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 2

print(find_datasheet(df["url"][number]))
print(df["url"][number])
  58:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find all links within this div
    if downloads_div:
        datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

        # If the links are found, return their href attributes
        if datasheet_links:
            return [link.get('href') for link in datasheet_links]

    # If the div or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 2

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  59:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find all links within this div
    if downloads_div:
        datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

        # If the links are found, return their href attributes
        if datasheet_links:
            return [link.get('href') for link in datasheet_links]

    # If the div or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 1

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  60:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find all links within this div
    if downloads_div:
        datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

        # If the links are found, return their href attributes
        if datasheet_links:
            return [link.get('href') for link in datasheet_links]

    # If the div or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 3

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  61:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheets" within this div
    if downloads_div:
        datasheets_section = downloads_div.find('h3', class_="Section__subtitle", text="Data sheets").find_next_sibling()

        # If the section is found, find all links within this section
        if datasheets_section:
            datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

            # If the links are found, return their href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links]

    # If the div, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 3

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  62:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string="Data sheets")

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 3

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  63:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string="Data sheet")

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 3

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  64:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string="Data sheet")

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 4

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  65:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string="Data sheet")

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 5

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  66:
from bs4 import BeautifulSoup
import requests

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string="Data sheet")

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 6

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  67:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 7

print(find_datasheets(df["url"][number]))
  68:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 5

print(find_datasheets(df["url"][number]))
  69:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 10

print(find_datasheets(df["url"][number]))
  70:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 11

print(find_datasheets(df["url"][number]))
  71:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 12

print(find_datasheets(df["url"][number]))
  72:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 13

print(find_datasheets(df["url"][number]))
  73:
import requests
from bs4 import BeautifulSoup
import pandas as pd 
import re
  74:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 13

print(find_datasheets(df["url"][number]))
  75:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 14

print(find_datasheets(df["url"][number]))
  76:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 15

print(find_datasheets(df["url"][number]))
  77:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return their href attributes
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 15

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  78:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 17

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  79:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 15

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  80:
df = pd.read_csv("data/tags_30_11.csv")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)
  81: df.to_csv("data/data_sheets.csv", index=False)
  82:
# only keep column url, product name and data sheets
df = df[['url', 'product_name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
  83:
# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
  84:
# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
  85:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 15

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  86:
df = pd.read_csv("data/tags_30_11.csv")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)
  87:
# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
  88:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 15

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  89:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 20

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  90:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 19

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  91:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links with the name "Datasheet" within the div
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink", string="Datasheet")

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 19

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  92:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links with the name "Datasheet" within the div
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink", string="Datasheet")

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 24

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  93:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links with the name "Datasheet" within the div
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink", string="Datasheet")

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 25

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  94:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links with the name "Datasheet" within the div
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink", string="Datasheet")

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  95:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the preceding sibling has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink", string="Datasheet")

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  96:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  97:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  98:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
  99:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, return the list of href attributes
                if datasheet_links:
                    return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 100:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
            print(datasheet_links)

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
            if datasheet_links:
                return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the div, the subtitle, the section, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 101:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 102:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
            print(datasheet_links)
            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 103:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            downloads_section = soup.find('h2', class_="Section__title", string="Downloads")

            # If the section is found, find the next sibling section
            if downloads_section:
                downloads_div = downloads_section.find_next_sibling()

                # If the div is found, find all links where the direct child span has the text "Datasheet"
                if downloads_div:
                    datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

                    # Filter the links where the direct child span has the text "Datasheet"
                    datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

                    # If the links are found, return the list of href attributes
                    if datasheet_links:
                        return [link.get('href') for link in datasheet_links]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 104:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            downloads_section = soup.find('h2', class_="Section__title", string="Downloads")

            # If the section is found, find the next sibling section
            if downloads_section:
                downloads_div = downloads_section.find_next_sibling()

                # If the div is found, find all links where the direct child span has the text "Datasheet"
                if downloads_div:
                    datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
                    print(datasheet_links)

                    # Filter the links where the direct child span has the text "Datasheet"
                    datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

                    # If the links are found, return the list of href attributes
                    if datasheet_links:
                        return [link.get('href') for link in datasheet_links]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 23

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 105:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            downloads_section = soup.find('h2', class_="Section__title", string="Downloads")

    # If the section is found, find the next sibling section
    if downloads_section:
        downloads_div = downloads_section.find_next_sibling()

        # If the div is found, find all links where the direct child span has the text "Datasheet"
        if downloads_div:
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 83

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 106:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            downloads_section = soup.find('h2', class_="Section__title", string="Downloads")

    # If the section is found, find the next sibling section
    if downloads_section:
        downloads_div = downloads_section.find_next_sibling()

        # If the div is found, find all links where the direct child span has the text "Datasheet"
        if downloads_div:
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 82

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 107:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            downloads_section = soup.find('h2', class_="Section__title", string="Downloads")

    # If the section is found, find the next sibling section
    if downloads_section:
        downloads_div = downloads_section.find_next_sibling()

        # If the div is found, find all links where the direct child span has the text "Datasheet"
        if downloads_div:
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 81

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 108:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            downloads_section = soup.find('h2', class_="Section__title", string="Downloads")

    # If the section is found, find the next sibling section
    if downloads_section:
        downloads_div = downloads_section.find_next_sibling()

        # If the div is found, find all links where the direct child span has the text "Datasheet"
        if downloads_div:
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 82

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 109:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            downloads_section = soup.find('h2', class_="Section__title", string="Downloads")

    # If the section is found, find the next sibling section
    if downloads_section:
        downloads_div = downloads_section.find_next_sibling()

        # If the div is found, find all links where the direct child span has the text "Datasheet"
        if downloads_div:
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")

            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 83

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 110:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
        
            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 84

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 111:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
        
            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 85

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 112:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
        
            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the list of href attributes
            if datasheet_links:
                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 83

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 113:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
        
            # Filter the links where the direct child span has the text "Datasheet"
            datasheet_links = [link for link in datasheet_links if link.find('span', class_="Downloads__itemName", string="Datasheet")]

            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 83

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 114:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 83

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 115:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return ["https://www.kongsberg.com" + href]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_csv("data/tags_30_11.csv")
number = 83

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 116:
df = pd.read_csv("data/tags_30_11.csv")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 117:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return ["https://www.kongsberg.com" + href]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_excel("data/tags_12_04.xslx")
number = 83

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 118:
from bs4 import BeautifulSoup
import requests
import re

def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return ["https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return ["https://www.kongsberg.com" + href]

    # If the section, the div, or the links are not found, return None
    return None

df = pd.read_excel("data/12_04/tags_12_04.xlsx")
number = 83

print(find_datasheets(df["url"][number]))
print(df["url"][number])
 119:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 120:
import requests
import PyPDF2
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with open('temp.pdf', 'rb') as f:
        pdf_reader = PyPDF2.PdfFileReader(f)

        # Extract text from each page
        text = ''
        for page_num in range(pdf_reader.numPages):
            page = pdf_reader.getPage(page_num)
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://example.com/sample.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 121:
import requests
import PyPDF2
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with open('temp.pdf', 'rb') as f:
        pdf_reader = PyPDF2.PdfFileReader(f)

        # Extract text from each page
        text = ''
        for page_num in range(pdf_reader.numPages):
            page = pdf_reader.getPage(page_num)
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://example.com/sample.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 122:
import requests
import PyPDF2
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with open('temp.pdf', 'rb') as f:
        pdf_reader = PyPDF2.PdfReader(f)

        # Extract text from each page
        text = ''
        for page_num in range(pdf_reader.numPages):
            page = pdf_reader.getPage(page_num)
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://example.com/sample.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 123:
import requests
import PyPDF2
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with open('temp.pdf', 'rb') as f:
        pdf_reader = PyPDF2.PdfReader(f)

        # Extract text from each page
        text = ''
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://example.com/sample.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 124:
import requests
import PyPDF2
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with open('temp.pdf', 'rb') as f:
        pdf_reader = PyPDF2.PdfReader(f)

        # Extract text from each page
        text = ''
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 125:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return [link.get('href') if 'https://simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return [href if 'https://simrad.online' in href else "https://www.kongsberg.com" + href]

    # If the section, the div, or the links are not found, return None
    return None
 126:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 127:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return [link.get('href') if 'https://simrad.online' or "https://www.simrad.online"  in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return [href if 'https://simrad.online' or "https://www.simrad.online" in href else "https://www.kongsberg.com" + href]

    # If the section, the div, or the links are not found, return None
    return None
 128:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 129:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return [link.get('href') if 'https://simrad.online' or "https://www.simrad.online"  in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return [href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href]
    # If the section, the div, or the links are not found, return None
    return None
 130:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 131:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    return [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return [href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href]
    # If the section, the div, or the links are not found, return None
    return None
 132:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 133:
# loop through all data sheets
df = pd.read_csv("data/data_sheets.csv")
df["text"] = ""
for i in range(len(df)):
    print(i)
    try:
        df["text"][i] = scrape_pdf_text(df["Data sheets"][i])
    except:
        print("error")
 134:
import requests
import PyPDF2
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with open('temp.pdf', 'rb') as f:
        pdf_reader = PyPDF2.PdfReader(f)

        # Extract text from each page
        text = ''
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 135:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 136:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 137:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical' in page_text or "TECHNICAL" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 138:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 139:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 140:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 141:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
pdf_urls = pdf_urls.apply(eval)
pdf_urls = pdf_urls.explode()
pdf_urls = pdf_urls.unique()
pdf_urls = [url for url in pdf_urls if url.endswith('.pdf')]
print(pdf_urls)
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 142:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls)
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 143:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 144:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 145:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls)
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 146:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls[0])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 147:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls["Data sheets"][0]])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 148:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls["Data sheets"][0])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 149:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls["Data sheets"][0])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 150:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls["Data sheets"][0])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 151:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls["Data sheets"])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 152:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
pdf_urls = data_sheets['Data sheets'].dropna()

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 153:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
pdf_urls = data_sheets['Data sheets'].dropna()

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 154:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
pdf_urls = data_sheets['Data sheets'].dropna()
print(data_sheets.columns)

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 155:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
pdf_urls = data_sheets['Data sheets'].dropna()
print(data_sheets["Data sheets"][0])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 156:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
pdf_urls = data_sheets['Data sheets'].dropna()
print(data_sheets["Data sheets"][1])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 157:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls["Data sheets"][1])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 158:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls.columns)
print(pdf_urls["Data sheets"][1])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 159:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
pdf_urls = data_sheets['Data sheets'].dropna()
print(pdf_urls.columns)
# print(pdf_urls["Data sheets"][1])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 160:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
data_sheets['Data sheets'].dropna()
print(data_sheets.columns)
# print(pdf_urls["Data sheets"][1])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 161:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
print(data_sheets.columns)
data_sheets['Data sheets'].dropna()
print(data_sheets.columns)
print(data_sheets["Data sheets"][1])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 162:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
print(data_sheets["Data sheets"][1])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 163:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
print(data_sheets["Data sheets"])

pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 164:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
print(data_sheets["Data sheets"].iloc[0])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 165:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
print(data_sheets["Data sheets"].iloc[0])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 166:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
print(data_sheets["Data sheets"].iloc[2])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
# print(pdf_text)
 167:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
print(data_sheets["Data sheets"].iloc[2])
pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 168:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Product_Name"].iloc[2]
print(data_sheets["Data sheets"].iloc[2])
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 169:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Product_Name"].iloc[2]
print(product_datasheet)
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 170:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
                # Stop adding text once "Technical Specification" is encountered
                break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
print(product_datasheet)
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 171:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            # if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
            #     # Stop adding text once "Technical Specification" is encountered
            #     break
            text += page_text

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
print(product_datasheet)
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 172:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            # if 'Technical highlights' in page_text or "TECHNICAL HIGHLIGHTS" in page_text:
            #     # Stop adding text once "Technical Specification" is encountered
            #     break
            text += page_text
    # iterate through the text and find the technical highlights and technical specification section and return the text before it
    text = text.split("\n")
    for i in range(len(text)):
        if "Technical highlights" in text[i] or "TECHNICAL HIGHLIGHTS" in text[i]:
            text = text[:i]
            break
        elif "Technical specification" in text[i] or "TECHNICAL SPECIFICATION" in text[i]:
            text = text[:i]
            break

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
print(product_datasheet)
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 173:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            text += page_text

    # Split the text into lines and find the "Technical highlights" or "Technical specification" section
    text = text.split("\n")
    for i in range(len(text)):
        if "Technical highlights" in text[i] or "TECHNICAL HIGHLIGHTS" in text[i]:
            text = text[:i]
            break
        elif "Technical specification" in text[i] or "TECHNICAL SPECIFICATION" in text[i]:
            text = text[:i]
            break

    # Join the lines with a newline character
    text = "\n".join(text)

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 174:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            page_text = page.extract_text()
            text += page_text

    # Split the text into lines and find the "Technical highlights" or "Technical specification" section
    text = text.split("\n")
    for i in range(len(text)):
        if "Technical highlights" in text[i] or "TECHNICAL HIGHLIGHTS" in text[i]:
            text = text[:i]
            break
        elif "Technical specification" in text[i] or "TECHNICAL SPECIFICATION" in text[i]:
            text = text[:i]
            break

    # Join the lines with a newline character
    text = "\n".join(text)

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 175:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
            for line in page_text:
                if 'Features' in line:
                    in_features_section = True
                elif 'Technical Specification' in line:
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 176:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
            for line in page_text:
                if 'Features' in line:
                    in_features_section = True
                elif 'Technical Specification' in line or "TECHNICAL HIGHLIGHTS" in line or "Technical" in line or "TECHNICAL" in line or "TECHNICAL SPECIFICATIONS" in line or "Technical Specifications" in line or "Technical specification" in line or "Technical specifications" in line or "TECHNICAL SPECIFICATION" in line or "TECHNICAL SPECIFICATIONS" in line or "Technical Specification" in line or "Technical Specifications" in line or "Technical specification" in line or "Technical specifications" in line or "TECHNICAL SPECIFICATION" in line or "TECHNICAL SPECIFICATIONS" in line or "Technical Specification" in line or "Technical Specifications" in line or "Technical specification" in line or "Technical specifications" in line or "TECHNICAL SPECIFICATION" in line or "TECHNICAL SPECIFICATIONS" in line or "Technical Specification" in line or "Technical Specifications" in line or "Technical specification" in line or "Technical specifications" in line or "TECHNICAL SPECIFICATION" in line or "TECHNICAL SPECIFICATIONS" in line:
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 177:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 178:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 179:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 180:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[3]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
 181:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[3]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 182:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[3]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 183:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            # Define a crop box that excludes the margins
            # (You may need to adjust these values depending on the size of your margins)
            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height * 0.9)
            cropped_page = page.crop(crop_box)

            page_text = cropped_page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text
 184:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            # Define a crop box that excludes the margins
            # (You may need to adjust these values depending on the size of your margins)
            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height * 0.9)
            cropped_page = page.crop(crop_box)

            page_text = cropped_page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text
 185:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            # Define a crop box that excludes the margins
            # (You may need to adjust these values depending on the size of your margins)
            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height * 0.9)
            cropped_page = page.crop(crop_box)

            page_text = cropped_page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[3]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 186:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break
                elif not line.startswith('') and in_features_section:
                    in_features_section = False

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[3]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 187:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[3]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 188:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[4]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 189:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 190:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 191:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            # Define a crop box that excludes the margins and the last 1 cm from the bottom
            # (You may need to adjust these values depending on the size of your margins)
            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height - pdfplumber.utils.pdf_to_inches(1))
            cropped_page = page.crop(crop_box)

            page_text = cropped_page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text
 192:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            # Define a crop box that excludes the margins and the last 1 cm from the bottom
            # (You may need to adjust these values depending on the size of your margins)
            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height - pdfplumber.utils.pdf_to_inches(1))
            cropped_page = page.crop(crop_box)

            page_text = cropped_page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 193:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for page in pdf.pages:
            # Define a crop box that excludes the margins and the last 1 cm from the bottom
            # (You may need to adjust these values depending on the size of your margins)
            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height - 28.3)            
            cropped_page = page.crop(crop_box)

            page_text = cropped_page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 194:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height - 28.3)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 195:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 28.3)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 196:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 26.3)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 197:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 1.3)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 198:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 28.3, page.width, page.height)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 199:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 50, page.width, page.height)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 200:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 100, page.width, page.height)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 201:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 200, page.width, page.height)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 202:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                print(i)
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 200, page.width, page.height)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 203:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 200, page.width, page.height)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 204:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 400, page.width, page.height)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 205:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, 400)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 206:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, 200)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 207:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, 150)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 208:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, 100)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 209:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, 50)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 210:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, 800)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 211:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 28)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 212:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 100)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 213:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 214:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 50)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 215:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 60)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 216:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 65)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 217:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 69)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 218:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[6]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 219:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 220:
import requests
import pdfplumber
import os

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        for page in pdf.pages:
            text += page.extract_text()

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 221:
import requests
import PyPDF2
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PyPDF2.PdfFileReader(f)

        # Extract text from each page
        text = ''
        for i in range(reader.getNumPages()):
            page = reader.getPage(i)
            text += page.extractText()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 222:
import requests
from PyPDF2 import PdfReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for page in reader.pages:
            text += page.extract_text()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 223:
import requests
from PyPDF2 import PdfReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for page in reader.pages:
            text += page.extract_text()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url) 
print(pdf_text)
 224:
import requests
from pdfminer.high_level import extract_text
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Extract text from the PDF
        text = extract_text(f)

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 225:
import requests
from pdfminer.high_level import extract_text
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Extract text from the PDF
        text = extract_text(f)

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 226:
import requests
from PIL import Image
import pytesseract
import io
import pdf2image

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Convert PDF to images
        images = pdf2image.convert_from_bytes(f.read())

    # Extract text from each image
    text = ''
    for image in images:
        text += pytesseract.image_to_string(image)

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 227:
import requests
from PIL import Image
import pytesseract
import io
import pdf2image

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Convert PDF to images
        images = pdf2image.convert_from_bytes(f.read())

    # Extract text from each image
    text = ''
    for image in images:
        text += pytesseract.image_to_string(image)

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 228:
import requests
from PIL import Image
import pytesseract
import io
import pdf2image

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Convert PDF to images
        images = pdf2image.convert_from_bytes(f.read())

    # Extract text from each image
    text = ''
    for image in images:
        text += pytesseract.image_to_string(image)

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 229:
import requests
from PIL import Image
import pytesseract
import io
import pdf2image

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Convert PDF to images
        images = pdf2image.convert_from_bytes(f.read())

    # Extract text from each image
    text = ''
    for image in images:
        text += pytesseract.image_to_string(image)

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 230:
import requests

def print_pdf_content(url):
    # Download the PDF file
    response = requests.get(url)
    print(response.content)

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
print_pdf_content(pdf_url)
 231:
import requests
from PyPDF2 import PdfFileReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfFileReader(f)

        # Extract text from each page
        text = ''
        for i in range(reader.getNumPages()):
            page = reader.getPage(i)
            text += page.extractText()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 232:
import requests
from PyPDF2 import PdfFileReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for i in range(reader.getNumPages()):
            page = reader.getPage(i)
            text += page.extractText()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 233:
import requests
from PyPDF2 import PdfReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for i in range(reader.getNumPages()):
            page = reader.getPage(i)
            text += page.extractText()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 234:
import requests
from PyPDF2 import PdfReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for i in range(len(reader.pages)):
            page = reader.getPage(i)
            text += page.extractText()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url)
print(pdf_text)
 235:
import requests
from PyPDF2 import PdfReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for page in reader.pages:
            text += page.extract_text()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url) 
print(pdf_text)
 236:
import requests
from PyPDF2 import PdfReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for page in reader.pages:
            text += page.extract_text()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url) 
print(pdf_text)
 237:
import requests
from PyPDF2 import PdfReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for page in reader.pages:
            text += page.extract_text()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url) 
print(pdf_text)
69/1:
import requests
import PyPDF2
import io

pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'

# Download the PDF file
response = requests.get(pdf_url)
with io.BytesIO(response.content) as pdfFileObj:
    # creating a pdf reader object
    pdfReader = PyPDF2.PdfReader(pdfFileObj)

    # printing number of pages in pdf file
    count = 0
    for page in pdfReader.pages:
        count += 1
        if "features" in page.extract_text().lower():

            index = page.extract_text().lower().index("features")
            page_text = page.extract_text()[index:]
            # button_index =  page_text.index("")
            # Splitting the text into lines
            lines = page_text.split('\n')

            # Extracting bullet points and handling multi-line bullet points
            bullet_points = []
            current_point = ""
            for line in lines:
                if line.strip().startswith(''):
                    if current_point:
                        bullet_points.append(current_point.strip())
                    current_point = line.strip(' ')
                elif current_point:
                    current_point += " " + line.strip()

            # Adding the last point if it exists
            if current_point:
                bullet_points.append(current_point.strip())

            # Printing the extracted list
            # for index, bullet in enumerate(bullet_points, start=1):
            #     print(f"{index}. {bullet}")
            print(bullet_points)
69/2:
import requests
import PyPDF2
import io

pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'

# Download the PDF file
response = requests.get(pdf_url)
with io.BytesIO(response.content) as pdfFileObj:
    # creating a pdf reader object
    pdfReader = PyPDF2.PdfReader(pdfFileObj)

    # printing number of pages in pdf file
    count = 0
    for page in pdfReader.pages:
        count += 1
        if "features" in page.extract_text().lower():

            index = page.extract_text().lower().index("features")
            page_text = page.extract_text()[index:]
            # button_index =  page_text.index("")
            # Splitting the text into lines
            lines = page_text.split('\n')

            # Extracting bullet points and handling multi-line bullet points
            bullet_points = []
            current_point = ""
            for line in lines:
                if line.strip().startswith(''):
                    if current_point:
                        bullet_points.append(current_point.strip())
                    current_point = line.strip(' ')
                elif current_point:
                    current_point += " " + line.strip()

            # Adding the last point if it exists
            if current_point:
                bullet_points.append(current_point.strip())

            # Printing the extracted list
            # for index, bullet in enumerate(bullet_points, start=1):
            #     print(f"{index}. {bullet}")
            print(bullet_points)
69/3:
import requests
import PyPDF2
import io

pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'

# Download the PDF file
response = requests.get(pdf_url)
with io.BytesIO(response.content) as pdfFileObj:
    # creating a pdf reader object
    pdfReader = PyPDF2.PdfReader(pdfFileObj)

    # printing number of pages in pdf file
    count = 0
    for page in pdfReader.pages:
        count += 1
        if "features" in page.extract_text().lower():

            index = page.extract_text().lower().index("features")
            page_text = page.extract_text()[index:]
            # button_index =  page_text.index("")
            # Splitting the text into lines
            lines = page_text.split('\n')

            # Extracting bullet points and handling multi-line bullet points
            bullet_points = []
            current_point = ""
            for line in lines:
                if line.strip().startswith(''):
                    if current_point:
                        bullet_points.append(current_point.strip())
                    current_point = line.strip(' ')
                elif current_point:
                    current_point += " " + line.strip()

            # Adding the last point if it exists
            if current_point:
                bullet_points.append(current_point.strip())

            # Printing the extracted list
            # for index, bullet in enumerate(bullet_points, start=1):
            #     print(f"{index}. {bullet}")
            print(bullet_points)
69/4:
import requests
import PyPDF2
import io

pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'

# Download the PDF file
response = requests.get(pdf_url)
with io.BytesIO(response.content) as pdfFileObj:
    # creating a pdf reader object
    pdfReader = PyPDF2.PdfReader(pdfFileObj)

    # printing number of pages in pdf file
    count = 0
    for page in pdfReader.pages:
        count += 1
        page_text = page.extract_text()
        print(f"Page {count} text:")
        print(page_text)
        print("--------------------")
 238:
import requests
from bs4 import BeautifulSoup
import pandas as pd 
import re
import os
import pdfplumber
 239:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 240:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 241:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 242:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 243:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                print(len(pdf.pages))
                print(page.height)
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 244:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 245:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            print(i)
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 246:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 247:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 248:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 249:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 250:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 251:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets.head(40)

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 252:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 253:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets.head(40)

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 254:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 255:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets.head(40)

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 256:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 257:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": "You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing."
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\nPlease format this text."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        response_format={ "type": "json_object" }
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 258:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 259:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

print(df_with_text["Scraped Text"][0])
 260:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][0])
output_text = generate_text(input_text)

print(output_text)
 261:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": "You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing."
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\nPlease format this text."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 262:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][0])
output_text = generate_text(input_text)

print(output_text)
 263: print(input_text)
 264:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": "You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible."
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\nPlease format this text."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 265:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][0])
output_text = generate_text(input_text)

print(output_text)
 266: print(input_text)
 267:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 268:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
number= 3
print(find_datasheets(df["url"][number]))
print(df["url"][number])
 269:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
number= 4
print(find_datasheets(df["url"][number]))
print(df["url"][number])
 270:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
number= 4
print("text = ",find_datasheets(df["url"][number]))
print(df["url"][number])
 271:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[5]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 272:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):
                    in_features_section = True
                elif re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 273:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 274:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets.head(40)

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 275:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 276: print(input_text)
 277:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions="""
      Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 250 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed.Write these three blocks: 
      Block 1 : [100 - 250 words. ] Provide factual information blocks to explain what value the products bring.
      minimum of one or a maximum of three.

      Block 2 : [40-100 words] Provide the key features for the product. As many as needed

      Block 3 : [10-150] Add technical specifications related to the product. expected structure is "category, parameters and parameter value" As many as needed.

      Return as Json. """,
    model="gpt-3.5-turbo-1106",
    tools=[{"type": "retrieval"}],
)
 278:
# from a pdf url create a pdf file
def create_pdf(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)
    return 'temp.pdf'

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 279: ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 280:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[create_file(url)]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)
      pprint(messages)
      break
    else:
      ### sleep again
      time.sleep(2)
 281:
# from a pdf url create a pdf file
def create_pdf(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)
    return 'temp.pdf'

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 282:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions="""
      Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 250 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed.Write these three blocks: 
      Block 1 : [100 - 250 words. ] Provide factual information blocks to explain what value the products bring.
      minimum of one or a maximum of three.

      Block 2 : [40-100 words] Provide the key features for the product. As many as needed

      Block 3 : [10-150] Add technical specifications related to the product. expected structure is "category, parameters and parameter value" As many as needed.

      Return as Json. """,
    model="gpt-3.5-turbo-1106",
    tools=[{"type": "retrieval"}],
)
thread = client.beta.threads.create()
 283: ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 284:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url)
  file = client.files.create(file=pdf_file,purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)
      pprint(messages)
      break
    else:
      ### sleep again
      time.sleep(2)
 285: ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 286:
# from a pdf url create a pdf file
def create_pdf(url, path):
    # Download the PDF file
    response = requests.get(url)
    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:
        f.write(response.content)
    return os.path.join(path, 'temp.pdf')

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 287:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url)
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)
      pprint(messages)
      break
    else:
      ### sleep again
      time.sleep(2)
 288: ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 289:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)
      pprint(messages)
      break
    else:
      ### sleep again
      time.sleep(2)
 290: ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 291:
# from a pdf url create a pdf file
def create_pdf(url, path):
    # Download the PDF file
    response = requests.get(url)
    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:
        f.write(response.content)
    return os.path.join(path, 'temp.pdf')

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 292:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)
      pprint(messages)
      break
    else:
      ### sleep again
      time.sleep(2)
 293: ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 294:
# from a pdf url create a pdf file
def create_pdf(url, path):
    os.makedirs(path, exist_ok=True)
    # Download the PDF file
    response = requests.get(url)
    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:
        f.write(response.content)
    return os.path.join(path, 'temp.pdf')

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 295:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)
      pprint(messages)
      break
    else:
      ### sleep again
      time.sleep(2)
 296: ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 297:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      print(last_message_content)
      break
    else:
      ### sleep again
      time.sleep(2)
 298: ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 299: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 300:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions="""
      Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 250 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed.Write these three blocks: 
      Block 1 : [100 - 250 words. ] Provide factual information blocks to explain what value the products bring.
      minimum of one or a maximum of three.

      Block 2 : [40-100 words] Provide the key features for the product. As many as needed

      Block 3 : [10-150] Add technical specifications related to the product. expected structure is "category, parameters and parameter value" As many as needed.

      Return as Json. """,
    model="gpt-3.5-turbo-1106",
    tools=[{"type": "retrieval"}],
)
thread = client.beta.threads.create()
 301:
# from a pdf url create a pdf file
def create_pdf(url, path):
    os.makedirs(path, exist_ok=True)
    # Download the PDF file
    response = requests.get(url)
    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:
        f.write(response.content)
    return os.path.join(path, 'temp.pdf')

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 302:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      print(last_message_content)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 303: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 304: print(output_text)
 305:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Tell me who is acquirer",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      print(last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 306: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 307: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 308:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="I have a text that I want to format into headings and text bulks. Please format this text.",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      print(last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 309: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 310: print(output_text)
 311:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 250 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed.Write these three blocks: 
      Block 1 : [100 - 250 words. ] Provide factual information blocks to explain what value the products bring.
      minimum of one or a maximum of three.

      Block 2 : [40-100 words] Provide the key features for the product. As many as needed

      Block 3 : [10-150] Add technical specifications related to the product. expected structure is "category, parameters and parameter value" As many as needed.

      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      print(last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 312: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 313: print(output_text)
 314:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 250 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      print(last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 315:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      print(last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 316: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 317: print(output_text)
 318:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print(message_content.value)
      print(last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 319: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 320: print(output_text)
 321:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 322: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 323: print(output_text)
 324:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Extract the message content
      message_content = message.content[0].text.value
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 325: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 326:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )
  message = client.beta.threads.messages.retrieve(
    thread_id=thread.id,
    message_id=message.id
  )
  # Extract the message content
  message_content = message.content[0].text
  annotations = message_content.annotations
  citations = []

  # Iterate over the annotations and add footnotes
  for index, annotation in enumerate(annotations):
      # Replace the text with a footnote
      message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

      # Gather citations based on annotation attributes
      if (file_citation := getattr(annotation, 'file_citation', None)):
          cited_file = client.files.retrieve(file_citation.file_id)
          citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
      elif (file_path := getattr(annotation, 'file_path', None)):
          cited_file = client.files.retrieve(file_path.file_id)
          citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
          # Note: File download functionality not implemented above for brevity

  # Add footnotes to the end of the message before displaying to user
  message_content.value += '\n' + '\n'.join(citations)
  print("content_annotation " , message_content.value)

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

     
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 327: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 328: print(output_text)
 329:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 330: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 331: print(output_text)
 332:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.
      For example block: heading: "Greater endurance", text: "With spaciuous moopool and rack mount the Sounder USV System is a flexible and adaptive paltform for different applications. Te onboard power generation provides enough power to operate several payloads at once. 

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 333: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 334: print(output_text)
 335:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 336: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 337:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions="""
      Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.

     """,
    model="gpt-3.5-turbo-1106",
    tools=[{"type": "retrieval"}],
)
thread = client.beta.threads.create()
 338:
# from a pdf url create a pdf file
def create_pdf(url, path):
    os.makedirs(path, exist_ok=True)
    # Download the PDF file
    response = requests.get(url)
    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:
        f.write(response.content)
    return os.path.join(path, 'temp.pdf')

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 339:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 340:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(10)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 341: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 342: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 343:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(3)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 344:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    print(run_status.model_dump_json(indent=4))
    time.sleep(3)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 345: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 346: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 347: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 348:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    # print(run_status.model_dump_json(indent=4))
    time.sleep(3)
    print(run_status.status)
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 349: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 350:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions="""
      Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.

     """,
    model="gpt-3.5-turbo-1106",
    tools=[{"type": "retrieval"}],
)
thread = client.beta.threads.create()
 351:
# from a pdf url create a pdf file
def create_pdf(url, path):
    os.makedirs(path, exist_ok=True)
    # Download the PDF file
    response = requests.get(url)
    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:
        f.write(response.content)
    return os.path.join(path, 'temp.pdf')

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 352:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    # print(run_status.model_dump_json(indent=4))
    time.sleep(3)
    print(run_status.status)
    if run_status.status == 'failed':
      print("failed")
      break
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 353: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 354: print(output_text)
 355:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": "You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible."
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\nPlease format this text."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 356:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 357:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 358: print(input_text)
 359:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 10)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 360:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    
                    break

                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 361:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 362:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets.head(40)

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 363:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": "You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible."
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\nPlease format this text."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 364:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 365:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 366:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 367:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 368:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 369:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 370:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 371:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": "You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible."
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\nPlease format this text."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 372:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 373:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": """ 
                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                Block 1 (100 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
                For example : 
                Product Evolution
                HUGIN has been continuously evolving since development began in 1991.
                From the first commercial survey in 1997, KONGSBERG and our partners
                at the Norwegian Defense Research Establishment (FFI) have been at the
                forefront of underwater robotic technology. HUGIN continues to deliver
                World-class performance and new capabilities and features are added
                through software updates and vehicle upgrades.
                Deployability
                HUGIN can be deployed from dedicated vessels, vessels of opportunity
                or from shore. The complete HUGIN system including operator consoles,
                launch and recovery system and the AUV itself can be delivered in
                DNV-certified offshore containers. The containers allows for transport
                by sea, air and land and mobilization is easy with only an external power
                connection required. Various container sizes are available to meet the
                customer needs.

                Block 2 (40 - 100 words):
                List the key features of the product. Include as many as needed. Use bullet points for each feature.

                Block 3 (10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.

                """
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\nPlease format this text."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 374:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 375:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions="""
      Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.

     """,
    model="gpt-3.5-turbo-1106",
    tools=[{"type": "retrieval"}],
)
thread = client.beta.threads.create()
 376:
# from a pdf url create a pdf file
def create_pdf(url, path):
    os.makedirs(path, exist_ok=True)
    # Download the PDF file
    response = requests.get(url)
    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:
        f.write(response.content)
    return os.path.join(path, 'temp.pdf')

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 377:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
      For example : 
      Product Evolution
      HUGIN has been continuously evolving since development began in 1991.
      From the first commercial survey in 1997, KONGSBERG and our partners
      at the Norwegian Defense Research Establishment (FFI) have been at the
      forefront of underwater robotic technology. HUGIN continues to deliver
      World-class performance and new capabilities and features are added
      through software updates and vehicle upgrades.
      Deployability
      HUGIN can be deployed from dedicated vessels, vessels of opportunity
      or from shore. The complete HUGIN system including operator consoles,
      launch and recovery system and the AUV itself can be delivered in
      DNV-certified offshore containers. The containers allows for transport
      by sea, air and land and mobilization is easy with only an external power
      connection required. Various container sizes are available to meet the
      customer needs.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    # print(run_status.model_dump_json(indent=4))
    time.sleep(3)
    print(run_status.status)
    if run_status.status == 'failed':
      print("failed")
      break
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 378: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 379: print(output_text)
 380:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets.head(40)

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 381:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 6:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 60)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 382:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets.head(40)

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 383:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": """ 
                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                Block 1 (100 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.
                For example : 
                Product Evolution
                HUGIN has been continuously evolving since development began in 1991.
                From the first commercial survey in 1997, KONGSBERG and our partners
                at the Norwegian Defense Research Establishment (FFI) have been at the
                forefront of underwater robotic technology. HUGIN continues to deliver
                World-class performance and new capabilities and features are added
                through software updates and vehicle upgrades.
                Deployability
                HUGIN can be deployed from dedicated vessels, vessels of opportunity
                or from shore. The complete HUGIN system including operator consoles,
                launch and recovery system and the AUV itself can be delivered in
                DNV-certified offshore containers. The containers allows for transport
                by sea, air and land and mobilization is easy with only an external power
                connection required. Various container sizes are available to meet the
                customer needs.

                Block 2 (40 - 100 words):
                List the key features of the product. Include as many as needed. Use bullet points for each feature.

                Block 3 (10 - 350 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.

                """
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 384:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    # print(run_status.model_dump_json(indent=4))
    time.sleep(3)
    print(run_status.status)
    if run_status.status == 'failed':
      print("failed")
      break
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 385: output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
 386:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    model="gpt-3.5-turbo-1106",
    tools=[{"type": "retrieval"}],
)
thread = client.beta.threads.create()
 387:
# from a pdf url create a pdf file
def create_pdf(url, path):
    os.makedirs(path, exist_ok=True)
    # Download the PDF file
    response = requests.get(url)
    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:
        f.write(response.content)
    return os.path.join(path, 'temp.pdf')

def create_file(url):
  file = client.files.create(file= create_pdf(url),purpose='assistants')
  return file
 388:
import os
import openai
import time
from pprint import pprint

def ask_gpt(url):
  pdf_file = create_pdf(url, "data/gpt_content")
  file = client.files.create(file=open(pdf_file, "rb"),purpose='assistants')
# Add a Message to a Thread
  message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="""Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

      Write the following three content blocks:

      Block 1 (100 - 400 words):
      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings.

      Block 2 (40 - 100 words):
      List the key features of the product. Include as many as needed. Use bullet points for each feature.

      Block 3 (10 - 150 words):
      Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


      Return as Json.""",
    file_ids=[file.id]  # Add the file to the message
  )

  # Run the Assistant
  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions="Please answer the user, just using text from the file.")
  print(run.model_dump_json(indent=4))

  # If run is 'completed', get messages and print
  while True:
    # Retrieve the run status
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)
    # print(run_status.model_dump_json(indent=4))
    time.sleep(3)
    print(run_status.status)
    if run_status.status == 'failed':
      print("failed")
      break
    if run_status.status == 'completed':
      messages = client.beta.threads.messages.list(thread_id=thread.id)

      # Get the last message from the assistant
      last_message = messages.data[0]

      # Get the content of the last message
      last_message_content = last_message.content[0].text.value

      # Retrieve the message object
      message = client.beta.threads.messages.retrieve(
        thread_id=thread.id,
        message_id=message.id
      )

      # Extract the message content
      message_content = message.content[0].text
      annotations = message_content.annotations
      citations = []

      # Iterate over the annotations and add footnotes
      for index, annotation in enumerate(annotations):
          # Replace the text with a footnote
          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')

          # Gather citations based on annotation attributes
          if (file_citation := getattr(annotation, 'file_citation', None)):
              cited_file = client.files.retrieve(file_citation.file_id)
              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')
          elif (file_path := getattr(annotation, 'file_path', None)):
              cited_file = client.files.retrieve(file_path.file_id)
              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')
              # Note: File download functionality not implemented above for brevity

      # Add footnotes to the end of the message before displaying to user
      message_content.value += '\n' + '\n'.join(citations)
      print("content_annotation " , message_content.value)
      print("lat_message " ,last_message_content)
      print("whole message: ", message)
      return last_message_content
      break
    else:
      ### sleep again
      time.sleep(2)
 389: print(output_text)
 390:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 391:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets.head(40)

# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 392:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 393:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 394:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": """Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                Block 1 (100 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings.

                Block 2 (40 - 100 words):
                List the key features of the product. Include as many as needed. Use bullet points for each feature.

                Block 3 (10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


                Return as Json."""
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 395:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 396: print(input_text)
 397:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": """Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                Block 1 (200 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block.

                Block 2 (40 - 100 words):
                List the key features of the product. Include as many as needed. Use bullet points for each feature.

                Block 3 (10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


                Return as Json."""
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 398:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 399:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": """Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                Block 1 (200 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.

                Block 2 (40 - 100 words):
                List the key features of the product. Include as many as needed. Use bullet points for each feature.

                Block 3 (10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


                Return as Json."""
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 400:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 401:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 402:
system_prompt = """
                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                Block 1 (200 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.

                Block 2 (40 - 100 words):
                List the key features of the product. Include as many as needed. Use bullet points for each feature.

                Block 3 (10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


                Return as Json."""
 403:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 404:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 405: print(input_text)
 406:
system_prompt = """
                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                Block 1 (200 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.

                Block 2 (40 - 100 words):
                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.

                Block 3 (10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.


                Return as Json."""
 407:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions=system_prompt,
    model="gpt-3.5-turbo-1106",
    response_format={ "type": "json_object" },

    tools=[{"type": "retrieval"}],
)
thread = client.beta.threads.create()
 408:
assistant = client.beta.assistants.create(
    name="Kongberg content",
    instructions=system_prompt,
    model="gpt-3.5-turbo-1106",
    tools=[{"type": "retrieval"}],
)
thread = client.beta.threads.create()
 409:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        response_format={ "type": "json_object" },
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 410:
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

print(output_text)
 411:
import json
import csv

# Parse the JSON data
data = json.loads(output_text)

# Prepare the data for the CSV
csv_data = []

# Extract data from Block 1
for item in data['Block 1']:
    csv_data.append([item['heading'], item['content'], ''])

# Extract data from Block 2
for feature in data['Block 2']['key_features']:
    csv_data.append(['', feature, ''])

# Extract data from Block 3
for spec in data['Block 3']['technical_specifications']:
    csv_data.append(['', '', spec])

# Write the data to a CSV file
with open('output.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows(csv_data)
 412:
# output_text = ask_gpt("https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf")
output_text = ask_gpt("https://www.kongsberg.com/contentassets/88df0cc7b3574a0ab3d1d0d560fae99b/datasheet_blueinsight.pdf")
 413:
import pandas as pd
import json

def add_data_to_df(df, data):
    # Extract data from Block 1
    block1_data = [{'Heading': item['heading'], 'Content': item['content']} for item in data['Block 1']]
    block1_df = pd.DataFrame(block1_data)

    # Extract data from Block 2
    block2_data = [{'Key Features': feature} for feature in data['Block 2']['key_features']]
    block2_df = pd.DataFrame(block2_data)

    # Extract data from Block 3
    block3_data = [{'Technical Specifications': spec} for spec in data['Block 3']['technical_specifications']]
    block3_df = pd.DataFrame(block3_data)

    # Concatenate the dataframes
    df = pd.concat([df, block1_df, block2_df, block3_df], axis=1)

    return df

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

input_text = (df_with_text["Scraped Text"][2])
output_text = generate_text(input_text)

# Parse the JSON data
data = json.loads(output_text)

# Add the data to the dataframe
df_with_text = add_data_to_df(df_with_text, data)

print(df_with_text)
 414:
import json
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")
index = 2
input_text = (df_with_text["Scraped Text"][index])
output_text = generate_text(input_text)
# Parse the JSON data
data = json.loads(output_text)
block1 = data['Block 1']
block2 = data['Block 2']
block3 = data['Block 3']
# Assuming df_with_text is your DataFrame and '2' is the index of the row you want to modify
df_with_text.loc[index, 'Block 1'] = str(block1)
df_with_text.loc[index, 'Block 2'] = str(block2)
df_with_text.loc[index, 'Block 3'] = str(block3)


print(output_text)
 415: df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 416:
df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 417:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv",usecols=["Product_Name", "url", "Data sheets", "Scraped Text"])

for index, row in df_with_text.head(4).iterrows():
    input_text = row["Scraped Text"]
    output_text = generate_text(input_text)
    # Parse the JSON data
    data = json.loads(output_text)
    block1 = data['Block 1']
    block2 = data['Block 2']
    block3 = data['Block 3']
    # Add the blocks to the DataFrame
    df_with_text.loc[index, 'General text'] = str(block1)
    df_with_text.loc[index, 'Features'] = str(block2)
    df_with_text.loc[index, 'Technical Specifications'] = str(block3)


df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 418:
system_prompt = """
                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                "General text"  (200 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}

                 "Key Features": { (40 - 100 words):
                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}

                "Technical Specifications": {(10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.}


                Return as Json with the same formats as always."""
 419:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        response_format={ "type": "json_object" },
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 420:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv",usecols=["Product_Name", "url", "Data sheets", "Scraped Text"])

for index, row in df_with_text.head(4).iterrows():
    input_text = row["Scraped Text"]
    output_text = generate_text(input_text)
    # Parse the JSON data
    data = json.loads(output_text)
    block1 = data['Block 1']
    block2 = data['Block 2']
    block3 = data['Block 3']
    # Add the blocks to the DataFrame
    df_with_text.loc[index, 'General text'] = str(block1)
    df_with_text.loc[index, 'Features'] = str(block2)
    df_with_text.loc[index, 'Technical Specifications'] = str(block3)


df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 421:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv",usecols=["Product_Name", "url", "Data sheets", "Scraped Text"])

for index, row in df_with_text.head(4).iterrows():
    input_text = row["Scraped Text"]
    output_text = generate_text(input_text)
    print(output_text)
    # Parse the JSON data
    data = json.loads(output_text)
    block1 = data['General text']
    block2 = data['Key Features']
    block3 = data['Technical Specifications']
    # Add the blocks to the DataFrame
    df_with_text.loc[index, 'General text'] = str(block1)
    df_with_text.loc[index, 'Features'] = str(block2)
    df_with_text.loc[index, 'Technical Specifications'] = str(block3)


df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 422:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv",usecols=["Product_Name", "url", "Data sheets", "Scraped Text"])

for index, row in df_with_text.head(15).iterrows():
    input_text = row["Scraped Text"]
    output_text = generate_text(input_text)
    print(output_text)
    # Parse the JSON data
    data = json.loads(output_text)
    block1 = data['General text']
    block2 = data['Key Features']
    block3 = data['Technical Specifications']
    # Add the blocks to the DataFrame
    df_with_text.loc[index, 'General text'] = str(block1)
    df_with_text.loc[index, 'Features'] = str(block2)
    df_with_text.loc[index, 'Technical Specifications'] = str(block3)


df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 423:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 12:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 60)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 424:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure|data sheets|Data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)
        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 425:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 426:
import requests
from PyPDF2 import PdfReader
import io

def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with io.BytesIO(response.content) as f:
        # Open the PDF file
        reader = PdfReader(f)

        # Extract text from each page
        text = ''
        for page in reader.pages:
            text += page.extract_text()

    return text

# Example usage
pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'
pdf_text = scrape_pdf_text(pdf_url) 
print(pdf_text)
 427:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):

        input_text = row["Scraped Text"]
        output_text = generate_text(input_text)
        print(output_text)
        # Parse the JSON data
        data = json.loads(output_text)
        block1 = data['General text']
        block2 = data['Key Features']
        block3 = data['Technical Specifications']
        # Add the blocks to the DataFrame
        df_with_text.loc[index, 'General text'] = str(block1)
        df_with_text.loc[index, 'Features'] = str(block2)
        df_with_text.loc[index, 'Technical Specifications'] = str(block3)


df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 428:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):

        input_text = row["Scraped Text"]
        output_text = generate_text(input_text)
        print(output_text)
        # Parse the JSON data
        data = json.loads(output_text)
        block1 = data['General text']
        block2 = data['Key Features']
        block3 = data['Technical Specifications']
        # Add the blocks to the DataFrame
        df_with_text.loc[index, 'General text'] = str(block1)
        df_with_text.loc[index, 'Features'] = str(block2)
        df_with_text.loc[index, 'Technical Specifications'] = str(block3)
        df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
        df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 429:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")


# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 430:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
            df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 431:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    # if pd.isnull(row["General text"]):
    if not pd.isnull(row["Scraped Text"]):

        input_text = row["Scraped Text"]
        output_text = generate_text(input_text)
        print(output_text)
        # Parse the JSON data
        data = json.loads(output_text)
        block1 = data['General text']
        block2 = data['Key Features']
        block3 = data['Technical Specifications']
        # Add the blocks to the DataFrame
        df_with_text.loc[index, 'General text'] = str(block1)
        df_with_text.loc[index, 'Features'] = str(block2)
        df_with_text.loc[index, 'Technical Specifications'] = str(block3)
        df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
        df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 432:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    # if pd.isnull(row["General text"]):
    if not pd.isnull(row["Scraped Text"]):

        input_text = row["Scraped Text"]
        output_text = generate_text(input_text)
        print(output_text)
        # Parse the JSON data
        data = json.loads(output_text)
        block1 = data['General text']
        block2 = data['Key Features']
        block3 = data['Technical Specifications']
        # Add the blocks to the DataFrame
        df_with_text.loc[index, 'General text'] = (block1)
        df_with_text.loc[index, 'Features'] = (block2)
        df_with_text.loc[index, 'Technical Specifications'] = (block3)
        df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
        df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 433:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = (block1)
            df_with_text.loc[index, 'Features'] = (block2)
            df_with_text.loc[index, 'Technical Specifications'] = (block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
            df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 434:
import pandas as pd
import json
import re

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Remove illegal characters
            block1 = re.sub(r'[\0\v\f\xa0]', '', block1)
            block2 = re.sub(r'[\0\v\f\xa0]', '', block2)
            block3 = re.sub(r'[\0\v\f\xa0]', '', block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = (block1)
            df_with_text.loc[index, 'Features'] = (block2)
            df_with_text.loc[index, 'Technical Specifications'] = (block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
            df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 435:
import pandas as pd
import json
import re

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")
for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Remove illegal characters
            block1 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block1]
            block2 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block2]
            block3 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block3]
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = (block1)
            df_with_text.loc[index, 'Features'] = (block2)
            df_with_text.loc[index, 'Technical Specifications'] = (block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
            df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 436:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        response_format={ "type": "json_object" },
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 437:
system_prompt = """
                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                "General text":{ (200 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}

                "Key Features": { (40 - 100 words):
                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}

                "Technical Specifications": {(10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.}



                Return as Json with the same formats as always."""
 438:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        response_format={ "type": "json_object" },
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 439:
import pandas as pd
import json
import re

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")
for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Remove illegal characters
            block1 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block1]
            block2 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block2]
            block3 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block3]
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = (block1)
            df_with_text.loc[index, 'Features'] = (block2)
            df_with_text.loc[index, 'Technical Specifications'] = (block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
            df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 440:
import pandas as pd
import json
import re

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")
for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):
            print(index," product name: ", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Remove illegal characters
            block1 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block1]
            block2 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block2]
            block3 = [re.sub(r'[\0\v\f\xa0]', '', item) for item in block3]
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = (block1)
            df_with_text.loc[index, 'Features'] = (block2)
            df_with_text.loc[index, 'Technical Specifications'] = (block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 441:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if not pd.isnull(row["Scraped Text"]):
        print(index,"product_name:", row["Product_Name"])

        input_text = row["Scraped Text"]
        output_text = generate_text(input_text)
        print(output_text)
        # Parse the JSON data
        data = json.loads(output_text)
        block1 = data['General text']
        block2 = data['Key Features']
        block3 = data['Technical Specifications']
        # Add the blocks to the DataFrame
        df_with_text.loc[index, 'General text'] = (block1)
        df_with_text.loc[index, 'Features'] = (block2)
        df_with_text.loc[index, 'Technical Specifications'] = (block3)
        df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 442: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/modems/cnode-minis/"))
 443:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure|data sheets|Data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("forund links 1")
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 444: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/modems/cnode-minis/"))
 445: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 446:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure|data sheets|Data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 447:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data- and product sheets|Brochure|data sheets|Data sheet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 448: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 449:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets|Brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 450: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 451:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets|Brochure", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 452: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 453:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return links

        # If no data sheets are found, look for brochures
        brochure_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("brochure", re.I))

        if brochure_subtitle:
            brochure_section = brochure_subtitle.find_next_sibling()

            if brochure_section:
                brochure_links = brochure_section.find_all('a', class_="Downloads__itemLink")

                if brochure_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in brochure_links if link.get('href').endswith('.pdf')]
                    return links

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 454: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 455:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        brochure_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("brochure", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return links

        # If no data sheets are found, look for brochures
       

        elif brochure_subtitle:
            brochure_section = brochure_subtitle.find_next_sibling()

            if brochure_section:
                brochure_links = brochure_section.find_all('a', class_="Downloads__itemLink")

                if brochure_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in brochure_links if link.get('href').endswith('.pdf')]
                    return links

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 456: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 457:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 458: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 459:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    print(links)
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 460: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 461:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]
                    print(', '.join(links))
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 462: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 463:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links]
                    print(', '.join(links))
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 464: print(find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 465:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links]
                    links = [link.get('href') + '.pdf' if not link.get('href').endswith('.pdf') else link.get('href') for link in datasheet_links]
                    print(', '.join(links))
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 466:
new_data = (find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
)
 467: new_data = (find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 468:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    print("found links 1")
                    print(datasheet_links)
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links]
                    print(', '.join(links))
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
    # If the section, the div, or the links are not found, return None
    return None
 469: new_data = (find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 470:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links]
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                #Found a random link that was not data sheet
                href = link.get('href')
                if href.endswith('.pdf'):
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
                print(href + " is not a data sheet ", "for product: ", url)
    # If the section, the div, or the links are not found, return None

    return None
 471: new_data = (find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/"))
 472:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 473:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets", re.I))
        print(datasheets_subtitle)
        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links]
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                #Found a random link that was not data sheet
                href = link.get('href')
                if href.endswith('.pdf'):
                    print(href + " is not a data sheet ", "for product: ", url)
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
                
    # If the section, the div, or the links are not found, return None

    return None
 474:
def find_datasheets(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the div with the class "Downloads"
    downloads_div = soup.find('div', class_="Downloads")

    # If the div is found, find the section with the subtitle "Data sheet", "DATA SHEET", or "Data- and product sheets" within this div
    if downloads_div:
        datasheets_subtitle = downloads_div.find('h3', class_="Section__subtitle", string=re.compile("data sheet|data sheets|data- and product sheets|Product leaflet", re.I))

        # If the subtitle is found, find the next sibling section
        if datasheets_subtitle:
            datasheets_section = datasheets_subtitle.find_next_sibling()

            # If the section is found, find all links within this section
            if datasheets_section:
                datasheet_links = datasheets_section.find_all('a', class_="Downloads__itemLink")

                # If the links are found, prepend "https://www.kongsberg.com" to each href attribute and return the list
                if datasheet_links:
                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else "https://www.kongsberg.com" + link.get('href') for link in datasheet_links]
                    print(datasheets_subtitle.get_text(strip=True) + " for product: ", url, " are: ", links)
                    return ', '.join(links)

        else:
            # If no subtitle is found, find all links where the direct child span has the text "Datasheet"
            datasheet_links = downloads_div.find_all('a', class_="Downloads__itemLink")
    
            # If the links are found, return the href attribute of the first link that ends with ".pdf"
            for link in datasheet_links:
                #Found a random link that was not data sheet
                href = link.get('href')
                if href.endswith('.pdf'):
                    print(href + " is not a data sheet ", "for product: ", url)
                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else "https://www.kongsberg.com" + href
                
    # If the section, the div, or the links are not found, return None

    return None
 475:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 476:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 477:
df = pd.read_excel("data/12_04/tags_12_04.xlsx")

# Add a new column "Data sheets" to the dataframe
df['Data sheets'] = df['url'].apply(find_datasheets)

# only keep column url, product name and data sheets
df = df[['url', 'Product_Name', 'Data sheets']]
df.to_csv("data/data_sheets.csv", index=False)
df.to_excel("data/data_sheets.xlsx", index=False)
 478: new_data = (find_datasheets("https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/"))
 479:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 10:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 480:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")


# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 481:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")


# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 482:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 10:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 483:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")


# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 484:

# Load the CSV file
data_sheets = pd.read_csv("data/data_sheets.csv")


# Filter rows with valid data sheet URLs
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]

# Scrape the text from each data sheet and save it as a new column
data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)

# Save the DataFrame to a new CSV file
data_sheets.to_csv("data/data_sheets_with_text.csv", index=False)
 485:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = ' '.join(block1)
            df_with_text.loc[index, 'Features'] = ' '.join(block2)
            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 486:
import streamlit as st
import pandas as pd
from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
 487:
system_prompt = """
                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                "General text":{ (200 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}

                "Key Features": { (40 - 100 words):
                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}

                "Technical Specifications": {(10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.}



                Return as Json with the same formats as always."""
 488:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        response_format={ "type": "json_object" },
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 489:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = ' '.join(block1)
            df_with_text.loc[index, 'Features'] = ' '.join(block2)
            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 490:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if not pd.isnull(row["Scraped Text"]):
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = ' '.join(block1)
            df_with_text.loc[index, 'Features'] = ' '.join(block2)
            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 491:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
        if not pd.isnull(row["Scraped Text"]):
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = ' '.join(block1)
            df_with_text.loc[index, 'Features'] = ' '.join(block2)
            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 492:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
        if not pd.isnull(row["Scraped Text"]):
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            block1 = data['General text']
            block2 = data['Key Features']
            block3 = data['Technical Specifications']
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = ' '.join(block1)
            df_with_text.loc[index, 'Features'] = ' '.join(block2)
            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 493:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
        if not pd.isnull(row["Scraped Text"]):
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = ' '.join(block1)
            df_with_text.loc[index, 'Features'] = ' '.join(block2)
            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 494:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
        if not pd.isnull(row["Scraped Text"]):
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = ' '.join(block1)
            df_with_text.loc[index, 'Features'] = ' '.join(block2)
            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 495:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
        if not pd.isnull(row["Scraped Text"]):
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 496: print(scrape_pdf_text("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/"))
 497:
import requests

def save_pdf(url, filename):
    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Write the content to a PDF file
        with open(filename, 'wb') as f:
            f.write(response.content)
        print(f"PDF saved as {filename}")
    else:
        print(f"Failed to retrieve PDF from {url}")

# Example usage
save_pdf("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/", "output.pdf")
 498: print(scrape_pdf_text("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/acs500-emergency-acoustic-bop-control-system/"))
 499: print(save_pdf("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/acs500-emergency-acoustic-bop-control-system/"))
 500: print(save_pdf("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/acs500-emergency-acoustic-bop-control-system/", "output.pdf"))
 501: print(scrape_pdf_text("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/acs500-emergency-acoustic-bop-control-system/"))
 502: print(scrape_pdf_text("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/"))
 503:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 10:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
print(scrape_pdf_text("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/"))
 504:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 10:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file


    return text

# Example usage
print(scrape_pdf_text("https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/"))
 505:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 506:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        print(index,"product_name:", row["Product_Name"])
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            # df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            # df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 507:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
 508:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
            df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 509:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
            df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 510:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_with_text.csv", index=False)
df_with_text.to_excel("data/data_sheets_with_text.xlsx", index=False)
 511:
produkter_alle = pd.read_excel("data/styrings_data/produkter_alle.xlsx", usecols=["Product_Name","url","Product category"])
df_with_text = pd.read_csv("data/data_sheets_with_text.csv")

#Merge the two dataframes on the product name.
 512:
produkter_alle = pd.read_excel("data/styrings_data/produkter_alle.xlsx", usecols=["Product_Name","url","Product category"])
df_with_text = pd.read_csv("data/data_sheets_with_text.csv", usecols=["Product_Name","Data sheets","General text","Features","Technical Specifications"])

#Merge the two dataframes on the product name, and keep product name, url, product category from produkter_alle, the rest from df_with_text
merged = pd.merge(produkter_alle, df_with_text, on="Product_Name", how="left")
merged.to_csv("data/merged_data_test.csv", index=False)
 513:
produkter_alle = pd.read_excel("data/styrings_data/produkter_alle.xlsx", usecols=["Product_Name","url","Product category"])
df_with_text = pd.read_csv("data/data_sheets_with_text.csv", usecols=["Product_Name","Data sheets","General text","Features","Technical Specifications"])

#Merge the two dataframes on the product name, and keep product name, url, product category from produkter_alle, the rest from df_with_text
merged = pd.merge(produkter_alle, df_with_text, on="Product_Name", how="left")
merged.to_csv("data/styrings_data/ai_produkter.csv", index=False)
merged.to_excel("data/styrings_data/ai_produkter.xlsx", index=False)
 514:
data_sheets_df = pd.read_csv("data/data_sheets.csv")
# find all products that have more than one data sheet
data_sheets_df = data_sheets_df[data_sheets_df['Data sheets'].str.count(',') > 0]
data_sheets_df.to_csv("data/data_sheets_multiple.csv", index=False)
 515:
data_sheets_df = pd.read_csv("data/data_sheets.csv")
# find all products that have more than one data sheet
data_sheets_df = data_sheets_df[data_sheets_df['Data sheets'].str.count(',') > 0]
data_sheets_df.to_csv("data/data_sheets_multiple.csv", index=False)
data_sheets_df.to_excel("data/data_sheets_multiple.xlsx", index=False)
 516:
# Read the CSV file
data_sheets_df = pd.read_csv("data/data_sheets.csv")

# Find all products that have more than one data sheet
data_sheets_df = data_sheets_df[data_sheets_df['Data sheets'].str.count(',') > 0]

# Split the 'Data sheets' column into multiple columns
data_sheets_split = data_sheets_df['Data sheets'].str.split(',', expand=True)

# Rename the new columns
data_sheets_split.columns = [f'Data sheet {i+1}' for i in range(data_sheets_split.shape[1])]

# Concatenate the original DataFrame with the new DataFrame
data_sheets_df = pd.concat([data_sheets_df, data_sheets_split], axis=1)

# Drop the original 'Data sheets' column
data_sheets_df = data_sheets_df.drop(columns=['Data sheets'])

# Save the DataFrame to CSV and Excel files
data_sheets_df.to_csv("data/data_sheets_multiple.csv", index=False)
data_sheets_df.to_excel("data/data_sheets_multiple.xlsx", index=False)
 517:
merged_df = pd.read_csv("data/styrings_data/ai_produkter.csv")

# Find all products that have more no data sheets
merged_df_without_datasheet = merged_df[merged_df['Data sheets'].isna()]

# Save the DataFrame to CSV and Excel files
merged_df_without_datasheet.to_csv("data/styrings_data/ai_produkter_uten_datasheet.csv", index=False)
merged_df_without_datasheet.to_excel("data/styrings_data/ai_produkter_uten_datasheet.xlsx", index=False)
 518:
import requests

def scrape_webpage(url):
    # Send an HTTP GET request to the URL
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Get the HTML content of the webpage
        html_content = response.text
        return html_content
    else:
        print(f"Failed to scrape webpage. Status code: {response.status_code}")
        return None
 519:
data_sheets_df = pd.read_csv("data/data_sheets.csv")

# Find all products that have more no data sheets
df_without_datasheet = data_sheets_df[data_sheets_df['Data sheets'].isna()]

# Save the DataFrame to CSV and Excel files
df_without_datasheet.to_csv("data/styrings_data/ai_produkter_uten_datasheet.csv", index=False)
df_without_datasheet.to_excel("data/styrings_data/ai_produkter_uten_datasheet.xlsx", index=False)
 520:
df_without_datasheet = pd.read_csv("data/styrings_data/ai_produkter_uten_datasheet.csv")
# scrape the text from each product page and save it as a new column
df_without_datasheet['Scraped Text'] = df_without_datasheet['url'].apply(scrape_webpage)
 521: df_without_datasheet.to_csv("data/styrings_data/uten_datasheet_scraped.csv", index=False)
 522:
import pandas as pd
import json

df_with_text = pd.read_csv("data/styrings_data/uten_datasheet_scraped.csv")

for index, row in df_with_text.iterrows():
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 523:
from bs4 import BeautifulSoup

def scrape_webpage(url):
    # Send an HTTP GET request to the URL
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Get the HTML content of the webpage
        html_content = response.text

        # Parse the HTML content with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')

        # Extract the text from the parsed HTML content
        text_content = soup.get_text()

        return text_content
    else:
        print(f"Failed to scrape webpage. Status code: {response.status_code}")
        return None
 524:
df_without_datasheet = pd.read_csv("data/styrings_data/ai_produkter_uten_datasheet.csv")
# scrape the text from each product page and save it as a new column
df_without_datasheet['Scraped Text'] = df_without_datasheet['url'].apply(scrape_webpage)
 525: df_without_datasheet.to_csv("data/styrings_data/uten_datasheet_scraped.csv", index=False)
 526:
import pandas as pd
import json

df_with_text = pd.read_csv("data/styrings_data/uten_datasheet_scraped.csv")

for index, row in df_with_text.iterrows():
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 527:
import pandas as pd
import json

df_with_text = pd.read_csv("data/styrings_data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 528:
import pandas as pd
import json

df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 529:
from bs4 import BeautifulSoup, NavigableString
import requests

def extract_text(url):
    # Get the HTML of the page
    response = requests.get(url)
    html = response.text

    # Parse the HTML with BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')

    # Find the divs with the specified classes
    divs = soup.find_all(class_="RichtextArea ProductPage__richtext text-wrapper")

    # Extract the text of each div
    text_string = ''
    for div in divs:
        text_string += div.get_text() + '\n'

    return text_string

print(extract_text("https://www.kongsberg.com/maritime/products/onshore/onshore-systems/ais-network-infrastructure-shorebased/ais-service-management-system/"))
 530:
df_without_datasheet = pd.read_csv("data/styrings_data/ai_produkter_uten_datasheet.csv")
# scrape the text from each product page and save it as a new column
df_without_datasheet['Scraped Text'] = df_without_datasheet['url'].apply(extract_text)
 531: df_without_datasheet.to_csv("data/styrings_data/uten_datasheet_scraped.csv", index=False)
 532:
import pandas as pd
import json
df_with_text = pd.read_csv("data/styrings_data/uten_datasheet_scraped.csv")

for index, row in df_with_text.iterrows():
    # if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 533:
import pandas as pd
import json
df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 534:
import pandas as pd
import json
df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 535:
import pandas as pd
import json
df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 536:
import pandas as pd
import json
df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 537:
system_prompt = """
                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert "I don't know" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.

                Write the following three content blocks:

                "General text":{ (200 - 400 words):
                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}

                "Key Features": { (40 - 100 words):
                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}

                "Technical Specifications": {(10 - 150 words):
                Add technical specifications related to the product, following the expected structure: "category, parameters, and parameter value." Include as many specifications as needed. Use bullet points for each specification.}



                Return as Json with the same formats as always. Always add General text, Key Features, Techical Specification. If you have no information say I don`t know """
 538:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        response_format={ "type": "json_object" },
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 539:
def generate_text(text):
    messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"I have a text that I want to format into headings and text bulks. The text is:\n\n{text}\n\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary."
            }
        ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        response_format={ "type": "json_object" },
        messages=messages,
    )
    output_text = response.choices[0].message.content.strip()
    return output_text
 540:
import pandas as pd
import json
df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}


            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 541:
import pandas as pd
import json
df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            # print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}
            print(data)

            block1 = data['general text']
            block2 = data['key features']
            block3 = data['technical specifications']
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 542:
import pandas as pd
import json
df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            # print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}
            print(data)

            block1 = data.get('general text', '')
            block2 = data.get('key features', '')
            block3 = data.get('technical specifications', '')
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
 543:
import pandas as pd
import json
df_with_text = pd.read_csv("data/data_sheets_web_scraped_ai.csv")

for index, row in df_with_text.iterrows():
    if pd.isnull(row["General text"]):
        if (row["Scraped Text"]) != "":
            print(index,"product_name:", row["Product_Name"])

            input_text = row["Scraped Text"]
            output_text = generate_text(input_text)
            # print(output_text)
            # Parse the JSON data
            data = json.loads(output_text)
            data = {k.lower(): v for k, v in data.items()}
            print(data)

            block1 = data.get('general text', '')
            block2 = data.get('key features', '')
            block3 = data.get('technical specifications', '')
            print(block1)
            print(block2)
            print(block3)
            # Add the blocks to the DataFrame
            df_with_text.loc[index, 'General text'] = str(block1)
            df_with_text.loc[index, 'Features'] = str(block2)
            df_with_text.loc[index, 'Technical Specifications'] = str(block3)
            df_with_text.to_csv("data/data_sheets_web_scraped_ai.csv", index=False)
df_with_text.to_excel("data/data_sheets_web_scraped_ai.xlsx", index=False)
67/22:

file_path_original_dataframe_csv = "data/tags_30_11.csv"
file_path_original_dataframe_excel = "data/tags_30_11.xlsx"

file_path_filter_dataframe_csv = "filter_tags.csv"
file_path_filter_dataframe_excel = "filter_tags.xlsx"

new_file_path_splitted_dataframe_excel = "data/"+today+"/splitted_tags_" + today_hour + ".xlsx"
new_file_path_splitted_dataframe_csv = "data/"+today+"/splitted_tags_" + today_hour + ".csv"

new_file_path_grouped_dataframe_excel = "data/"+today+"/grouped_tags_" + today_hour + ".xlsx"
new_file_path_grouped_dataframe_csv = "data/"+today+"/grouped_tags_" + today_hour + ".csv"
67/23:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of filter_dataframe for easier lookup
filter_dataframe.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in filter_dataframe.index:
        category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")
        print(row["Product category"])
        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/24:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json

from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
67/25:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json

from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
67/26:

today = time.strftime("%m_%d")
today_hour = time.strftime("%m_%d_%H:%M")

# Make a folder for today's data inside the data folder
if not os.path.exists("data/" + today):
    os.mkdir("data/" + today)
67/27:

file_path_original_dataframe_csv = "data/tags_30_11.csv"
file_path_original_dataframe_excel = "data/tags_30_11.xlsx"

file_path_filter_dataframe_csv = "filter_tags.csv"
file_path_filter_dataframe_excel = "filter_tags.xlsx"

new_file_path_splitted_dataframe_excel = "data/"+today+"/splitted_tags_" + today_hour + ".xlsx"
new_file_path_splitted_dataframe_csv = "data/"+today+"/splitted_tags_" + today_hour + ".csv"

new_file_path_grouped_dataframe_excel = "data/"+today+"/grouped_tags_" + today_hour + ".xlsx"
new_file_path_grouped_dataframe_csv = "data/"+today+"/grouped_tags_" + today_hour + ".csv"
67/28:

file_path_original_dataframe_csv = "data/tags_30_11.csv"
file_path_original_dataframe_excel = "data/tags_30_11.xlsx"

file_path_filter_dataframe_csv = "filter_tags.csv"
file_path_filter_dataframe_excel = "filter_tags.xlsx"

new_file_path_splitted_dataframe_excel = "data/"+today+"/splitted_tags_" + today_hour + ".xlsx"
new_file_path_splitted_dataframe_csv = "data/"+today+"/splitted_tags_" + today_hour + ".csv"

new_file_path_grouped_dataframe_excel = "data/"+today+"/grouped_tags_" + today_hour + ".xlsx"
new_file_path_grouped_dataframe_csv = "data/"+today+"/grouped_tags_" + today_hour + ".csv"
67/29:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
67/30:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of filter_dataframe for easier lookup
filter_dataframe.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in filter_dataframe.index:
        category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")
        print(row["Product category"])
        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/31:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of filter_dataframe for easier lookup
filter_dataframe.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in filter_dataframe.index:
        category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")
        print(row["Product category"])
        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/32:
filter_df = pd.read_excel(file_path_filter_dataframe_excel, usecols=["Product category", "Filter tags"])
print(len(filter_df))
67/33:
filter_df = pd.read_excel(file_path_filter_dataframe_excel, usecols=["category", "Filter tags"])
print(len(filter_df))
67/34:
filter_df = pd.read_excel(file_path_filter_dataframe_excel)
print((filter_df))
67/35:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of filter_dataframe for easier lookup
filter_dataframe.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in filter_dataframe.index:
        category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")
        print(row["Product category"])
        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/36:
import pandas as pd
import numpy as np
import streamlit as st
import openai
import time
from bs4 import BeautifulSoup
import requests
import re
import os
import json

from openai._client import OpenAI

client = OpenAI(
    api_key=st.secrets["openai"]["api_key"],
)
67/37:

today = time.strftime("%m_%d")
today_hour = time.strftime("%m_%d_%H:%M")

# Make a folder for today's data inside the data folder
if not os.path.exists("data/" + today):
    os.mkdir("data/" + today)
67/38:

file_path_original_dataframe_csv = "data/tags_30_11.csv"
file_path_original_dataframe_excel = "data/tags_30_11.xlsx"

file_path_filter_dataframe_csv = "filter_tags.csv"
file_path_filter_dataframe_excel = "filter_tags.xlsx"

new_file_path_splitted_dataframe_excel = "data/"+today+"/splitted_tags_" + today_hour + ".xlsx"
new_file_path_splitted_dataframe_csv = "data/"+today+"/splitted_tags_" + today_hour + ".csv"

new_file_path_grouped_dataframe_excel = "data/"+today+"/grouped_tags_" + today_hour + ".xlsx"
new_file_path_grouped_dataframe_csv = "data/"+today+"/grouped_tags_" + today_hour + ".csv"
67/39:
# load the dict from pickle file
import pickle
with open('./data/url_technical_text_dict.pkl', 'rb') as handle:
    url_text_dict = pickle.load(handle)
67/40:
df = pd.read_excel(file_path_original_dataframe_excel, usecols=["Product_Name","Product category", "url", "image_url"])
print(len(df))

# Split the 'Product category' column into multiple rows
df = df.assign(
    **{"Product category": df["Product category"].str.split(",")}
).explode("Product category")

# Trim whitespace from the 'Product category' column
df["Product category"] = df["Product category"].str.strip()


print(len(df))
67/41:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of filter_dataframe for easier lookup
filter_dataframe.set_index('category', inplace=True)

# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    print(index)
    product_category = str(row["Product category"])  # Convert to string
    # Get the tags for the product category
    if product_category in filter_dataframe.index:
        category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
        print(row["Product_Name"])
        url = row["url"]
        website_text = url_text_dict.get(url, "")
        print(row["Product category"])
        print(category_tags)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
            },
            {
                "role": "user",
                "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
            }
        ]
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages
            # response_format={ "type": "json_object" }
        )
        tags = response.choices[0].message.content.strip()
        df_tech_new_tags.loc[index, "product_tags"] = tags
        # print(row["Product_Name"])
        print(tags)
        print("______________")
    else:
        print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/42: print(df_tech_new_tags)
67/43: df_tech_new_tags.to_excel("test.xlsx", index=False)
67/44:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
# df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# # Set 'category' as the index of filter_dataframe for easier lookup
# filter_dataframe.set_index('category', inplace=True)
df_tech_new_tags = pd.read_excel("test.xlsx", index=False)
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    if pd.isnull(row["product_tags"]):
        print(index)
        product_category = str(row["Product category"])  # Convert to string
        # Get the tags for the product category
        if product_category in filter_dataframe.index:
            category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
            print(row["Product_Name"])
            url = row["url"]
            website_text = url_text_dict.get(url, "")
            print(row["Product category"])
            print(category_tags)
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
                },
                {
                    "role": "user",
                    "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
                }
            ]
            response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=messages
                # response_format={ "type": "json_object" }
            )
            tags = response.choices[0].message.content.strip()
            df_tech_new_tags.loc[index, "product_tags"] = tags
            # print(row["Product_Name"])
            print(tags)
            print("______________")
        else:
            print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/45:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
# df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# # Set 'category' as the index of filter_dataframe for easier lookup
# filter_dataframe.set_index('category', inplace=True)
df_tech_new_tags = pd.read_excel("test.xlsx")
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    if pd.isnull(row["product_tags"]):
        print(index)
        product_category = str(row["Product category"])  # Convert to string
        # Get the tags for the product category
        if product_category in filter_dataframe.index:
            category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
            print(row["Product_Name"])
            url = row["url"]
            website_text = url_text_dict.get(url, "")
            print(row["Product category"])
            print(category_tags)
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
                },
                {
                    "role": "user",
                    "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
                }
            ]
            response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=messages
                # response_format={ "type": "json_object" }
            )
            tags = response.choices[0].message.content.strip()
            df_tech_new_tags.loc[index, "product_tags"] = tags
            # print(row["Product_Name"])
            print(tags)
            print("______________")
        else:
            print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/46:
# Create a new column to store the tags for each product
# df_tech_new_tags = pd.read_csv("./data/tags_30_11.csv", usecols=["Product_Name", "Product category", "url", "image_url"])
# df_tech_new_tags = df
filter_dataframe = pd.read_csv(file_path_filter_dataframe_csv, usecols=["category", "product type", "technology", "application"])

# Set 'category' as the index of filter_dataframe for easier lookup
filter_dataframe.set_index('category', inplace=True)
df_tech_new_tags = pd.read_excel("test.xlsx")
# Iterate over the rows of the dataframe
for index, row in df_tech_new_tags.iterrows():
    if pd.isnull(row["product_tags"]):
        print(index)
        product_category = str(row["Product category"])  # Convert to string
        # Get the tags for the product category
        if product_category in filter_dataframe.index:
            category_tags = filter_dataframe.loc[product_category].to_dict()  # Use a different variable
            print(row["Product_Name"])
            url = row["url"]
            website_text = url_text_dict.get(url, "")
            print(row["Product category"])
            print(category_tags)
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that generates relevant tags for products. Your responses should be a comma-separated list of tags. For example tag1, tag2, tag3, etc..."
                },
                {
                    "role": "user",
                    "content": f"For the product named '{row['Product_Name']}' in the category ' {row['Product category']} ' with the following description: '{website_text}', please select up to 2 tags from each top-level category from the provided list: {category_tags}. The top-level categories include 'Product Type', 'Technology', and 'Application'. The maximum total number of tags is 6, but you are not required to use all 6. Your response should be a comma-separated list of tags. Please note: avoid using 'tag' as a tag, and do not use the top-level category names as tags."
                }
            ]
            response = client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=messages
                # response_format={ "type": "json_object" }
            )
            tags = response.choices[0].message.content.strip()
            df_tech_new_tags.loc[index, "product_tags"] = tags
            # print(row["Product_Name"])
            print(tags)
            print("______________")
        else:
            print(f"Category '{product_category}' not found in filter_dataframe")

# save the dataframe as csv
df_tech_new_tags.to_csv(new_file_path_splitted_dataframe_csv, index=False)
df_tech_new_tags.to_excel(new_file_path_splitted_dataframe_excel, index=False)
67/47:
import ast
def extract_tags(tags_str):
    tags_obj = ast.literal_eval(tags_str)
    if isinstance(tags_obj, dict):
        tags_values = tags_obj.values()
    elif isinstance(tags_obj, tuple):
        tags_values = tags_obj
    else:
        raise ValueError(f"Unexpected type {type(tags_obj)}: {tags_obj}")
    
    tags_list = []
    for value in tags_values:
        if isinstance(value, list):
            tags_list.append(', '.join(map(str.strip, value)))
        elif isinstance(value, dict):
            tags_list.append(str(value))
        else:
            tags_list.append(value.strip())
    return ', '.join(tags_list)
67/48:
df_with_many_tags = pd.read_csv(new_file_path_splitted_dataframe_csv)
print(len(df_with_many_tags))
# df_with_many_tags['product_tags'] = df_with_many_tags['product_tags'].apply(extract_tags)
# combine all the tags with same product_name into one row. Let the name be the same as the first row, but for tags, combine them into two lists of tags. For the category, combine them into a list of categories. For website_url, image_url, choose the first one. 
df_with_many_tags = df_with_many_tags.groupby('Product_Name').agg({
    'product_tags': lambda x: ', '.join(set(map(lambda y: str(y).strip(), x))),
    'Product category': lambda x: ', '.join(map(lambda y: str(y).strip(), x)),
    'url': 'first',
    'image_url': 'first'
    
}).reset_index()
print(len(df_with_many_tags))

# save to excel
df_with_many_tags.to_excel(new_file_path_grouped_dataframe_excel, index=False)
df_with_many_tags.to_csv(new_file_path_grouped_dataframe_csv, index=False)
67/49:
# if dataframe do not have column "is_software" then merge the column from software dataframe to the dataframe on the product name column
df_grouped_tags = pd.read_csv(new_file_path_grouped_dataframe_csv)
#check if the column is_software exists
if "is_software" not in df_grouped_tags.columns:
    # read the software dataframe
    df_software = pd.read_csv("data/tags_27_11.csv")
    # merge the column is_software to the dataframe on the product name column
    df_grouped_tags = pd.merge(df_grouped_tags, df_software[["Product_Name", "is_software"]], on="Product_Name", how="left")
    # save the dataframe
    print(df_grouped_tags.head())
67/50:
filter_ = pd.read_excel("filter_tags.xlsx")
filter_.to_csv("filter_tags.csv", index=False)
70/1:
from scapy.all import ARP, Ether, srp
from getmac import get_mac_address

def scan_network():
    # Get the IP of the default network interface
    default_gateway_ip = get_mac_address()

    # Create an ARP request packet to get the MAC addresses of all devices on the network
    arp = ARP(pdst=default_gateway_ip)
    ether = Ether(dst="ff:ff:ff:ff:ff:ff")
    packet = ether/arp

    # Send the packet on the network and capture the responses
    result = srp(packet, timeout=3, verbose=0)[0]

    # Print the IP and MAC address of each responding device
    devices = []
    for sent, received in result:
        devices.append({'ip': received.psrc, 'mac': received.hwsrc})

    return devices

# Example usage
devices = scan_network()
for device in devices:
    print(f"IP: {device['ip']}, MAC: {device['mac']}")
 544:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv")
df_without_datasheet = df.read_csv("data/styrings_data/ai_produkter.csv")

merged = pd.merge(df_without_datasheet, df_without_datasheet, on="Product_Name", how="left")
merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 545:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv")
df_without_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv")

merged = pd.merge(df_without_datasheet, df_without_datasheet, on="Product_Name", how="left")
merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 546:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","url","Data sheets","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","url","General text","Features","Technical Specifications"])

merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")
merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 547:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","url","Data sheets","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])

merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")
merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 548:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")
merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 549:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")
merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 550:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

merged = pd.merge(df_with_datasheet, df_without_datasheet, on=["Product_Name","General text","Features","Technical Specifications"] , how="left")
merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 551:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
merged = df_with_datasheet.combine_first(df_without_datasheet)

# Reset the index
merged.reset_index(inplace=True)

merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 552:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
merged = df_with_datasheet.combine_first(df_without_datasheet)

# Reset the index
# merged.reset_index(inplace=True)

merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 553:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_without_datasheet.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
merged = df_with_datasheet.combine_first(df_without_datasheet)

# Reset the index
# merged.reset_index(inplace=True)

merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 554:
df_without_datasheet = pd.read_csv("data/data_sheets_web_scraped_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/ai_produkter.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_without_datasheet.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
merged = df_with_datasheet.combine_first(df_without_datasheet)

# Reset the index
merged.reset_index(inplace=True)

merged.to_csv("data/styrings_data/alle_produkter_ai.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai.xlsx", index=False)
 555:
df_data_sheet_manual = pd.read_csv("data/styrings_data/produkter_alle.xlsx")
# only keep the one with url ends with .pdf
df_data_sheet_manual = df_data_sheet_manual[df_data_sheet_manual['url'].str.endswith('.pdf')]
print(len(df_data_sheet_manual))
 556:
df_data_sheet_manual = pd.read_csv("data/styrings_data/ai_produkter.csv")
# only keep the one with url ends with .pdf
df_data_sheet_manual = df_data_sheet_manual[df_data_sheet_manual['url'].str.endswith('.pdf')]
print(len(df_data_sheet_manual))
 557:
import pandas as pd
import json

def process_text(df_with_text, output_path):
    for index, row in df_with_text.iterrows():
            if (row["Scraped Text"]) != "":
                print(index,"product_name:", row["Product_Name"])

                input_text = row["Scraped Text"]
                output_text = generate_text(input_text)
                print(output_text)
                # Parse the JSON data
                data = json.loads(output_text)
                data = {k.lower(): v for k, v in data.items()}

                block1 = data.get('general text', '')
                block2 = data.get('key features', '')
                block3 = data.get('technical specifications', '')
                print(block1)
                print(block2)
                print(block3)
                # Add the blocks to the DataFrame
                df_with_text.loc[index, 'General text'] = str(block1)
                df_with_text.loc[index, 'Features'] = str(block2)
                df_with_text.loc[index, 'Technical Specifications'] = str(block3)

    return df_with_text
 558:
df_data_sheet_manual = pd.read_csv("data/styrings_data/ai_produkter.csv")
# only keep the one with url ends with .pdf
df_data_sheet_manual = df_data_sheet_manual[df_data_sheet_manual['url'].str.endswith('.pdf')]
print(len(df_data_sheet_manual))
 559:
import pandas as pd
import json

def process_text(df_with_text):
    for index, row in df_with_text.iterrows():
            if (row["Scraped Text"]) != "":
                print(index,"product_name:", row["Product_Name"])

                input_text = row["Scraped Text"]
                output_text = generate_text(input_text)
                print(output_text)
                # Parse the JSON data
                data = json.loads(output_text)
                data = {k.lower(): v for k, v in data.items()}

                block1 = data.get('general text', '')
                block2 = data.get('key features', '')
                block3 = data.get('technical specifications', '')
                print(block1)
                print(block2)
                print(block3)
                # Add the blocks to the DataFrame
                df_with_text.loc[index, 'General text'] = str(block1)
                df_with_text.loc[index, 'Features'] = str(block2)
                df_with_text.loc[index, 'Technical Specifications'] = str(block3)

    return df_with_text
 560:
df_data_sheet_manual_ai = process_text(df_data_sheet_manual)
df_data_sheet_manual_ai.to_csv("data/styrings_data/df_data_sheet_manual_ai.csv", index=False)
 561:
def scrape_pdf_text(url):
    # Download the PDF file
    response = requests.get(url)
    with open('temp.pdf', 'wb') as f:
        f.write(response.content)

    # Open the PDF file
    with pdfplumber.open('temp.pdf') as pdf:
        # Extract text from each page
        text = ''
        in_features_section = False
        if len(pdf.pages) > 10:
            return None
        for i, page in enumerate(pdf.pages):
            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom
            if i == len(pdf.pages) - 1:
                crop_box = (0, 0, page.width, page.height - 70)
                page = page.crop(crop_box)

            page_text = page.extract_text().split('\n')
                        
            for line in page_text:
                if not in_features_section:
                    line = line.replace('', '')  # Remove  symbol
                    if line.startswith(''):
                        text += '\n' + line  # Add bullet point to new line
                    else:
                        text += line + '\n'

    # Remove the temporary PDF file
    os.remove('temp.pdf')

    return text

# Example usage
data_sheets = pd.read_csv("data/data_sheets.csv")
data_sheets = data_sheets[data_sheets['Data sheets'].notna()]
data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]
product_datasheet = data_sheets["Data sheets"].iloc[2]
pdf_text = scrape_pdf_text(product_datasheet)
print(pdf_text)
print(product_datasheet)
 562:
df_data_sheet_manual['Scraped Text'] = df_data_sheet_manual['url'].apply(scrape_pdf_text)
df_data_sheet_manual_ai = process_text(df_data_sheet_manual)
df_data_sheet_manual_ai.to_csv("data/styrings_data/df_data_sheet_manual_ai.csv", index=False)
 563:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)

# Reset the index
merged.reset_index(inplace=True)
merged.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 564:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)

# Reset the index

merged.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 565:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)
df_with_datasheet.update(df_data_sheet_manual_ai)


# Reset the index

df_with_datasheet.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
df_with_datasheet.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 566:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)
df_data_sheet_manual_ai.update(df_with_datasheet)


# Reset the index

df_with_datasheet.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
df_with_datasheet.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 567:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)
df_data_sheet_manual_ai.update(df_with_datasheet)


# Reset the index

df_data_sheet_manual_ai.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
df_data_sheet_manual_ai.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 568:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)
df_with_datasheet.update(df_data_sheet_manual_ai)


# Reset the index

df_with_datasheet.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
df_with_datasheet.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 569:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)
merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)


# Reset the index

df_with_datasheet.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
df_with_datasheet.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 570:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)
merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)


# Reset the index

merged.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 571:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

# df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)
# df_with_datasheet.set_index('Product_Name', inplace=True)

# merged = pd.merge(df_with_datasheet, df_without_datasheet, on="Product_Name", how="left")

# Combine the DataFrames
# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)
merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)


# Reset the index

merged.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 572:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

# Merge the DataFrames
merged = pd.merge(df_with_datasheet, df_data_sheet_manual_ai, on="Product_Name", how="left")

merged.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 573:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

# Merge the DataFrames
merged = pd.merge(df_with_datasheet, df_data_sheet_manual_ai, on="Product_Name", how="right")

merged.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 574:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

# Merge the DataFrames
merged = pd.merge(df_with_datasheet, df_data_sheet_manual_ai, on="Product_Name", how="inner")

merged.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 575:
df_data_sheet_manual_ai = pd.read_csv("data/styrings_data/df_data_sheet_manual_ai.csv", usecols=["Product_Name","General text","Features","Technical Specifications"])
df_with_datasheet = pd.read_csv("data/styrings_data/alle_produkter_ai.csv", usecols=["Product_Name","Data sheets","url","General text","Features","Technical Specifications"])

# Merge the DataFrames
merged = pd.concat([df_with_datasheet, df_data_sheet_manual_ai])

# Remove duplicates based on 'Product_Name', keeping the last occurrence
merged.drop_duplicates(subset='Product_Name', keep='last', inplace=True)

merged.to_csv("data/styrings_data/alle_produkter_ai_test.csv", index=False)
merged.to_excel("data/styrings_data/alle_produkter_ai_test.xlsx", index=False)
 576: %history -f scraping.py
 577: %history
 578: _ih[-15:]
 579: %history -g -f notebook_file.ipynb
