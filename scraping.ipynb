{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import pdfplumber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrping av del 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " EK80 Portable EntryThe EK80 Portable Entry is all you need for ecosystem monitoring and analysis in one package â€“ at a reasonable price.  EK80 Portable Entry bundle comprises:   This is a complete rugged echo sounder with a built-in computer, GPS, and WiFi for remote operation. The core of the product is the WBT Mini, a compact EK80 wideband transceiver with four channels. The Entry bundle uses three of these channels for the 38 kHz split beam, while the last channel is used for the 200 kHz single beam.    The EK80 Portable entry is provided with the ES38-18/200-18 transducer. This is a dual-frequency transducer combining 38 and 200 kHz in one housing. The beamwidth is 18 degrees at both frequencies to secure a large sampling volume in shallow water. Combining high and low frequencies is ideal for mapping ecosystem components and separating fish from plankton.   Key features \n",
      "https://www.kongsberg.com/maritime/products/ocean-science/ocean-science/es_scientific/sci_es_transceivers/ek80-portable-entry/\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "\n",
    "def extract_text(url):\n",
    "    # Get the HTML of the page\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the divs with the specified classes\n",
    "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
    "\n",
    "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
    "    text = []\n",
    "    for div in divs:\n",
    "        for child in div.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                text.append(child.strip())\n",
    "            elif child.name in ['ul']:\n",
    "                break\n",
    "            else:\n",
    "                text.append(child.get_text(strip=True))\n",
    "    text_string = ' '.join(text)\n",
    "        \n",
    "    return text_string\n",
    "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
    "number = 23\n",
    "\n",
    "print(extract_text(df[\"url\"][number]))\n",
    "\n",
    "print(df[\"url\"][number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping av data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datasheets(url):\n",
    "    # Get the HTML of the page\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the div with the class \"Downloads\"\n",
    "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
    "\n",
    "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
    "    if downloads_div:\n",
    "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
    "\n",
    "        # If the subtitle is found, find the next sibling section\n",
    "        if datasheets_subtitle:\n",
    "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
    "\n",
    "            # If the section is found, find all links within this section\n",
    "            if datasheets_section:\n",
    "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
    "\n",
    "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
    "                if datasheet_links:\n",
    "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
    "                    return ', '.join(links)\n",
    "        else:\n",
    "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
    "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
    "    \n",
    "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
    "            for link in datasheet_links:\n",
    "                href = link.get('href')\n",
    "                if href.endswith('.pdf'):\n",
    "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
    "    # If the section, the div, or the links are not found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
    "\n",
    "# Add a new column \"Data sheets\" to the dataframe\n",
    "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
    "\n",
    "# only keep column url, product name and data sheets\n",
    "df = df[['url', 'Product_Name', 'Data sheets']]\n",
    "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
    "df.to_excel(\"data/data_sheets.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping av PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "\n",
    "def scrape_pdf_text(url):\n",
    "    # Download the PDF file\n",
    "    response = requests.get(url)\n",
    "    with io.BytesIO(response.content) as f:\n",
    "        # Open the PDF file\n",
    "        reader = PdfReader(f)\n",
    "\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
    "pdf_text = scrape_pdf_text(pdf_url) \n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecationError",
     "evalue": "reader.getPage(pageNumber) is deprecated and was removed in PyPDF2 3.0.0. Use reader.pages[page_number] instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeprecationError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m pdf_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m pdf_text \u001b[39m=\u001b[39m scrape_pdf_text(pdf_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(pdf_text)\n",
      "\u001b[1;32m/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(reader\u001b[39m.\u001b[39mpages)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         page \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39;49mgetPage(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m page\u001b[39m.\u001b[39mextractText()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgenstensrud/Documents/tags_creator/scraping.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/miniconda3/envs/tonality_test/lib/python3.11/site-packages/PyPDF2/_reader.py:476\u001b[0m, in \u001b[0;36mPdfReader.getPage\u001b[0;34m(self, pageNumber)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetPage\u001b[39m(\u001b[39mself\u001b[39m, pageNumber: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PageObject:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    .. deprecated:: 1.28.0\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m \u001b[39m        Use :code:`reader.pages[page_number]` instead.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m     deprecation_with_replacement(\n\u001b[1;32m    477\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mreader.getPage(pageNumber)\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mreader.pages[page_number]\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m3.0.0\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    478\u001b[0m     )\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_page(pageNumber)\n",
      "File \u001b[0;32m~/miniconda3/envs/tonality_test/lib/python3.11/site-packages/PyPDF2/_utils.py:369\u001b[0m, in \u001b[0;36mdeprecation_with_replacement\u001b[0;34m(old_name, new_name, removed_in)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeprecation_with_replacement\u001b[39m(\n\u001b[1;32m    364\u001b[0m     old_name: \u001b[39mstr\u001b[39m, new_name: \u001b[39mstr\u001b[39m, removed_in: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m3.0.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    Raise an exception that a feature was already removed, but has a replacement.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     deprecation(DEPR_MSG_HAPPENED\u001b[39m.\u001b[39;49mformat(old_name, removed_in, new_name))\n",
      "File \u001b[0;32m~/miniconda3/envs/tonality_test/lib/python3.11/site-packages/PyPDF2/_utils.py:351\u001b[0m, in \u001b[0;36mdeprecation\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeprecation\u001b[39m(msg: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mraise\u001b[39;00m DeprecationError(msg)\n",
      "\u001b[0;31mDeprecationError\u001b[0m: reader.getPage(pageNumber) is deprecated and was removed in PyPDF2 3.0.0. Use reader.pages[page_number] instead."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "\n",
    "def scrape_pdf_text(url):\n",
    "    # Download the PDF file\n",
    "    response = requests.get(url)\n",
    "    with io.BytesIO(response.content) as f:\n",
    "        # Open the PDF file\n",
    "        reader = PdfReader(f)\n",
    "\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        for i in range(len(reader.pages)):\n",
    "            page = reader.getPage(i)\n",
    "            text += page.extractText()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
    "pdf_text = scrape_pdf_text(pdf_url)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf\n"
     ]
    }
   ],
   "source": [
    "def scrape_pdf_text(url):\n",
    "    # Download the PDF file\n",
    "    response = requests.get(url)\n",
    "    with open('temp.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Open the PDF file\n",
    "    with pdfplumber.open('temp.pdf') as pdf:\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        in_features_section = False\n",
    "        if len(pdf.pages) > 6:\n",
    "            return None\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
    "            if i == len(pdf.pages) - 1:\n",
    "                crop_box = (0, 0, page.width, page.height - 70)\n",
    "                page = page.crop(crop_box)\n",
    "\n",
    "            page_text = page.extract_text().split('\\n')\n",
    "                        \n",
    "            for line in page_text:\n",
    "                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):\n",
    "                    in_features_section = True\n",
    "                elif re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    \n",
    "                    break\n",
    "\n",
    "                if not in_features_section:\n",
    "                    line = line.replace('Â®', '')  # Remove Â® symbol\n",
    "                    if line.startswith('â€¢'):\n",
    "                        text += '\\n' + line  # Add bullet point to new line\n",
    "                    else:\n",
    "                        text += line + '\\n'\n",
    "\n",
    "    # Remove the temporary PDF file\n",
    "    os.remove('temp.pdf')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
    "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
    "pdf_text = scrape_pdf_text(product_datasheet)\n",
    "print(pdf_text)\n",
    "print(product_datasheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
    "data_sheets = data_sheets.head(40)\n",
    "\n",
    "# Filter rows with valid data sheet URLs\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
    "\n",
    "# Scrape the text from each data sheet and save it as a new column\n",
    "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from openai._client import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    output_text = response.choices[0].message.content.strip()\n",
    "    return output_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KONGSBERG AUTONOMOUS UNDERWATER VEHICLES\n",
      "\n",
      "**HUGIN AUV System**\n",
      "\n",
      "HUGIN is the most successful commercial Autonomous Underwater Vehicle (AUV) available. It combines IHO quality positioning with the\n"
     ]
    }
   ],
   "source": [
    "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
    "\n",
    "input_text = (df_with_text[\"Scraped Text\"][2])\n",
    "output_text = generate_text(input_text)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE INSIGHT\n",
      "The digital ocean toolbox\n",
      "Seamless data acquisition, fusion, processing, visualization,\n",
      "contextualization and management of ocean data.\n",
      "Kongsberg Blue Insight is an open and flexible digital infrastructure\n",
      "product designed to meet the increasing demand for accurate information\n",
      "from the oceans, focusing on internet security and seamless integration.\n",
      "The Blue Insight automates the data stream from a wide range of sensors\n",
      "KEY BENEFITS: and platform types and visualizes them in a common framework. The\n",
      "solution facilitates modern data processing and machine learning, provid-\n",
      "\n",
      "â€¢ One-stop solution that ing new understanding and in-depth knowledge.provides real time\n",
      "overview across platforms Through a range of dedicated modules, Blue Insight offers flexibil-\n",
      "and data types\n",
      "ity, security and robustness required by any ocean data stakeholder:\n",
      "\n",
      "â€¢ Flexible and scalable,scientific, observational, or commercial.\n",
      "suitable for both small\n",
      "and large organizations\n",
      "\n",
      "â€¢ Open ecosystem buildingon FAIR data principles\n",
      "\n",
      "â€¢ Allows for deploymentof customized machine\n",
      "learning algorithms\n",
      "\n",
      "â€¢ Cyber secure solutionkongsberg.com/maritime October 2021\n",
      "BLUE\n",
      "INSIGHT\n",
      "BLUE INSIGHT CORE\n",
      "The Blue Insight Core module provides solutions for secure interactions between sensor platforms,\n",
      "cloud storage and data management enabled by the industrialized Internet of Things modules found in\n",
      "the Kognifai digital ecosystem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tonality_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
