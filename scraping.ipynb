{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import pdfplumber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrping av del 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "\n",
    "def extract_text(url):\n",
    "    # Get the HTML of the page\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the divs with the specified classes\n",
    "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
    "\n",
    "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
    "    text = []\n",
    "    for div in divs:\n",
    "        for child in div.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                text.append(child.strip())\n",
    "            elif child.name in ['ul']:\n",
    "                break\n",
    "            else:\n",
    "                text.append(child.get_text(strip=True))\n",
    "    text_string = ' '.join(text)\n",
    "        \n",
    "    return text_string\n",
    "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
    "number = 23\n",
    "\n",
    "print(extract_text(df[\"url\"][number]))\n",
    "\n",
    "print(df[\"url\"][number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping av data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datasheets(url):\n",
    "    # Get the HTML of the page\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the div with the class \"Downloads\"\n",
    "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
    "    # downloads_span = soup.find('span', class_=\"Downloads_itemName\")\n",
    "\n",
    "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
    "    if downloads_div:\n",
    "        # datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|datasheet|data- and product sheets|Brochure\", re.I))\n",
    "        datasheets_itemname = downloads_div.find('span', class_=\"Downloads__itemName\", string=re.compile(\"data sheet|Datasheet|data- and product sheets|Brochure\", re.I))\n",
    "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
    "        \n",
    "        # If the subtitle is found, find the next sibling section\n",
    "        if datasheets_subtitle:\n",
    "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
    "\n",
    "            # If the section is found, find all links within this section\n",
    "            if datasheets_section:\n",
    "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
    "\n",
    "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
    "                if datasheet_links:\n",
    "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
    "                    return ', '.join(links)        \n",
    "\n",
    "        elif datasheets_itemname:\n",
    "            datasheets_section = datasheets_itemname.find_parent().find_parent()\n",
    "\n",
    "            if datasheets_section:\n",
    "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
    "                datasheet_links_with_datasheet = [link for link in datasheet_links if link.get_text(strip=True) == \"Datasheet\"]\n",
    "                if datasheet_links_with_datasheet:\n",
    "                    return [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links_with_datasheet if link.get('href').endswith('.pdf')]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_downloads(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Dictionary to store the result\n",
    "    section_links = {}\n",
    "\n",
    "    # Find all section subtitles and corresponding links\n",
    "    for subtitle in soup.find_all(\"h3\", class_=\"Section__subtitle\"):\n",
    "        section_name = subtitle.text.strip()\n",
    "        links = [a['href'] for a in subtitle.find_next_sibling(\"ul\").find_all(\"a\", href=True)]\n",
    "        section_links[section_name] = links\n",
    "\n",
    "    return section_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
    "\n",
    "df[\"downloads\"] = df[\"url\"].apply(find_downloads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['url', 'Product_Name', \"downloads\"]]\n",
    "\n",
    "df.to_excel(\"data/downloads_files.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
    "\n",
    "# Add a new column \"Data sheets\" to the dataframe\n",
    "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
    "df[\"downloads\"] = df[\"url\"].apply(find_downloads)\n",
    "\n",
    "# only keep column url, product name and data sheets\n",
    "df = df[['url', 'Product_Name', 'Data sheets', \"downloads\"]]\n",
    "# df.to_csv(\"data/data_sheets_temp_4.csv\", index=False)\n",
    "# df.to_excel(\"data/data_sheets_temp_4.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping av PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "\n",
    "def scrape_pdf_text(url):\n",
    "    # Download the PDF file\n",
    "    response = requests.get(url)\n",
    "    with io.BytesIO(response.content) as f:\n",
    "        # Open the PDF file\n",
    "        reader = PdfReader(f)\n",
    "\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
    "pdf_text = scrape_pdf_text(pdf_url) \n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "\n",
    "def scrape_pdf_text(url):\n",
    "    # Download the PDF file\n",
    "    response = requests.get(url)\n",
    "    with io.BytesIO(response.content) as f:\n",
    "        # Open the PDF file\n",
    "        reader = PdfReader(f)\n",
    "\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        for i in range(len(reader.pages)):\n",
    "            page = reader.getPage(i)\n",
    "            text += page.extractText()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
    "pdf_text = scrape_pdf_text(pdf_url)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pdf_text(url):\n",
    "    # Download the PDF file\n",
    "    response = requests.get(url)\n",
    "    with open('temp.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Open the PDF file\n",
    "    with pdfplumber.open('temp.pdf') as pdf:\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        in_features_section = False\n",
    "        if len(pdf.pages) > 6:\n",
    "            return None\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
    "            if i == len(pdf.pages) - 1:\n",
    "                crop_box = (0, 0, page.width, page.height - 70)\n",
    "                page = page.crop(crop_box)\n",
    "\n",
    "            page_text = page.extract_text().split('\\n')\n",
    "                        \n",
    "            for line in page_text:\n",
    "                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):\n",
    "                    in_features_section = True\n",
    "                elif re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    \n",
    "                    break\n",
    "\n",
    "                if not in_features_section:\n",
    "                    line = line.replace('®', '')  # Remove ® symbol\n",
    "                    if line.startswith('•'):\n",
    "                        text += '\\n' + line  # Add bullet point to new line\n",
    "                    else:\n",
    "                        text += line + '\\n'\n",
    "\n",
    "    # Remove the temporary PDF file\n",
    "    os.remove('temp.pdf')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
    "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
    "pdf_text = scrape_pdf_text(product_datasheet)\n",
    "print(pdf_text)\n",
    "print(product_datasheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
    "data_sheets = data_sheets.head(40)\n",
    "\n",
    "# Filter rows with valid data sheet URLs\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
    "\n",
    "# Scrape the text from each data sheet and save it as a new column\n",
    "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from openai._client import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    output_text = response.choices[0].message.content.strip()\n",
    "    return output_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
    "\n",
    "input_text = (df_with_text[\"Scraped Text\"][2])\n",
    "output_text = generate_text(input_text)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tonality_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
