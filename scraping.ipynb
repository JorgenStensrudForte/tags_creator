{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import pdfplumber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrping av del 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " EK80 Portable EntryThe EK80 Portable Entry is all you need for ecosystem monitoring and analysis in one package – at a reasonable price.  EK80 Portable Entry bundle comprises:   This is a complete rugged echo sounder with a built-in computer, GPS, and WiFi for remote operation. The core of the product is the WBT Mini, a compact EK80 wideband transceiver with four channels. The Entry bundle uses three of these channels for the 38 kHz split beam, while the last channel is used for the 200 kHz single beam.    The EK80 Portable entry is provided with the ES38-18/200-18 transducer. This is a dual-frequency transducer combining 38 and 200 kHz in one housing. The beamwidth is 18 degrees at both frequencies to secure a large sampling volume in shallow water. Combining high and low frequencies is ideal for mapping ecosystem components and separating fish from plankton.   Key features \n",
      "https://www.kongsberg.com/maritime/products/ocean-science/ocean-science/es_scientific/sci_es_transceivers/ek80-portable-entry/\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "\n",
    "def extract_text(url):\n",
    "    # Get the HTML of the page\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the divs with the specified classes\n",
    "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
    "\n",
    "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
    "    text = []\n",
    "    for div in divs:\n",
    "        for child in div.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                text.append(child.strip())\n",
    "            elif child.name in ['ul']:\n",
    "                break\n",
    "            else:\n",
    "                text.append(child.get_text(strip=True))\n",
    "    text_string = ' '.join(text)\n",
    "        \n",
    "    return text_string\n",
    "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
    "number = 23\n",
    "\n",
    "print(extract_text(df[\"url\"][number]))\n",
    "\n",
    "print(df[\"url\"][number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping av data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datasheets(url):\n",
    "    # Get the HTML of the page\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the div with the class \"Downloads\"\n",
    "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
    "\n",
    "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
    "    if downloads_div:\n",
    "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
    "\n",
    "        # If the subtitle is found, find the next sibling section\n",
    "        if datasheets_subtitle:\n",
    "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
    "\n",
    "            # If the section is found, find all links within this section\n",
    "            if datasheets_section:\n",
    "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
    "\n",
    "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
    "                if datasheet_links:\n",
    "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
    "                    return ', '.join(links)\n",
    "        else:\n",
    "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
    "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
    "    \n",
    "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
    "            for link in datasheet_links:\n",
    "                href = link.get('href')\n",
    "                if href.endswith('.pdf'):\n",
    "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
    "    # If the section, the div, or the links are not found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
    "\n",
    "# Add a new column \"Data sheets\" to the dataframe\n",
    "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
    "\n",
    "# only keep column url, product name and data sheets\n",
    "df = df[['url', 'Product_Name', 'Data sheets']]\n",
    "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
    "df.to_excel(\"data/data_sheets.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping av PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pdf_text(url):\n",
    "    print(url)\n",
    "    # Download the PDF file\n",
    "    response = requests.get(url)\n",
    "    with open('temp.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Open the PDF file\n",
    "    with pdfplumber.open('temp.pdf') as pdf:\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        in_features_section = False\n",
    "        if len(pdf.pages) > 10:\n",
    "            return None\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
    "            if i == len(pdf.pages) - 1:\n",
    "                crop_box = (0, 0, page.width, page.height - 70)\n",
    "                page = page.crop(crop_box)\n",
    "\n",
    "            page_text = page.extract_text().split('\\n')\n",
    "                        \n",
    "            for line in page_text:\n",
    "                if not in_features_section:\n",
    "                    line = line.replace('®', '')  # Remove ® symbol\n",
    "                    if line.startswith('•'):\n",
    "                        text += '\\n' + line  # Add bullet point to new line\n",
    "                    else:\n",
    "                        text += line + '\\n'\n",
    "\n",
    "    # Remove the temporary PDF file\n",
    "    os.remove('temp.pdf')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
    "data_sheets = data_sheets.head(40)\n",
    "\n",
    "# Filter rows with valid data sheet URLs\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
    "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
    "\n",
    "# Scrape the text from each data sheet and save it as a new column\n",
    "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find all elements with the specified class\n",
      "    elements = soup.find_all(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each element\n",
      "    rich_text = [element.get_text(strip=True) for element in elements]\n",
      "    rich_text_string = ' '.join(rich_text)\n",
      "\n",
      "    link = soup.find('a', href='#technicalInformation')\n",
      "\n",
      "    bullet_points = []\n",
      "    bullet_points_string = \"\"\n",
      "    # If the link was found, find the element it links to\n",
      "    if link is not None:\n",
      "        target_id = link['href'].lstrip('#')\n",
      "        target_element = soup.find(id=target_id)\n",
      "\n",
      "        # If the target element was found, get its text\n",
      "        if target_element is not None:\n",
      "            # Find all the list items in the target element\n",
      "            list_items = target_element.find_all('li')\n",
      "\n",
      "            # Extract the text of each list item\n",
      "            bullet_points = [li.get_text(strip=True) for li in list_items]\n",
      "            bullet_points_string = '\\n'.join(bullet_points)\n",
      "\n",
      "        \n",
      "    text = rich_text_string +\" \\n \"+ bullet_points_string\n",
      "    return text\n",
      "url_text_dict = {}\n",
      "extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find all elements with the specified class\n",
      "    elements = soup.find_all(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each element\n",
      "    rich_text = [element.get_text(strip=True) for element in elements]\n",
      "    rich_text_string = ' '.join(rich_text)\n",
      "\n",
      "    link = soup.find('a', href='#technicalInformation')\n",
      "\n",
      "    bullet_points = []\n",
      "    bullet_points_string = \"\"\n",
      "    # If the link was found, find the element it links to\n",
      "    if link is not None:\n",
      "        target_id = link['href'].lstrip('#')\n",
      "        target_element = soup.find(id=target_id)\n",
      "\n",
      "        # If the target element was found, get its text\n",
      "        if target_element is not None:\n",
      "            # Find all the list items in the target element\n",
      "            list_items = target_element.find_all('li')\n",
      "\n",
      "            # Extract the text of each list item\n",
      "            bullet_points = [li.get_text(strip=True) for li in list_items]\n",
      "            bullet_points_string = '\\n'.join(bullet_points)\n",
      "\n",
      "        \n",
      "    text = rich_text_string\n",
      "    return text\n",
      "url_text_dict = {}\n",
      "extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find all elements with the specified class\n",
      "    elements = soup.find_all(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each element and stop when encountering a heading\n",
      "    rich_text = []\n",
      "    for element in elements:\n",
      "        if element.find_next(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
      "            break\n",
      "        rich_text.append(element.get_text(strip=True))\n",
      "    rich_text_string = ' '.join(rich_text)\n",
      "        \n",
      "    text = rich_text_string\n",
      "    return text\n",
      "\n",
      "url_text_dict = {}\n",
      "extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find all elements with the specified class\n",
      "    elements = soup.find_all(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each element\n",
      "    rich_text = [element.get_text(strip=True) for element in elements]\n",
      "    rich_text_string = ' '.join(rich_text)\n",
      "        \n",
      "    text = rich_text_string\n",
      "    return text\n",
      "url_text_dict = {}\n",
      "extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name in ['h2', 'h3', 'ul']:\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "print(extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\"))\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name in ['ul']:\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "print(extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\"))\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "print(extract_text(df[\"url\"][0]))\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "print(extract_text(df[\"url\"][0]))\n",
      "print(df[\"url\"][0])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    img_url = None\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name == 'img':\n",
      "            img_url = child['src']\n",
      "        elif child.name == 'ul':\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string, img_url\n",
      "\n",
      "text, img_url = extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "print(text)\n",
      "print(img_url)\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    img_url = None\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name == 'picture':\n",
      "            img_url = child['src']\n",
      "        elif child.name == 'ul':\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string, img_url\n",
      "\n",
      "text, img_url = extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "print(text)\n",
      "print(img_url)\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "print(extract_text(df[\"url\"][0]))\n",
      "print(df[\"url\"][0])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    img_url = None\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name == 'picture':\n",
      "            img = child.find('img')\n",
      "            if img:\n",
      "                img_url = img['src']\n",
      "        elif child.name == 'ul':\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string, img_url\n",
      "\n",
      "text, img_url = extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "print(text)\n",
      "print(img_url)\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "print(extract_text(df[\"url\"][0]))\n",
      "print(df[\"url\"][0])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    img_url = None\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name == 'picture':\n",
      "            img = child.find('img')\n",
      "            if img:\n",
      "                img_url = img['src']\n",
      "        elif child.name in ['h2', 'h3', 'ul']:\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "            # Check for picture tag within other tags\n",
      "            picture = child.find('picture')\n",
      "            if picture:\n",
      "                img = picture.find('img')\n",
      "                if img and not img_url:  # Only set img_url if it hasn't been set yet\n",
      "                    img_url = img['src']\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string, img_url\n",
      "\n",
      "text, img_url = extract_text(df[\"url\"][0])\n",
      "print(text)\n",
      "print(img_url)\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    img_url = None\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name == 'picture':\n",
      "            img = child.find('img')\n",
      "            if img:\n",
      "                img_url = img['src']\n",
      "        elif child.name in ['h2', 'h3', 'ul']:\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "            # Check for picture tag within other tags\n",
      "            picture = child.find('picture')\n",
      "            if picture:\n",
      "                img = picture.find('img')\n",
      "                if img and not img_url:  # Only set img_url if it hasn't been set yet\n",
      "                    img_url = img['src']\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string, img_url\n",
      "\n",
      "text, img_url = extract_text(df[\"url\"][0])\n",
      "print(text)\n",
      "print(img_url)\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    img_url = None\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name == 'picture':\n",
      "            img = child.find('img')\n",
      "            if img:\n",
      "                img_url = img['src']\n",
      "        elif child.name in ['h2', 'h3', 'ul']:\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "            # Check for picture tag within other tags\n",
      "            picture = child.find('picture')\n",
      "            if picture:\n",
      "                img = picture.find('img')\n",
      "                if img and not img_url:  # Only set img_url if it hasn't been set yet\n",
      "                    img_url = img['src']\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string, img_url\n",
      "\n",
      "text, img_url = extract_text(df[\"url\"][0])\n",
      "print(text)\n",
      "print(img_url)\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "print(extract_text(df[\"url\"][0]))\n",
      "print(df[\"url\"][0])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name in ['ul']:\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "print(extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\"))\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "print(extract_text(df[\"url\"][0]))\n",
      "print(df[\"url\"][0])\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a ul tag is encountered\n",
      "    text = []\n",
      "    img_url = None\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name == 'picture':\n",
      "            img = child.find('img')\n",
      "            if img:\n",
      "                img_url = img['src']\n",
      "        elif child.name == 'ul':\n",
      "            break\n",
      "        else:\n",
      "            text.append(str(child))  # Append the string representation of the child, including HTML tags\n",
      "    text_string = ''.join(text)\n",
      "        \n",
      "    return text_string, img_url\n",
      "\n",
      "text, img_url = extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\")\n",
      "print(text)\n",
      "print(img_url)\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h3 tag is encountered\n",
      "    text = []\n",
      "    img_url = None\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name == 'picture':\n",
      "                img = child.find('img')\n",
      "                if img:\n",
      "                    img_url = img['src']\n",
      "            elif child.name == 'h3':\n",
      "                break\n",
      "            else:\n",
      "                text.append(str(child))  # Append the string representation of the child, including HTML tags\n",
      "    text_string = ''.join(text)\n",
      "        \n",
      "    return text_string, img_url\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h3 tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name == 'h3':\n",
      "                break\n",
      "            elif child.name != 'picture':  # Exclude the picture tag\n",
      "                text.append(str(child))  # Append the string representation of the child, including HTML tags\n",
      "    text_string = ''.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h3 tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child):\n",
      "                text.append(child.strip())\n",
      "            elif child.name == 'ul':\n",
      "                break\n",
      "            elif child.name != 'picture':  # Exclude the picture tag\n",
      "                text.append(str(child))  # Append the string representation of the child, including HTML tags\n",
      "    text_string = ''.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    div = soup.find(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for child in div.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name in ['ul']:\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "print(extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\"))\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the specified class\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of the div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for child in divs.children:\n",
      "        if isinstance(child, NavigableString):\n",
      "            text.append(child.strip())\n",
      "        elif child.name in ['ul']:\n",
      "            break\n",
      "        else:\n",
      "            text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "print(extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\"))\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "print(extract_text(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\"))\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        children = list(div.children)\n",
      "        for i in range(len(children)):\n",
      "            child = children[i]\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            elif i < len(children) - 1 and children[i + 1].name == 'ul':\n",
      "                continue  # Skip the heading before the ul tag\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 2\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 3\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 4\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 5\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 6\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 2\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 3\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 4\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 5\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 8\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 21\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 22\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=[\"RichtextArea ProductPage__richtext text-wrapper\", \"layout--2col wrapper\"])\n",
      "\n",
      "    # Extract the text of each child of each div until a h2, h3 or ul tag is encountered\n",
      "    text = []\n",
      "    for div in divs:\n",
      "        for child in div.children:\n",
      "            if isinstance(child, NavigableString):\n",
      "                text.append(child.strip())\n",
      "            elif child.name in ['ul']:\n",
      "                break\n",
      "            else:\n",
      "                text.append(child.get_text(strip=True))\n",
      "    text_string = ' '.join(text)\n",
      "        \n",
      "    return text_string\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(extract_text(df[\"url\"][number]))\n",
      "\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheet(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the span with the specified class and text\n",
      "    datasheet_span = soup.find('span', class_=\"Downloads__itemName\", text=\"Datasheet\")\n",
      "\n",
      "    # If the span is found, return the parent link's href attribute\n",
      "    if datasheet_span:\n",
      "        datasheet_link = datasheet_span.parent.get('href')\n",
      "        return datasheet_link\n",
      "\n",
      "    # If the span is not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(find_datasheet(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheet(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the span with the text \"Datasheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheet_span = downloads_div.find('span', class_=\"Downloads__itemName\", text=\"Datasheet\")\n",
      "\n",
      "        # If the span is found, return the parent link's href attribute\n",
      "        if datasheet_span:\n",
      "            datasheet_link = datasheet_span.parent.get('href')\n",
      "            return datasheet_link\n",
      "\n",
      "    # If the div or the span is not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(find_datasheet(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheet(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the span with the text \"Datasheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheet_span = downloads_div.find('span', class_=\"Downloads__itemName\", text=\"Datasheet\")\n",
      "\n",
      "        # If the span is found, return the parent link's href attribute\n",
      "        if datasheet_span:\n",
      "            datasheet_link = datasheet_span.parent.get('href')\n",
      "            return datasheet_link\n",
      "\n",
      "    # If the div or the span is not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 2\n",
      "\n",
      "print(find_datasheet(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find all links within this div\n",
      "    if downloads_div:\n",
      "        datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "        # If the links are found, return their href attributes\n",
      "        if datasheet_links:\n",
      "            return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 2\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find all links within this div\n",
      "    if downloads_div:\n",
      "        datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "        # If the links are found, return their href attributes\n",
      "        if datasheet_links:\n",
      "            return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 1\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find all links within this div\n",
      "    if downloads_div:\n",
      "        datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "        # If the links are found, return their href attributes\n",
      "        if datasheet_links:\n",
      "            return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 3\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_section = downloads_div.find('h3', class_=\"Section__subtitle\", text=\"Data sheets\").find_next_sibling()\n",
      "\n",
      "        # If the section is found, find all links within this section\n",
      "        if datasheets_section:\n",
      "            datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # If the links are found, return their href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 3\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=\"Data sheets\")\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 3\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=\"Data sheet\")\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 3\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=\"Data sheet\")\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 4\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=\"Data sheet\")\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 5\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=\"Data sheet\")\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 6\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 7\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 5\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 10\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 11\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 12\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 13\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd \n",
      "import re\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 13\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 14\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 15\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return their href attributes\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 15\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 17\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 15\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'product_name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 15\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 15\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 20\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 19\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links with the name \"Datasheet\" within the div\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\", string=\"Datasheet\")\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 19\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links with the name \"Datasheet\" within the div\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\", string=\"Datasheet\")\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 24\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links with the name \"Datasheet\" within the div\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\", string=\"Datasheet\")\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 25\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links with the name \"Datasheet\" within the div\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\", string=\"Datasheet\")\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the preceding sibling has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\", string=\"Datasheet\")\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, return the list of href attributes\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "            print(datasheet_links)\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "            if datasheet_links:\n",
      "                return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the div, the subtitle, the section, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "            print(datasheet_links)\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            downloads_section = soup.find('h2', class_=\"Section__title\", string=\"Downloads\")\n",
      "\n",
      "            # If the section is found, find the next sibling section\n",
      "            if downloads_section:\n",
      "                downloads_div = downloads_section.find_next_sibling()\n",
      "\n",
      "                # If the div is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "                if downloads_div:\n",
      "                    datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                    # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "                    datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "                    # If the links are found, return the list of href attributes\n",
      "                    if datasheet_links:\n",
      "                        return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            downloads_section = soup.find('h2', class_=\"Section__title\", string=\"Downloads\")\n",
      "\n",
      "            # If the section is found, find the next sibling section\n",
      "            if downloads_section:\n",
      "                downloads_div = downloads_section.find_next_sibling()\n",
      "\n",
      "                # If the div is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "                if downloads_div:\n",
      "                    datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "                    print(datasheet_links)\n",
      "\n",
      "                    # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "                    datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "                    # If the links are found, return the list of href attributes\n",
      "                    if datasheet_links:\n",
      "                        return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 23\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            downloads_section = soup.find('h2', class_=\"Section__title\", string=\"Downloads\")\n",
      "\n",
      "    # If the section is found, find the next sibling section\n",
      "    if downloads_section:\n",
      "        downloads_div = downloads_section.find_next_sibling()\n",
      "\n",
      "        # If the div is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "        if downloads_div:\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 83\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            downloads_section = soup.find('h2', class_=\"Section__title\", string=\"Downloads\")\n",
      "\n",
      "    # If the section is found, find the next sibling section\n",
      "    if downloads_section:\n",
      "        downloads_div = downloads_section.find_next_sibling()\n",
      "\n",
      "        # If the div is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "        if downloads_div:\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 82\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            downloads_section = soup.find('h2', class_=\"Section__title\", string=\"Downloads\")\n",
      "\n",
      "    # If the section is found, find the next sibling section\n",
      "    if downloads_section:\n",
      "        downloads_div = downloads_section.find_next_sibling()\n",
      "\n",
      "        # If the div is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "        if downloads_div:\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 81\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            downloads_section = soup.find('h2', class_=\"Section__title\", string=\"Downloads\")\n",
      "\n",
      "    # If the section is found, find the next sibling section\n",
      "    if downloads_section:\n",
      "        downloads_div = downloads_section.find_next_sibling()\n",
      "\n",
      "        # If the div is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "        if downloads_div:\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 82\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            downloads_section = soup.find('h2', class_=\"Section__title\", string=\"Downloads\")\n",
      "\n",
      "    # If the section is found, find the next sibling section\n",
      "    if downloads_section:\n",
      "        downloads_div = downloads_section.find_next_sibling()\n",
      "\n",
      "        # If the div is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "        if downloads_div:\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 83\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "        \n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 84\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "        \n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 85\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "        \n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the list of href attributes\n",
      "            if datasheet_links:\n",
      "                return [link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 83\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "        \n",
      "            # Filter the links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = [link for link in datasheet_links if link.find('span', class_=\"Downloads__itemName\", string=\"Datasheet\")]\n",
      "\n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 83\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 83\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return [\"https://www.kongsberg.com\" + href]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "number = 83\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "df = pd.read_csv(\"data/tags_30_11.csv\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return [\"https://www.kongsberg.com\" + href]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_excel(\"data/tags_12_04.xslx\")\n",
      "number = 83\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [\"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return [\"https://www.kongsberg.com\" + href]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "number = 83\n",
      "\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "import requests\n",
      "import PyPDF2\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with open('temp.pdf', 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfFileReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page_num in range(pdf_reader.numPages):\n",
      "            page = pdf_reader.getPage(page_num)\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://example.com/sample.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import PyPDF2\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with open('temp.pdf', 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfFileReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page_num in range(pdf_reader.numPages):\n",
      "            page = pdf_reader.getPage(page_num)\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://example.com/sample.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import PyPDF2\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with open('temp.pdf', 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page_num in range(pdf_reader.numPages):\n",
      "            page = pdf_reader.getPage(page_num)\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://example.com/sample.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import PyPDF2\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with open('temp.pdf', 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page_num in range(len(pdf_reader.pages)):\n",
      "            page = pdf_reader.pages[page_num]\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://example.com/sample.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import PyPDF2\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with open('temp.pdf', 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page_num in range(len(pdf_reader.pages)):\n",
      "            page = pdf_reader.pages[page_num]\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') if 'https://simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return [href if 'https://simrad.online' in href else \"https://www.kongsberg.com\" + href]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') if 'https://simrad.online' or \"https://www.simrad.online\"  in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return [href if 'https://simrad.online' or \"https://www.simrad.online\" in href else \"https://www.kongsberg.com\" + href]\n",
      "\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') if 'https://simrad.online' or \"https://www.simrad.online\"  in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return [href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href]\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    return [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return [href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href]\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "# loop through all data sheets\n",
      "df = pd.read_csv(\"data/data_sheets.csv\")\n",
      "df[\"text\"] = \"\"\n",
      "for i in range(len(df)):\n",
      "    print(i)\n",
      "    try:\n",
      "        df[\"text\"][i] = scrape_pdf_text(df[\"Data sheets\"][i])\n",
      "    except:\n",
      "        print(\"error\")\n",
      "import requests\n",
      "import PyPDF2\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with open('temp.pdf', 'rb') as f:\n",
      "        pdf_reader = PyPDF2.PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page_num in range(len(pdf_reader.pages)):\n",
      "            page = pdf_reader.pages[page_num]\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical' in page_text or \"TECHNICAL\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "pdf_urls = pdf_urls.apply(eval)\n",
      "pdf_urls = pdf_urls.explode()\n",
      "pdf_urls = pdf_urls.unique()\n",
      "pdf_urls = [url for url in pdf_urls if url.endswith('.pdf')]\n",
      "print(pdf_urls)\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls)\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls)\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls[0])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls[\"Data sheets\"][0]])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls[\"Data sheets\"][0])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls[\"Data sheets\"][0])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls[\"Data sheets\"][0])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls[\"Data sheets\"])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(data_sheets.columns)\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(data_sheets[\"Data sheets\"][0])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(data_sheets[\"Data sheets\"][1])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls[\"Data sheets\"][1])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls.columns)\n",
      "print(pdf_urls[\"Data sheets\"][1])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "pdf_urls = data_sheets['Data sheets'].dropna()\n",
      "print(pdf_urls.columns)\n",
      "# print(pdf_urls[\"Data sheets\"][1])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "data_sheets['Data sheets'].dropna()\n",
      "print(data_sheets.columns)\n",
      "# print(pdf_urls[\"Data sheets\"][1])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "print(data_sheets.columns)\n",
      "data_sheets['Data sheets'].dropna()\n",
      "print(data_sheets.columns)\n",
      "print(data_sheets[\"Data sheets\"][1])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "print(data_sheets[\"Data sheets\"][1])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "print(data_sheets[\"Data sheets\"])\n",
      "\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "print(data_sheets[\"Data sheets\"].iloc[0])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "print(data_sheets[\"Data sheets\"].iloc[0])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "print(data_sheets[\"Data sheets\"].iloc[2])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "# print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "print(data_sheets[\"Data sheets\"].iloc[2])\n",
      "pdf_url = 'https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Product_Name\"].iloc[2]\n",
      "print(data_sheets[\"Data sheets\"].iloc[2])\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Product_Name\"].iloc[2]\n",
      "print(product_datasheet)\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "                # Stop adding text once \"Technical Specification\" is encountered\n",
      "                break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "print(product_datasheet)\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            # if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "            #     # Stop adding text once \"Technical Specification\" is encountered\n",
      "            #     break\n",
      "            text += page_text\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "print(product_datasheet)\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            # if 'Technical highlights' in page_text or \"TECHNICAL HIGHLIGHTS\" in page_text:\n",
      "            #     # Stop adding text once \"Technical Specification\" is encountered\n",
      "            #     break\n",
      "            text += page_text\n",
      "    # iterate through the text and find the technical highlights and technical specification section and return the text before it\n",
      "    text = text.split(\"\\n\")\n",
      "    for i in range(len(text)):\n",
      "        if \"Technical highlights\" in text[i] or \"TECHNICAL HIGHLIGHTS\" in text[i]:\n",
      "            text = text[:i]\n",
      "            break\n",
      "        elif \"Technical specification\" in text[i] or \"TECHNICAL SPECIFICATION\" in text[i]:\n",
      "            text = text[:i]\n",
      "            break\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "print(product_datasheet)\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            text += page_text\n",
      "\n",
      "    # Split the text into lines and find the \"Technical highlights\" or \"Technical specification\" section\n",
      "    text = text.split(\"\\n\")\n",
      "    for i in range(len(text)):\n",
      "        if \"Technical highlights\" in text[i] or \"TECHNICAL HIGHLIGHTS\" in text[i]:\n",
      "            text = text[:i]\n",
      "            break\n",
      "        elif \"Technical specification\" in text[i] or \"TECHNICAL SPECIFICATION\" in text[i]:\n",
      "            text = text[:i]\n",
      "            break\n",
      "\n",
      "    # Join the lines with a newline character\n",
      "    text = \"\\n\".join(text)\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text()\n",
      "            text += page_text\n",
      "\n",
      "    # Split the text into lines and find the \"Technical highlights\" or \"Technical specification\" section\n",
      "    text = text.split(\"\\n\")\n",
      "    for i in range(len(text)):\n",
      "        if \"Technical highlights\" in text[i] or \"TECHNICAL HIGHLIGHTS\" in text[i]:\n",
      "            text = text[:i]\n",
      "            break\n",
      "        elif \"Technical specification\" in text[i] or \"TECHNICAL SPECIFICATION\" in text[i]:\n",
      "            text = text[:i]\n",
      "            break\n",
      "\n",
      "    # Join the lines with a newline character\n",
      "    text = \"\\n\".join(text)\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "            for line in page_text:\n",
      "                if 'Features' in line:\n",
      "                    in_features_section = True\n",
      "                elif 'Technical Specification' in line:\n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "            for line in page_text:\n",
      "                if 'Features' in line:\n",
      "                    in_features_section = True\n",
      "                elif 'Technical Specification' in line or \"TECHNICAL HIGHLIGHTS\" in line or \"Technical\" in line or \"TECHNICAL\" in line or \"TECHNICAL SPECIFICATIONS\" in line or \"Technical Specifications\" in line or \"Technical specification\" in line or \"Technical specifications\" in line or \"TECHNICAL SPECIFICATION\" in line or \"TECHNICAL SPECIFICATIONS\" in line or \"Technical Specification\" in line or \"Technical Specifications\" in line or \"Technical specification\" in line or \"Technical specifications\" in line or \"TECHNICAL SPECIFICATION\" in line or \"TECHNICAL SPECIFICATIONS\" in line or \"Technical Specification\" in line or \"Technical Specifications\" in line or \"Technical specification\" in line or \"Technical specifications\" in line or \"TECHNICAL SPECIFICATION\" in line or \"TECHNICAL SPECIFICATIONS\" in line or \"Technical Specification\" in line or \"Technical Specifications\" in line or \"Technical specification\" in line or \"Technical specifications\" in line or \"TECHNICAL SPECIFICATION\" in line or \"TECHNICAL SPECIFICATIONS\" in line:\n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):\n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):\n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):\n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):\n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[3]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights)', line, re.IGNORECASE):\n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[3]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[3]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            # Define a crop box that excludes the margins\n",
      "            # (You may need to adjust these values depending on the size of your margins)\n",
      "            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height * 0.9)\n",
      "            cropped_page = page.crop(crop_box)\n",
      "\n",
      "            page_text = cropped_page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            # Define a crop box that excludes the margins\n",
      "            # (You may need to adjust these values depending on the size of your margins)\n",
      "            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height * 0.9)\n",
      "            cropped_page = page.crop(crop_box)\n",
      "\n",
      "            page_text = cropped_page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            # Define a crop box that excludes the margins\n",
      "            # (You may need to adjust these values depending on the size of your margins)\n",
      "            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height * 0.9)\n",
      "            cropped_page = page.crop(crop_box)\n",
      "\n",
      "            page_text = cropped_page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[3]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "                elif not line.startswith('•') and in_features_section:\n",
      "                    in_features_section = False\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[3]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[3]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[4]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            # Define a crop box that excludes the margins and the last 1 cm from the bottom\n",
      "            # (You may need to adjust these values depending on the size of your margins)\n",
      "            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height - pdfplumber.utils.pdf_to_inches(1))\n",
      "            cropped_page = page.crop(crop_box)\n",
      "\n",
      "            page_text = cropped_page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            # Define a crop box that excludes the margins and the last 1 cm from the bottom\n",
      "            # (You may need to adjust these values depending on the size of your margins)\n",
      "            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height - pdfplumber.utils.pdf_to_inches(1))\n",
      "            cropped_page = page.crop(crop_box)\n",
      "\n",
      "            page_text = cropped_page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for page in pdf.pages:\n",
      "            # Define a crop box that excludes the margins and the last 1 cm from the bottom\n",
      "            # (You may need to adjust these values depending on the size of your margins)\n",
      "            crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height - 28.3)            \n",
      "            cropped_page = page.crop(crop_box)\n",
      "\n",
      "            page_text = cropped_page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (page.width * 0.1, page.height * 0.1, page.width * 0.9, page.height - 28.3)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 28.3)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 26.3)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 1.3)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 28.3, page.width, page.height)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 50, page.width, page.height)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 100, page.width, page.height)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 200, page.width, page.height)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(i)\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 200, page.width, page.height)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 200, page.width, page.height)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 400, page.width, page.height)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, 400)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, 200)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, 150)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, 100)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, 50)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, 800)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 28)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 100)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 50)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 60)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 65)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 69)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[6]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "import requests\n",
      "import pdfplumber\n",
      "import os\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in pdf.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "import PyPDF2\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PyPDF2.PdfFileReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for i in range(reader.getNumPages()):\n",
      "            page = reader.getPage(i)\n",
      "            text += page.extractText()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PyPDF2 import PdfReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in reader.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PyPDF2 import PdfReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in reader.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url) \n",
      "print(pdf_text)\n",
      "import requests\n",
      "from pdfminer.high_level import extract_text\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Extract text from the PDF\n",
      "        text = extract_text(f)\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from pdfminer.high_level import extract_text\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Extract text from the PDF\n",
      "        text = extract_text(f)\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PIL import Image\n",
      "import pytesseract\n",
      "import io\n",
      "import pdf2image\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Convert PDF to images\n",
      "        images = pdf2image.convert_from_bytes(f.read())\n",
      "\n",
      "    # Extract text from each image\n",
      "    text = ''\n",
      "    for image in images:\n",
      "        text += pytesseract.image_to_string(image)\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PIL import Image\n",
      "import pytesseract\n",
      "import io\n",
      "import pdf2image\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Convert PDF to images\n",
      "        images = pdf2image.convert_from_bytes(f.read())\n",
      "\n",
      "    # Extract text from each image\n",
      "    text = ''\n",
      "    for image in images:\n",
      "        text += pytesseract.image_to_string(image)\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PIL import Image\n",
      "import pytesseract\n",
      "import io\n",
      "import pdf2image\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Convert PDF to images\n",
      "        images = pdf2image.convert_from_bytes(f.read())\n",
      "\n",
      "    # Extract text from each image\n",
      "    text = ''\n",
      "    for image in images:\n",
      "        text += pytesseract.image_to_string(image)\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PIL import Image\n",
      "import pytesseract\n",
      "import io\n",
      "import pdf2image\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Convert PDF to images\n",
      "        images = pdf2image.convert_from_bytes(f.read())\n",
      "\n",
      "    # Extract text from each image\n",
      "    text = ''\n",
      "    for image in images:\n",
      "        text += pytesseract.image_to_string(image)\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "\n",
      "def print_pdf_content(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    print(response.content)\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "print_pdf_content(pdf_url)\n",
      "import requests\n",
      "from PyPDF2 import PdfFileReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfFileReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for i in range(reader.getNumPages()):\n",
      "            page = reader.getPage(i)\n",
      "            text += page.extractText()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PyPDF2 import PdfFileReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for i in range(reader.getNumPages()):\n",
      "            page = reader.getPage(i)\n",
      "            text += page.extractText()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PyPDF2 import PdfReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for i in range(reader.getNumPages()):\n",
      "            page = reader.getPage(i)\n",
      "            text += page.extractText()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PyPDF2 import PdfReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for i in range(len(reader.pages)):\n",
      "            page = reader.getPage(i)\n",
      "            text += page.extractText()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url)\n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PyPDF2 import PdfReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in reader.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url) \n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PyPDF2 import PdfReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in reader.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url) \n",
      "print(pdf_text)\n",
      "import requests\n",
      "from PyPDF2 import PdfReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in reader.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url) \n",
      "print(pdf_text)\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd \n",
      "import re\n",
      "import os\n",
      "import pdfplumber\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                print(len(pdf.pages))\n",
      "                print(page.height)\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            print(i)\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets.head(40)\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications?|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets.head(40)\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets.head(40)\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-4-1106-preview\",\n",
      "        messages=messages,\n",
      "        response_format={ \"type\": \"json_object\" }\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "print(df_with_text[\"Scraped Text\"][0])\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][0])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-4-1106-preview\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][0])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "print(input_text)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-4-1106-preview\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][0])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "print(input_text)\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "number= 3\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "number= 4\n",
      "print(find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "number= 4\n",
      "print(\"text = \",find_datasheets(df[\"url\"][number]))\n",
      "print(df[\"url\"][number])\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[5]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('features|FUNCTIONALITY', line, re.IGNORECASE):\n",
      "                    in_features_section = True\n",
      "                elif re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets.head(40)\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "print(input_text)\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=\"\"\"\n",
      "      Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 250 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed.Write these three blocks: \n",
      "      Block 1 : [100 - 250 words. ] Provide factual information blocks to explain what value the products bring.\n",
      "      minimum of one or a maximum of three.\n",
      "\n",
      "      Block 2 : [40-100 words] Provide the key features for the product. As many as needed\n",
      "\n",
      "      Block 3 : [10-150] Add technical specifications related to the product. expected structure is \"category, parameters and parameter value\" As many as needed.\n",
      "\n",
      "      Return as Json. \"\"\",\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return 'temp.pdf'\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[create_file(url)]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "      pprint(messages)\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return 'temp.pdf'\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=\"\"\"\n",
      "      Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 250 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed.Write these three blocks: \n",
      "      Block 1 : [100 - 250 words. ] Provide factual information blocks to explain what value the products bring.\n",
      "      minimum of one or a maximum of three.\n",
      "\n",
      "      Block 2 : [40-100 words] Provide the key features for the product. As many as needed\n",
      "\n",
      "      Block 3 : [10-150] Add technical specifications related to the product. expected structure is \"category, parameters and parameter value\" As many as needed.\n",
      "\n",
      "      Return as Json. \"\"\",\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "thread = client.beta.threads.create()\n",
      "ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url)\n",
      "  file = client.files.create(file=pdf_file,purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "      pprint(messages)\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url, path):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return os.path.join(path, 'temp.pdf')\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url)\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "      pprint(messages)\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "      pprint(messages)\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url, path):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return os.path.join(path, 'temp.pdf')\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "      pprint(messages)\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url, path):\n",
      "    os.makedirs(path, exist_ok=True)\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return os.path.join(path, 'temp.pdf')\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "      pprint(messages)\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      print(last_message_content)\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=\"\"\"\n",
      "      Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 250 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed.Write these three blocks: \n",
      "      Block 1 : [100 - 250 words. ] Provide factual information blocks to explain what value the products bring.\n",
      "      minimum of one or a maximum of three.\n",
      "\n",
      "      Block 2 : [40-100 words] Provide the key features for the product. As many as needed\n",
      "\n",
      "      Block 3 : [10-150] Add technical specifications related to the product. expected structure is \"category, parameters and parameter value\" As many as needed.\n",
      "\n",
      "      Return as Json. \"\"\",\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "thread = client.beta.threads.create()\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url, path):\n",
      "    os.makedirs(path, exist_ok=True)\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return os.path.join(path, 'temp.pdf')\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      print(last_message_content)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"Tell me who is acquirer\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      print(last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"I have a text that I want to format into headings and text bulks. Please format this text.\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      print(last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 250 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed.Write these three blocks: \n",
      "      Block 1 : [100 - 250 words. ] Provide factual information blocks to explain what value the products bring.\n",
      "      minimum of one or a maximum of three.\n",
      "\n",
      "      Block 2 : [40-100 words] Provide the key features for the product. As many as needed\n",
      "\n",
      "      Block 3 : [10-150] Add technical specifications related to the product. expected structure is \"category, parameters and parameter value\" As many as needed.\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      print(last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 250 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      print(last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      print(last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(message_content.value)\n",
      "      print(last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text.value\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "  message = client.beta.threads.messages.retrieve(\n",
      "    thread_id=thread.id,\n",
      "    message_id=message.id\n",
      "  )\n",
      "  # Extract the message content\n",
      "  message_content = message.content[0].text\n",
      "  annotations = message_content.annotations\n",
      "  citations = []\n",
      "\n",
      "  # Iterate over the annotations and add footnotes\n",
      "  for index, annotation in enumerate(annotations):\n",
      "      # Replace the text with a footnote\n",
      "      message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "      # Gather citations based on annotation attributes\n",
      "      if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "          cited_file = client.files.retrieve(file_citation.file_id)\n",
      "          citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "      elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "          cited_file = client.files.retrieve(file_path.file_id)\n",
      "          citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "          # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "  # Add footnotes to the end of the message before displaying to user\n",
      "  message_content.value += '\\n' + '\\n'.join(citations)\n",
      "  print(\"content_annotation \" , message_content.value)\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "     \n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Use the file as context. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks.\n",
      "      For example block: heading: \"Greater endurance\", text: \"With spaciuous moopool and rack mount the Sounder USV System is a flexible and adaptive paltform for different applications. Te onboard power generation provides enough power to operate several payloads at once. \n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=\"\"\"\n",
      "      Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "     \"\"\",\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "thread = client.beta.threads.create()\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url, path):\n",
      "    os.makedirs(path, exist_ok=True)\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return os.path.join(path, 'temp.pdf')\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(10)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(3)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(3)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    # print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(3)\n",
      "    print(run_status.status)\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=\"\"\"\n",
      "      Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "     \"\"\",\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "thread = client.beta.threads.create()\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url, path):\n",
      "    os.makedirs(path, exist_ok=True)\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return os.path.join(path, 'temp.pdf')\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    # print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(3)\n",
      "    print(run_status.status)\n",
      "    if run_status.status == 'failed':\n",
      "      print(\"failed\")\n",
      "      break\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "print(input_text)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 10)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if re.search('technical (specifications|highlights|data)', line, re.IGNORECASE):                    \n",
      "                    break\n",
      "\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets.head(40)\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"You are a text editor, that edits text into readable text bulks and headings. Use the same kind of language and tone and write style as the text you are editing. Do not change the meaning of the text, or add any new information. Format the text as little as possible.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"\"\" \n",
      "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                Block 1 (100 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "                For example : \n",
      "                Product Evolution\n",
      "                HUGIN has been continuously evolving since development began in 1991.\n",
      "                From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "                at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "                forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "                World-class performance and new capabilities and features are added\n",
      "                through software updates and vehicle upgrades.\n",
      "                Deployability\n",
      "                HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "                or from shore. The complete HUGIN system including operator consoles,\n",
      "                launch and recovery system and the AUV itself can be delivered in\n",
      "                DNV-certified offshore containers. The containers allows for transport\n",
      "                by sea, air and land and mobilization is easy with only an external power\n",
      "                connection required. Various container sizes are available to meet the\n",
      "                customer needs.\n",
      "\n",
      "                Block 2 (40 - 100 words):\n",
      "                List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "                Block 3 (10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "                \"\"\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\nPlease format this text.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=\"\"\"\n",
      "      Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "     \"\"\",\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "thread = client.beta.threads.create()\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url, path):\n",
      "    os.makedirs(path, exist_ok=True)\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return os.path.join(path, 'temp.pdf')\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "      For example : \n",
      "      Product Evolution\n",
      "      HUGIN has been continuously evolving since development began in 1991.\n",
      "      From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "      at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "      forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "      World-class performance and new capabilities and features are added\n",
      "      through software updates and vehicle upgrades.\n",
      "      Deployability\n",
      "      HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "      or from shore. The complete HUGIN system including operator consoles,\n",
      "      launch and recovery system and the AUV itself can be delivered in\n",
      "      DNV-certified offshore containers. The containers allows for transport\n",
      "      by sea, air and land and mobilization is easy with only an external power\n",
      "      connection required. Various container sizes are available to meet the\n",
      "      customer needs.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    # print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(3)\n",
      "    print(run_status.status)\n",
      "    if run_status.status == 'failed':\n",
      "      print(\"failed\")\n",
      "      break\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "print(output_text)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets.head(40)\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 6:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 60)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets.head(40)\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"\"\" \n",
      "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                Block 1 (100 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks.\n",
      "                For example : \n",
      "                Product Evolution\n",
      "                HUGIN has been continuously evolving since development began in 1991.\n",
      "                From the first commercial survey in 1997, KONGSBERG and our partners\n",
      "                at the Norwegian Defense Research Establishment (FFI) have been at the\n",
      "                forefront of underwater robotic technology. HUGIN continues to deliver\n",
      "                World-class performance and new capabilities and features are added\n",
      "                through software updates and vehicle upgrades.\n",
      "                Deployability\n",
      "                HUGIN can be deployed from dedicated vessels, vessels of opportunity\n",
      "                or from shore. The complete HUGIN system including operator consoles,\n",
      "                launch and recovery system and the AUV itself can be delivered in\n",
      "                DNV-certified offshore containers. The containers allows for transport\n",
      "                by sea, air and land and mobilization is easy with only an external power\n",
      "                connection required. Various container sizes are available to meet the\n",
      "                customer needs.\n",
      "\n",
      "                Block 2 (40 - 100 words):\n",
      "                List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "                Block 3 (10 - 350 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "                \"\"\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    # print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(3)\n",
      "    print(run_status.status)\n",
      "    if run_status.status == 'failed':\n",
      "      print(\"failed\")\n",
      "      break\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "thread = client.beta.threads.create()\n",
      "# from a pdf url create a pdf file\n",
      "def create_pdf(url, path):\n",
      "    os.makedirs(path, exist_ok=True)\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open(os.path.join(path, 'temp.pdf'), 'wb') as f:\n",
      "        f.write(response.content)\n",
      "    return os.path.join(path, 'temp.pdf')\n",
      "\n",
      "def create_file(url):\n",
      "  file = client.files.create(file= create_pdf(url),purpose='assistants')\n",
      "  return file\n",
      "import os\n",
      "import openai\n",
      "import time\n",
      "from pprint import pprint\n",
      "\n",
      "def ask_gpt(url):\n",
      "  pdf_file = create_pdf(url, \"data/gpt_content\")\n",
      "  file = client.files.create(file=open(pdf_file, \"rb\"),purpose='assistants')\n",
      "# Add a Message to a Thread\n",
      "  message = client.beta.threads.messages.create(\n",
      "    thread_id=thread.id,\n",
      "    role=\"user\",\n",
      "    content=\"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "      Write the following three content blocks:\n",
      "\n",
      "      Block 1 (100 - 400 words):\n",
      "      Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings.\n",
      "\n",
      "      Block 2 (40 - 100 words):\n",
      "      List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "      Block 3 (10 - 150 words):\n",
      "      Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "      Return as Json.\"\"\",\n",
      "    file_ids=[file.id]  # Add the file to the message\n",
      "  )\n",
      "\n",
      "  # Run the Assistant\n",
      "  run = client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant.id,instructions=\"Please answer the user, just using text from the file.\")\n",
      "  print(run.model_dump_json(indent=4))\n",
      "\n",
      "  # If run is 'completed', get messages and print\n",
      "  while True:\n",
      "    # Retrieve the run status\n",
      "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id,run_id=run.id)\n",
      "    # print(run_status.model_dump_json(indent=4))\n",
      "    time.sleep(3)\n",
      "    print(run_status.status)\n",
      "    if run_status.status == 'failed':\n",
      "      print(\"failed\")\n",
      "      break\n",
      "    if run_status.status == 'completed':\n",
      "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "\n",
      "      # Get the last message from the assistant\n",
      "      last_message = messages.data[0]\n",
      "\n",
      "      # Get the content of the last message\n",
      "      last_message_content = last_message.content[0].text.value\n",
      "\n",
      "      # Retrieve the message object\n",
      "      message = client.beta.threads.messages.retrieve(\n",
      "        thread_id=thread.id,\n",
      "        message_id=message.id\n",
      "      )\n",
      "\n",
      "      # Extract the message content\n",
      "      message_content = message.content[0].text\n",
      "      annotations = message_content.annotations\n",
      "      citations = []\n",
      "\n",
      "      # Iterate over the annotations and add footnotes\n",
      "      for index, annotation in enumerate(annotations):\n",
      "          # Replace the text with a footnote\n",
      "          message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
      "\n",
      "          # Gather citations based on annotation attributes\n",
      "          if (file_citation := getattr(annotation, 'file_citation', None)):\n",
      "              cited_file = client.files.retrieve(file_citation.file_id)\n",
      "              citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
      "          elif (file_path := getattr(annotation, 'file_path', None)):\n",
      "              cited_file = client.files.retrieve(file_path.file_id)\n",
      "              citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
      "              # Note: File download functionality not implemented above for brevity\n",
      "\n",
      "      # Add footnotes to the end of the message before displaying to user\n",
      "      message_content.value += '\\n' + '\\n'.join(citations)\n",
      "      print(\"content_annotation \" , message_content.value)\n",
      "      print(\"lat_message \" ,last_message_content)\n",
      "      print(\"whole message: \", message)\n",
      "      return last_message_content\n",
      "      break\n",
      "    else:\n",
      "      ### sleep again\n",
      "      time.sleep(2)\n",
      "print(output_text)\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets.head(40)\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                Block 1 (100 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings.\n",
      "\n",
      "                Block 2 (40 - 100 words):\n",
      "                List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "                Block 3 (10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "                Return as Json.\"\"\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "print(input_text)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                Block 1 (200 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block.\n",
      "\n",
      "                Block 2 (40 - 100 words):\n",
      "                List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "                Block 3 (10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "                Return as Json.\"\"\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": \"\"\"Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                Block 1 (200 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.\n",
      "\n",
      "                Block 2 (40 - 100 words):\n",
      "                List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "                Block 3 (10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "                Return as Json.\"\"\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "system_prompt = \"\"\"\n",
      "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                Block 1 (200 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.\n",
      "\n",
      "                Block 2 (40 - 100 words):\n",
      "                List the key features of the product. Include as many as needed. Use bullet points for each feature.\n",
      "\n",
      "                Block 3 (10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "                Return as Json.\"\"\"\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "print(input_text)\n",
      "system_prompt = \"\"\"\n",
      "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                Block 1 (200 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.\n",
      "\n",
      "                Block 2 (40 - 100 words):\n",
      "                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.\n",
      "\n",
      "                Block 3 (10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.\n",
      "\n",
      "\n",
      "                Return as Json.\"\"\"\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=system_prompt,\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    response_format={ \"type\": \"json_object\" },\n",
      "\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "thread = client.beta.threads.create()\n",
      "assistant = client.beta.assistants.create(\n",
      "    name=\"Kongberg content\",\n",
      "    instructions=system_prompt,\n",
      "    model=\"gpt-3.5-turbo-1106\",\n",
      "    tools=[{\"type\": \"retrieval\"}],\n",
      ")\n",
      "thread = client.beta.threads.create()\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        response_format={ \"type\": \"json_object\" },\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "print(output_text)\n",
      "import json\n",
      "import csv\n",
      "\n",
      "# Parse the JSON data\n",
      "data = json.loads(output_text)\n",
      "\n",
      "# Prepare the data for the CSV\n",
      "csv_data = []\n",
      "\n",
      "# Extract data from Block 1\n",
      "for item in data['Block 1']:\n",
      "    csv_data.append([item['heading'], item['content'], ''])\n",
      "\n",
      "# Extract data from Block 2\n",
      "for feature in data['Block 2']['key_features']:\n",
      "    csv_data.append(['', feature, ''])\n",
      "\n",
      "# Extract data from Block 3\n",
      "for spec in data['Block 3']['technical_specifications']:\n",
      "    csv_data.append(['', '', spec])\n",
      "\n",
      "# Write the data to a CSV file\n",
      "with open('output.csv', 'w', newline='') as file:\n",
      "    writer = csv.writer(file)\n",
      "    writer.writerows(csv_data)\n",
      "# output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/9a7a2014928540309caa9552b55e4b42/01.marine-robots-2p-03.09.21.pdf\")\n",
      "output_text = ask_gpt(\"https://www.kongsberg.com/contentassets/88df0cc7b3574a0ab3d1d0d560fae99b/datasheet_blueinsight.pdf\")\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "def add_data_to_df(df, data):\n",
      "    # Extract data from Block 1\n",
      "    block1_data = [{'Heading': item['heading'], 'Content': item['content']} for item in data['Block 1']]\n",
      "    block1_df = pd.DataFrame(block1_data)\n",
      "\n",
      "    # Extract data from Block 2\n",
      "    block2_data = [{'Key Features': feature} for feature in data['Block 2']['key_features']]\n",
      "    block2_df = pd.DataFrame(block2_data)\n",
      "\n",
      "    # Extract data from Block 3\n",
      "    block3_data = [{'Technical Specifications': spec} for spec in data['Block 3']['technical_specifications']]\n",
      "    block3_df = pd.DataFrame(block3_data)\n",
      "\n",
      "    # Concatenate the dataframes\n",
      "    df = pd.concat([df, block1_df, block2_df, block3_df], axis=1)\n",
      "\n",
      "    return df\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "input_text = (df_with_text[\"Scraped Text\"][2])\n",
      "output_text = generate_text(input_text)\n",
      "\n",
      "# Parse the JSON data\n",
      "data = json.loads(output_text)\n",
      "\n",
      "# Add the data to the dataframe\n",
      "df_with_text = add_data_to_df(df_with_text, data)\n",
      "\n",
      "print(df_with_text)\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "index = 2\n",
      "input_text = (df_with_text[\"Scraped Text\"][index])\n",
      "output_text = generate_text(input_text)\n",
      "# Parse the JSON data\n",
      "data = json.loads(output_text)\n",
      "block1 = data['Block 1']\n",
      "block2 = data['Block 2']\n",
      "block3 = data['Block 3']\n",
      "# Assuming df_with_text is your DataFrame and '2' is the index of the row you want to modify\n",
      "df_with_text.loc[index, 'Block 1'] = str(block1)\n",
      "df_with_text.loc[index, 'Block 2'] = str(block2)\n",
      "df_with_text.loc[index, 'Block 3'] = str(block3)\n",
      "\n",
      "\n",
      "print(output_text)\n",
      "df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\",usecols=[\"Product_Name\", \"url\", \"Data sheets\", \"Scraped Text\"])\n",
      "\n",
      "for index, row in df_with_text.head(4).iterrows():\n",
      "    input_text = row[\"Scraped Text\"]\n",
      "    output_text = generate_text(input_text)\n",
      "    # Parse the JSON data\n",
      "    data = json.loads(output_text)\n",
      "    block1 = data['Block 1']\n",
      "    block2 = data['Block 2']\n",
      "    block3 = data['Block 3']\n",
      "    # Add the blocks to the DataFrame\n",
      "    df_with_text.loc[index, 'General text'] = str(block1)\n",
      "    df_with_text.loc[index, 'Features'] = str(block2)\n",
      "    df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "\n",
      "\n",
      "df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "system_prompt = \"\"\"\n",
      "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                \"General text\"  (200 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}\n",
      "\n",
      "                 \"Key Features\": { (40 - 100 words):\n",
      "                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}\n",
      "\n",
      "                \"Technical Specifications\": {(10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.}\n",
      "\n",
      "\n",
      "                Return as Json with the same formats as always.\"\"\"\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        response_format={ \"type\": \"json_object\" },\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\",usecols=[\"Product_Name\", \"url\", \"Data sheets\", \"Scraped Text\"])\n",
      "\n",
      "for index, row in df_with_text.head(4).iterrows():\n",
      "    input_text = row[\"Scraped Text\"]\n",
      "    output_text = generate_text(input_text)\n",
      "    # Parse the JSON data\n",
      "    data = json.loads(output_text)\n",
      "    block1 = data['Block 1']\n",
      "    block2 = data['Block 2']\n",
      "    block3 = data['Block 3']\n",
      "    # Add the blocks to the DataFrame\n",
      "    df_with_text.loc[index, 'General text'] = str(block1)\n",
      "    df_with_text.loc[index, 'Features'] = str(block2)\n",
      "    df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "\n",
      "\n",
      "df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\",usecols=[\"Product_Name\", \"url\", \"Data sheets\", \"Scraped Text\"])\n",
      "\n",
      "for index, row in df_with_text.head(4).iterrows():\n",
      "    input_text = row[\"Scraped Text\"]\n",
      "    output_text = generate_text(input_text)\n",
      "    print(output_text)\n",
      "    # Parse the JSON data\n",
      "    data = json.loads(output_text)\n",
      "    block1 = data['General text']\n",
      "    block2 = data['Key Features']\n",
      "    block3 = data['Technical Specifications']\n",
      "    # Add the blocks to the DataFrame\n",
      "    df_with_text.loc[index, 'General text'] = str(block1)\n",
      "    df_with_text.loc[index, 'Features'] = str(block2)\n",
      "    df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "\n",
      "\n",
      "df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\",usecols=[\"Product_Name\", \"url\", \"Data sheets\", \"Scraped Text\"])\n",
      "\n",
      "for index, row in df_with_text.head(15).iterrows():\n",
      "    input_text = row[\"Scraped Text\"]\n",
      "    output_text = generate_text(input_text)\n",
      "    print(output_text)\n",
      "    # Parse the JSON data\n",
      "    data = json.loads(output_text)\n",
      "    block1 = data['General text']\n",
      "    block2 = data['Key Features']\n",
      "    block3 = data['Technical Specifications']\n",
      "    # Add the blocks to the DataFrame\n",
      "    df_with_text.loc[index, 'General text'] = str(block1)\n",
      "    df_with_text.loc[index, 'Features'] = str(block2)\n",
      "    df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "\n",
      "\n",
      "df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 12:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 60)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure|data sheets|Data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "import requests\n",
      "from PyPDF2 import PdfReader\n",
      "import io\n",
      "\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with io.BytesIO(response.content) as f:\n",
      "        # Open the PDF file\n",
      "        reader = PdfReader(f)\n",
      "\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        for page in reader.pages:\n",
      "            text += page.extract_text()\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "pdf_url = 'https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/hugin-superior.pdf'\n",
      "pdf_text = scrape_pdf_text(pdf_url) \n",
      "print(pdf_text)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "\n",
      "        input_text = row[\"Scraped Text\"]\n",
      "        output_text = generate_text(input_text)\n",
      "        print(output_text)\n",
      "        # Parse the JSON data\n",
      "        data = json.loads(output_text)\n",
      "        block1 = data['General text']\n",
      "        block2 = data['Key Features']\n",
      "        block3 = data['Technical Specifications']\n",
      "        # Add the blocks to the DataFrame\n",
      "        df_with_text.loc[index, 'General text'] = str(block1)\n",
      "        df_with_text.loc[index, 'Features'] = str(block2)\n",
      "        df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "\n",
      "\n",
      "df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "\n",
      "        input_text = row[\"Scraped Text\"]\n",
      "        output_text = generate_text(input_text)\n",
      "        print(output_text)\n",
      "        # Parse the JSON data\n",
      "        data = json.loads(output_text)\n",
      "        block1 = data['General text']\n",
      "        block2 = data['Key Features']\n",
      "        block3 = data['Technical Specifications']\n",
      "        # Add the blocks to the DataFrame\n",
      "        df_with_text.loc[index, 'General text'] = str(block1)\n",
      "        df_with_text.loc[index, 'Features'] = str(block2)\n",
      "        df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "        df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "        df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "            df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    # if pd.isnull(row[\"General text\"]):\n",
      "    if not pd.isnull(row[\"Scraped Text\"]):\n",
      "\n",
      "        input_text = row[\"Scraped Text\"]\n",
      "        output_text = generate_text(input_text)\n",
      "        print(output_text)\n",
      "        # Parse the JSON data\n",
      "        data = json.loads(output_text)\n",
      "        block1 = data['General text']\n",
      "        block2 = data['Key Features']\n",
      "        block3 = data['Technical Specifications']\n",
      "        # Add the blocks to the DataFrame\n",
      "        df_with_text.loc[index, 'General text'] = str(block1)\n",
      "        df_with_text.loc[index, 'Features'] = str(block2)\n",
      "        df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "        df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "        df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    # if pd.isnull(row[\"General text\"]):\n",
      "    if not pd.isnull(row[\"Scraped Text\"]):\n",
      "\n",
      "        input_text = row[\"Scraped Text\"]\n",
      "        output_text = generate_text(input_text)\n",
      "        print(output_text)\n",
      "        # Parse the JSON data\n",
      "        data = json.loads(output_text)\n",
      "        block1 = data['General text']\n",
      "        block2 = data['Key Features']\n",
      "        block3 = data['Technical Specifications']\n",
      "        # Add the blocks to the DataFrame\n",
      "        df_with_text.loc[index, 'General text'] = (block1)\n",
      "        df_with_text.loc[index, 'Features'] = (block2)\n",
      "        df_with_text.loc[index, 'Technical Specifications'] = (block3)\n",
      "        df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "        df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = (block1)\n",
      "            df_with_text.loc[index, 'Features'] = (block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = (block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "            df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "import re\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Remove illegal characters\n",
      "            block1 = re.sub(r'[\\0\\v\\f\\xa0]', '', block1)\n",
      "            block2 = re.sub(r'[\\0\\v\\f\\xa0]', '', block2)\n",
      "            block3 = re.sub(r'[\\0\\v\\f\\xa0]', '', block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = (block1)\n",
      "            df_with_text.loc[index, 'Features'] = (block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = (block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "            df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "import re\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Remove illegal characters\n",
      "            block1 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block1]\n",
      "            block2 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block2]\n",
      "            block3 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block3]\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = (block1)\n",
      "            df_with_text.loc[index, 'Features'] = (block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = (block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "            df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        response_format={ \"type\": \"json_object\" },\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "system_prompt = \"\"\"\n",
      "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                \"General text\":{ (200 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}\n",
      "\n",
      "                \"Key Features\": { (40 - 100 words):\n",
      "                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}\n",
      "\n",
      "                \"Technical Specifications\": {(10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.}\n",
      "\n",
      "\n",
      "\n",
      "                Return as Json with the same formats as always.\"\"\"\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        response_format={ \"type\": \"json_object\" },\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "import pandas as pd\n",
      "import json\n",
      "import re\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Remove illegal characters\n",
      "            block1 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block1]\n",
      "            block2 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block2]\n",
      "            block3 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block3]\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = (block1)\n",
      "            df_with_text.loc[index, 'Features'] = (block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = (block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "            df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "import re\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\" product name: \", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Remove illegal characters\n",
      "            block1 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block1]\n",
      "            block2 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block2]\n",
      "            block3 = [re.sub(r'[\\0\\v\\f\\xa0]', '', item) for item in block3]\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = (block1)\n",
      "            df_with_text.loc[index, 'Features'] = (block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = (block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if not pd.isnull(row[\"Scraped Text\"]):\n",
      "        print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "        input_text = row[\"Scraped Text\"]\n",
      "        output_text = generate_text(input_text)\n",
      "        print(output_text)\n",
      "        # Parse the JSON data\n",
      "        data = json.loads(output_text)\n",
      "        block1 = data['General text']\n",
      "        block2 = data['Key Features']\n",
      "        block3 = data['Technical Specifications']\n",
      "        # Add the blocks to the DataFrame\n",
      "        df_with_text.loc[index, 'General text'] = (block1)\n",
      "        df_with_text.loc[index, 'Features'] = (block2)\n",
      "        df_with_text.loc[index, 'Technical Specifications'] = (block3)\n",
      "        df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/modems/cnode-minis/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure|data sheets|Data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"forund links 1\")\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/modems/cnode-minis/\"))\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure|data sheets|Data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data- and product sheets|Brochure|data sheets|Data sheet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets|Brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets|Brochure\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return links\n",
      "\n",
      "        # If no data sheets are found, look for brochures\n",
      "        brochure_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"brochure\", re.I))\n",
      "\n",
      "        if brochure_subtitle:\n",
      "            brochure_section = brochure_subtitle.find_next_sibling()\n",
      "\n",
      "            if brochure_section:\n",
      "                brochure_links = brochure_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                if brochure_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in brochure_links if link.get('href').endswith('.pdf')]\n",
      "                    return links\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        brochure_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"brochure\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return links\n",
      "\n",
      "        # If no data sheets are found, look for brochures\n",
      "       \n",
      "\n",
      "        elif brochure_subtitle:\n",
      "            brochure_section = brochure_subtitle.find_next_sibling()\n",
      "\n",
      "            if brochure_section:\n",
      "                brochure_links = brochure_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                if brochure_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in brochure_links if link.get('href').endswith('.pdf')]\n",
      "                    return links\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    print(links)\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links if link.get('href').endswith('.pdf')]\n",
      "                    print(', '.join(links))\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "                    print(', '.join(links))\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "print(find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "                    links = [link.get('href') + '.pdf' if not link.get('href').endswith('.pdf') else link.get('href') for link in datasheet_links]\n",
      "                    print(', '.join(links))\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "new_data = (find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      ")\n",
      "new_data = (find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    print(\"found links 1\")\n",
      "                    print(datasheet_links)\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "                    print(', '.join(links))\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "    return None\n",
      "new_data = (find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                #Found a random link that was not data sheet\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "                print(href + \" is not a data sheet \", \"for product: \", url)\n",
      "    # If the section, the div, or the links are not found, return None\n",
      "\n",
      "    return None\n",
      "new_data = (find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-positioning-systems/hipap-models/HiPAP-HPR/\"))\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets\", re.I))\n",
      "        print(datasheets_subtitle)\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                #Found a random link that was not data sheet\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    print(href + \" is not a data sheet \", \"for product: \", url)\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "                \n",
      "    # If the section, the div, or the links are not found, return None\n",
      "\n",
      "    return None\n",
      "def find_datasheets(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the div with the class \"Downloads\"\n",
      "    downloads_div = soup.find('div', class_=\"Downloads\")\n",
      "\n",
      "    # If the div is found, find the section with the subtitle \"Data sheet\", \"DATA SHEET\", or \"Data- and product sheets\" within this div\n",
      "    if downloads_div:\n",
      "        datasheets_subtitle = downloads_div.find('h3', class_=\"Section__subtitle\", string=re.compile(\"data sheet|data sheets|data- and product sheets|Product leaflet\", re.I))\n",
      "\n",
      "        # If the subtitle is found, find the next sibling section\n",
      "        if datasheets_subtitle:\n",
      "            datasheets_section = datasheets_subtitle.find_next_sibling()\n",
      "\n",
      "            # If the section is found, find all links within this section\n",
      "            if datasheets_section:\n",
      "                datasheet_links = datasheets_section.find_all('a', class_=\"Downloads__itemLink\")\n",
      "\n",
      "                # If the links are found, prepend \"https://www.kongsberg.com\" to each href attribute and return the list\n",
      "                if datasheet_links:\n",
      "                    links = [link.get('href') if 'https://simrad.online' in link.get('href') or 'https://www.simrad.online' in link.get('href') else \"https://www.kongsberg.com\" + link.get('href') for link in datasheet_links]\n",
      "                    print(datasheets_subtitle.get_text(strip=True) + \" for product: \", url, \" are: \", links)\n",
      "                    return ', '.join(links)\n",
      "\n",
      "        else:\n",
      "            # If no subtitle is found, find all links where the direct child span has the text \"Datasheet\"\n",
      "            datasheet_links = downloads_div.find_all('a', class_=\"Downloads__itemLink\")\n",
      "    \n",
      "            # If the links are found, return the href attribute of the first link that ends with \".pdf\"\n",
      "            for link in datasheet_links:\n",
      "                #Found a random link that was not data sheet\n",
      "                href = link.get('href')\n",
      "                if href.endswith('.pdf'):\n",
      "                    print(href + \" is not a data sheet \", \"for product: \", url)\n",
      "                    return href if 'https://simrad.online' in href or 'https://www.simrad.online' in href else \"https://www.kongsberg.com\" + href\n",
      "                \n",
      "    # If the section, the div, or the links are not found, return None\n",
      "\n",
      "    return None\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "df = pd.read_excel(\"data/12_04/tags_12_04.xlsx\")\n",
      "\n",
      "# Add a new column \"Data sheets\" to the dataframe\n",
      "df['Data sheets'] = df['url'].apply(find_datasheets)\n",
      "\n",
      "# only keep column url, product name and data sheets\n",
      "df = df[['url', 'Product_Name', 'Data sheets']]\n",
      "df.to_csv(\"data/data_sheets.csv\", index=False)\n",
      "df.to_excel(\"data/data_sheets.xlsx\", index=False)\n",
      "new_data = (find_datasheets(\"https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/acoustic-control-system/ACS500/\"))\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 10:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 10:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "\n",
      "# Load the CSV file\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "\n",
      "# Filter rows with valid data sheet URLs\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "\n",
      "# Scrape the text from each data sheet and save it as a new column\n",
      "data_sheets['Scraped Text'] = data_sheets['Data sheets'].apply(scrape_pdf_text)\n",
      "\n",
      "# Save the DataFrame to a new CSV file\n",
      "data_sheets.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = ' '.join(block1)\n",
      "            df_with_text.loc[index, 'Features'] = ' '.join(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "from openai._client import OpenAI\n",
      "\n",
      "client = OpenAI(\n",
      "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
      ")\n",
      "system_prompt = \"\"\"\n",
      "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                \"General text\":{ (200 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}\n",
      "\n",
      "                \"Key Features\": { (40 - 100 words):\n",
      "                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}\n",
      "\n",
      "                \"Technical Specifications\": {(10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.}\n",
      "\n",
      "\n",
      "\n",
      "                Return as Json with the same formats as always.\"\"\"\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        response_format={ \"type\": \"json_object\" },\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = ' '.join(block1)\n",
      "            df_with_text.loc[index, 'Features'] = ' '.join(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = ' '.join(block1)\n",
      "            df_with_text.loc[index, 'Features'] = ' '.join(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = ' '.join(block1)\n",
      "            df_with_text.loc[index, 'Features'] = ' '.join(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            block1 = data['General text']\n",
      "            block2 = data['Key Features']\n",
      "            block3 = data['Technical Specifications']\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = ' '.join(block1)\n",
      "            df_with_text.loc[index, 'Features'] = ' '.join(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = ' '.join(block1)\n",
      "            df_with_text.loc[index, 'Features'] = ' '.join(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = ' '.join(block1)\n",
      "            df_with_text.loc[index, 'Features'] = ' '.join(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = ' '.join(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "        if not pd.isnull(row[\"Scraped Text\"]):\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "print(scrape_pdf_text(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/\"))\n",
      "import requests\n",
      "\n",
      "def save_pdf(url, filename):\n",
      "    # Send a GET request to the URL\n",
      "    response = requests.get(url)\n",
      "\n",
      "    # Check if the request was successful\n",
      "    if response.status_code == 200:\n",
      "        # Write the content to a PDF file\n",
      "        with open(filename, 'wb') as f:\n",
      "            f.write(response.content)\n",
      "        print(f\"PDF saved as {filename}\")\n",
      "    else:\n",
      "        print(f\"Failed to retrieve PDF from {url}\")\n",
      "\n",
      "# Example usage\n",
      "save_pdf(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/\", \"output.pdf\")\n",
      "print(scrape_pdf_text(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/acs500-emergency-acoustic-bop-control-system/\"))\n",
      "print(save_pdf(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/acs500-emergency-acoustic-bop-control-system/\"))\n",
      "print(save_pdf(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/acs500-emergency-acoustic-bop-control-system/\", \"output.pdf\"))\n",
      "print(scrape_pdf_text(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/acs500-emergency-acoustic-bop-control-system/\"))\n",
      "print(scrape_pdf_text(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/\"))\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 10:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "print(scrape_pdf_text(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/\"))\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 10:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "print(scrape_pdf_text(\"https://www.kongsberg.com/globalassets/maritime/km-products/product-documents/apos-survey---surveyors-independent-operator-station-for-hipap/\"))\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            # df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            # df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "            df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "            df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_with_text.csv\", index=False)\n",
      "df_with_text.to_excel(\"data/data_sheets_with_text.xlsx\", index=False)\n",
      "produkter_alle = pd.read_excel(\"data/styrings_data/produkter_alle.xlsx\", usecols=[\"Product_Name\",\"url\",\"Product category\"])\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\")\n",
      "\n",
      "#Merge the two dataframes on the product name.\n",
      "produkter_alle = pd.read_excel(\"data/styrings_data/produkter_alle.xlsx\", usecols=[\"Product_Name\",\"url\",\"Product category\"])\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "#Merge the two dataframes on the product name, and keep product name, url, product category from produkter_alle, the rest from df_with_text\n",
      "merged = pd.merge(produkter_alle, df_with_text, on=\"Product_Name\", how=\"left\")\n",
      "merged.to_csv(\"data/merged_data_test.csv\", index=False)\n",
      "produkter_alle = pd.read_excel(\"data/styrings_data/produkter_alle.xlsx\", usecols=[\"Product_Name\",\"url\",\"Product category\"])\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_with_text.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "#Merge the two dataframes on the product name, and keep product name, url, product category from produkter_alle, the rest from df_with_text\n",
      "merged = pd.merge(produkter_alle, df_with_text, on=\"Product_Name\", how=\"left\")\n",
      "merged.to_csv(\"data/styrings_data/ai_produkter.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/ai_produkter.xlsx\", index=False)\n",
      "data_sheets_df = pd.read_csv(\"data/data_sheets.csv\")\n",
      "# find all products that have more than one data sheet\n",
      "data_sheets_df = data_sheets_df[data_sheets_df['Data sheets'].str.count(',') > 0]\n",
      "data_sheets_df.to_csv(\"data/data_sheets_multiple.csv\", index=False)\n",
      "data_sheets_df = pd.read_csv(\"data/data_sheets.csv\")\n",
      "# find all products that have more than one data sheet\n",
      "data_sheets_df = data_sheets_df[data_sheets_df['Data sheets'].str.count(',') > 0]\n",
      "data_sheets_df.to_csv(\"data/data_sheets_multiple.csv\", index=False)\n",
      "data_sheets_df.to_excel(\"data/data_sheets_multiple.xlsx\", index=False)\n",
      "# Read the CSV file\n",
      "data_sheets_df = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "# Find all products that have more than one data sheet\n",
      "data_sheets_df = data_sheets_df[data_sheets_df['Data sheets'].str.count(',') > 0]\n",
      "\n",
      "# Split the 'Data sheets' column into multiple columns\n",
      "data_sheets_split = data_sheets_df['Data sheets'].str.split(',', expand=True)\n",
      "\n",
      "# Rename the new columns\n",
      "data_sheets_split.columns = [f'Data sheet {i+1}' for i in range(data_sheets_split.shape[1])]\n",
      "\n",
      "# Concatenate the original DataFrame with the new DataFrame\n",
      "data_sheets_df = pd.concat([data_sheets_df, data_sheets_split], axis=1)\n",
      "\n",
      "# Drop the original 'Data sheets' column\n",
      "data_sheets_df = data_sheets_df.drop(columns=['Data sheets'])\n",
      "\n",
      "# Save the DataFrame to CSV and Excel files\n",
      "data_sheets_df.to_csv(\"data/data_sheets_multiple.csv\", index=False)\n",
      "data_sheets_df.to_excel(\"data/data_sheets_multiple.xlsx\", index=False)\n",
      "merged_df = pd.read_csv(\"data/styrings_data/ai_produkter.csv\")\n",
      "\n",
      "# Find all products that have more no data sheets\n",
      "merged_df_without_datasheet = merged_df[merged_df['Data sheets'].isna()]\n",
      "\n",
      "# Save the DataFrame to CSV and Excel files\n",
      "merged_df_without_datasheet.to_csv(\"data/styrings_data/ai_produkter_uten_datasheet.csv\", index=False)\n",
      "merged_df_without_datasheet.to_excel(\"data/styrings_data/ai_produkter_uten_datasheet.xlsx\", index=False)\n",
      "import requests\n",
      "\n",
      "def scrape_webpage(url):\n",
      "    # Send an HTTP GET request to the URL\n",
      "    response = requests.get(url)\n",
      "\n",
      "    # Check if the request was successful (status code 200)\n",
      "    if response.status_code == 200:\n",
      "        # Get the HTML content of the webpage\n",
      "        html_content = response.text\n",
      "        return html_content\n",
      "    else:\n",
      "        print(f\"Failed to scrape webpage. Status code: {response.status_code}\")\n",
      "        return None\n",
      "data_sheets_df = pd.read_csv(\"data/data_sheets.csv\")\n",
      "\n",
      "# Find all products that have more no data sheets\n",
      "df_without_datasheet = data_sheets_df[data_sheets_df['Data sheets'].isna()]\n",
      "\n",
      "# Save the DataFrame to CSV and Excel files\n",
      "df_without_datasheet.to_csv(\"data/styrings_data/ai_produkter_uten_datasheet.csv\", index=False)\n",
      "df_without_datasheet.to_excel(\"data/styrings_data/ai_produkter_uten_datasheet.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter_uten_datasheet.csv\")\n",
      "# scrape the text from each product page and save it as a new column\n",
      "df_without_datasheet['Scraped Text'] = df_without_datasheet['url'].apply(scrape_webpage)\n",
      "df_without_datasheet.to_csv(\"data/styrings_data/uten_datasheet_scraped.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/styrings_data/uten_datasheet_scraped.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def scrape_webpage(url):\n",
      "    # Send an HTTP GET request to the URL\n",
      "    response = requests.get(url)\n",
      "\n",
      "    # Check if the request was successful (status code 200)\n",
      "    if response.status_code == 200:\n",
      "        # Get the HTML content of the webpage\n",
      "        html_content = response.text\n",
      "\n",
      "        # Parse the HTML content with BeautifulSoup\n",
      "        soup = BeautifulSoup(html_content, 'html.parser')\n",
      "\n",
      "        # Extract the text from the parsed HTML content\n",
      "        text_content = soup.get_text()\n",
      "\n",
      "        return text_content\n",
      "    else:\n",
      "        print(f\"Failed to scrape webpage. Status code: {response.status_code}\")\n",
      "        return None\n",
      "df_without_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter_uten_datasheet.csv\")\n",
      "# scrape the text from each product page and save it as a new column\n",
      "df_without_datasheet['Scraped Text'] = df_without_datasheet['url'].apply(scrape_webpage)\n",
      "df_without_datasheet.to_csv(\"data/styrings_data/uten_datasheet_scraped.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/styrings_data/uten_datasheet_scraped.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/styrings_data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "from bs4 import BeautifulSoup, NavigableString\n",
      "import requests\n",
      "\n",
      "def extract_text(url):\n",
      "    # Get the HTML of the page\n",
      "    response = requests.get(url)\n",
      "    html = response.text\n",
      "\n",
      "    # Parse the HTML with BeautifulSoup\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "    # Find the divs with the specified classes\n",
      "    divs = soup.find_all(class_=\"RichtextArea ProductPage__richtext text-wrapper\")\n",
      "\n",
      "    # Extract the text of each div\n",
      "    text_string = ''\n",
      "    for div in divs:\n",
      "        text_string += div.get_text() + '\\n'\n",
      "\n",
      "    return text_string\n",
      "\n",
      "print(extract_text(\"https://www.kongsberg.com/maritime/products/onshore/onshore-systems/ais-network-infrastructure-shorebased/ais-service-management-system/\"))\n",
      "df_without_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter_uten_datasheet.csv\")\n",
      "# scrape the text from each product page and save it as a new column\n",
      "df_without_datasheet['Scraped Text'] = df_without_datasheet['url'].apply(extract_text)\n",
      "df_without_datasheet.to_csv(\"data/styrings_data/uten_datasheet_scraped.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/styrings_data/uten_datasheet_scraped.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    # if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "system_prompt = \"\"\"\n",
      "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
      "\n",
      "                Write the following three content blocks:\n",
      "\n",
      "                \"General text\":{ (200 - 400 words):\n",
      "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}\n",
      "\n",
      "                \"Key Features\": { (40 - 100 words):\n",
      "                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}\n",
      "\n",
      "                \"Technical Specifications\": {(10 - 150 words):\n",
      "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.}\n",
      "\n",
      "\n",
      "\n",
      "                Return as Json with the same formats as always. Always add General text, Key Features, Techical Specification. If you have no information say I don`t know \"\"\"\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        response_format={ \"type\": \"json_object\" },\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "def generate_text(text):\n",
      "    messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary.\"\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo-1106\",\n",
      "        response_format={ \"type\": \"json_object\" },\n",
      "        messages=messages,\n",
      "    )\n",
      "    output_text = response.choices[0].message.content.strip()\n",
      "    return output_text\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            # print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "            print(data)\n",
      "\n",
      "            block1 = data['general text']\n",
      "            block2 = data['key features']\n",
      "            block3 = data['technical specifications']\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            # print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "            print(data)\n",
      "\n",
      "            block1 = data.get('general text', '')\n",
      "            block2 = data.get('key features', '')\n",
      "            block3 = data.get('technical specifications', '')\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "import pandas as pd\n",
      "import json\n",
      "df_with_text = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "\n",
      "for index, row in df_with_text.iterrows():\n",
      "    if pd.isnull(row[\"General text\"]):\n",
      "        if (row[\"Scraped Text\"]) != \"\":\n",
      "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "            input_text = row[\"Scraped Text\"]\n",
      "            output_text = generate_text(input_text)\n",
      "            # print(output_text)\n",
      "            # Parse the JSON data\n",
      "            data = json.loads(output_text)\n",
      "            data = {k.lower(): v for k, v in data.items()}\n",
      "            print(data)\n",
      "\n",
      "            block1 = data.get('general text', '')\n",
      "            block2 = data.get('key features', '')\n",
      "            block3 = data.get('technical specifications', '')\n",
      "            print(block1)\n",
      "            print(block2)\n",
      "            print(block3)\n",
      "            # Add the blocks to the DataFrame\n",
      "            df_with_text.loc[index, 'General text'] = str(block1)\n",
      "            df_with_text.loc[index, 'Features'] = str(block2)\n",
      "            df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "            df_with_text.to_csv(\"data/data_sheets_web_scraped_ai.csv\", index=False)\n",
      "df_with_text.to_excel(\"data/data_sheets_web_scraped_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "df_without_datasheet = df.read_csv(\"data/styrings_data/ai_produkter.csv\")\n",
      "\n",
      "merged = pd.merge(df_without_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\")\n",
      "df_without_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\")\n",
      "\n",
      "merged = pd.merge(df_without_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"url\",\"Data sheets\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"url\",\"Data sheets\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "merged = pd.merge(df_with_datasheet, df_without_datasheet, on=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"] , how=\"left\")\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "merged = df_with_datasheet.combine_first(df_without_datasheet)\n",
      "\n",
      "# Reset the index\n",
      "merged.reset_index(inplace=True)\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "merged = df_with_datasheet.combine_first(df_without_datasheet)\n",
      "\n",
      "# Reset the index\n",
      "# merged.reset_index(inplace=True)\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_without_datasheet.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "merged = df_with_datasheet.combine_first(df_without_datasheet)\n",
      "\n",
      "# Reset the index\n",
      "# merged.reset_index(inplace=True)\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_without_datasheet = pd.read_csv(\"data/data_sheets_web_scraped_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/ai_produkter.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_without_datasheet.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "merged = df_with_datasheet.combine_first(df_without_datasheet)\n",
      "\n",
      "# Reset the index\n",
      "merged.reset_index(inplace=True)\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai.xlsx\", index=False)\n",
      "df_data_sheet_manual = pd.read_csv(\"data/styrings_data/produkter_alle.xlsx\")\n",
      "# only keep the one with url ends with .pdf\n",
      "df_data_sheet_manual = df_data_sheet_manual[df_data_sheet_manual['url'].str.endswith('.pdf')]\n",
      "print(len(df_data_sheet_manual))\n",
      "df_data_sheet_manual = pd.read_csv(\"data/styrings_data/ai_produkter.csv\")\n",
      "# only keep the one with url ends with .pdf\n",
      "df_data_sheet_manual = df_data_sheet_manual[df_data_sheet_manual['url'].str.endswith('.pdf')]\n",
      "print(len(df_data_sheet_manual))\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "def process_text(df_with_text, output_path):\n",
      "    for index, row in df_with_text.iterrows():\n",
      "            if (row[\"Scraped Text\"]) != \"\":\n",
      "                print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "                input_text = row[\"Scraped Text\"]\n",
      "                output_text = generate_text(input_text)\n",
      "                print(output_text)\n",
      "                # Parse the JSON data\n",
      "                data = json.loads(output_text)\n",
      "                data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "                block1 = data.get('general text', '')\n",
      "                block2 = data.get('key features', '')\n",
      "                block3 = data.get('technical specifications', '')\n",
      "                print(block1)\n",
      "                print(block2)\n",
      "                print(block3)\n",
      "                # Add the blocks to the DataFrame\n",
      "                df_with_text.loc[index, 'General text'] = str(block1)\n",
      "                df_with_text.loc[index, 'Features'] = str(block2)\n",
      "                df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "\n",
      "    return df_with_text\n",
      "df_data_sheet_manual = pd.read_csv(\"data/styrings_data/ai_produkter.csv\")\n",
      "# only keep the one with url ends with .pdf\n",
      "df_data_sheet_manual = df_data_sheet_manual[df_data_sheet_manual['url'].str.endswith('.pdf')]\n",
      "print(len(df_data_sheet_manual))\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "def process_text(df_with_text):\n",
      "    for index, row in df_with_text.iterrows():\n",
      "            if (row[\"Scraped Text\"]) != \"\":\n",
      "                print(index,\"product_name:\", row[\"Product_Name\"])\n",
      "\n",
      "                input_text = row[\"Scraped Text\"]\n",
      "                output_text = generate_text(input_text)\n",
      "                print(output_text)\n",
      "                # Parse the JSON data\n",
      "                data = json.loads(output_text)\n",
      "                data = {k.lower(): v for k, v in data.items()}\n",
      "\n",
      "                block1 = data.get('general text', '')\n",
      "                block2 = data.get('key features', '')\n",
      "                block3 = data.get('technical specifications', '')\n",
      "                print(block1)\n",
      "                print(block2)\n",
      "                print(block3)\n",
      "                # Add the blocks to the DataFrame\n",
      "                df_with_text.loc[index, 'General text'] = str(block1)\n",
      "                df_with_text.loc[index, 'Features'] = str(block2)\n",
      "                df_with_text.loc[index, 'Technical Specifications'] = str(block3)\n",
      "\n",
      "    return df_with_text\n",
      "df_data_sheet_manual_ai = process_text(df_data_sheet_manual)\n",
      "df_data_sheet_manual_ai.to_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", index=False)\n",
      "def scrape_pdf_text(url):\n",
      "    # Download the PDF file\n",
      "    response = requests.get(url)\n",
      "    with open('temp.pdf', 'wb') as f:\n",
      "        f.write(response.content)\n",
      "\n",
      "    # Open the PDF file\n",
      "    with pdfplumber.open('temp.pdf') as pdf:\n",
      "        # Extract text from each page\n",
      "        text = ''\n",
      "        in_features_section = False\n",
      "        if len(pdf.pages) > 10:\n",
      "            return None\n",
      "        for i, page in enumerate(pdf.pages):\n",
      "            # If this is the last page, define a crop box that excludes the last 1 cm from the bottom\n",
      "            if i == len(pdf.pages) - 1:\n",
      "                crop_box = (0, 0, page.width, page.height - 70)\n",
      "                page = page.crop(crop_box)\n",
      "\n",
      "            page_text = page.extract_text().split('\\n')\n",
      "                        \n",
      "            for line in page_text:\n",
      "                if not in_features_section:\n",
      "                    line = line.replace('®', '')  # Remove ® symbol\n",
      "                    if line.startswith('•'):\n",
      "                        text += '\\n' + line  # Add bullet point to new line\n",
      "                    else:\n",
      "                        text += line + '\\n'\n",
      "\n",
      "    # Remove the temporary PDF file\n",
      "    os.remove('temp.pdf')\n",
      "\n",
      "    return text\n",
      "\n",
      "# Example usage\n",
      "data_sheets = pd.read_csv(\"data/data_sheets.csv\")\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].notna()]\n",
      "data_sheets = data_sheets[data_sheets['Data sheets'].str.count(',') < 1]\n",
      "product_datasheet = data_sheets[\"Data sheets\"].iloc[2]\n",
      "pdf_text = scrape_pdf_text(product_datasheet)\n",
      "print(pdf_text)\n",
      "print(product_datasheet)\n",
      "df_data_sheet_manual['Scraped Text'] = df_data_sheet_manual['url'].apply(scrape_pdf_text)\n",
      "df_data_sheet_manual_ai = process_text(df_data_sheet_manual)\n",
      "df_data_sheet_manual_ai.to_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "\n",
      "# Reset the index\n",
      "merged.reset_index(inplace=True)\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "\n",
      "# Reset the index\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "df_with_datasheet.update(df_data_sheet_manual_ai)\n",
      "\n",
      "\n",
      "# Reset the index\n",
      "\n",
      "df_with_datasheet.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "df_with_datasheet.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "df_data_sheet_manual_ai.update(df_with_datasheet)\n",
      "\n",
      "\n",
      "# Reset the index\n",
      "\n",
      "df_with_datasheet.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "df_with_datasheet.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "df_data_sheet_manual_ai.update(df_with_datasheet)\n",
      "\n",
      "\n",
      "# Reset the index\n",
      "\n",
      "df_data_sheet_manual_ai.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "df_data_sheet_manual_ai.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "df_with_datasheet.update(df_data_sheet_manual_ai)\n",
      "\n",
      "\n",
      "# Reset the index\n",
      "\n",
      "df_with_datasheet.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "df_with_datasheet.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "\n",
      "\n",
      "# Reset the index\n",
      "\n",
      "df_with_datasheet.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "df_with_datasheet.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "\n",
      "\n",
      "# Reset the index\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "# df_data_sheet_manual_ai.set_index('Product_Name', inplace=True)\n",
      "# df_with_datasheet.set_index('Product_Name', inplace=True)\n",
      "\n",
      "# merged = pd.merge(df_with_datasheet, df_without_datasheet, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "# Combine the DataFrames\n",
      "# merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "merged = df_with_datasheet.combine_first(df_data_sheet_manual_ai)\n",
      "\n",
      "\n",
      "# Reset the index\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "# Merge the DataFrames\n",
      "merged = pd.merge(df_with_datasheet, df_data_sheet_manual_ai, on=\"Product_Name\", how=\"left\")\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "# Merge the DataFrames\n",
      "merged = pd.merge(df_with_datasheet, df_data_sheet_manual_ai, on=\"Product_Name\", how=\"right\")\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "# Merge the DataFrames\n",
      "merged = pd.merge(df_with_datasheet, df_data_sheet_manual_ai, on=\"Product_Name\", how=\"inner\")\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "df_data_sheet_manual_ai = pd.read_csv(\"data/styrings_data/df_data_sheet_manual_ai.csv\", usecols=[\"Product_Name\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "df_with_datasheet = pd.read_csv(\"data/styrings_data/alle_produkter_ai.csv\", usecols=[\"Product_Name\",\"Data sheets\",\"url\",\"General text\",\"Features\",\"Technical Specifications\"])\n",
      "\n",
      "# Merge the DataFrames\n",
      "merged = pd.concat([df_with_datasheet, df_data_sheet_manual_ai])\n",
      "\n",
      "# Remove duplicates based on 'Product_Name', keeping the last occurrence\n",
      "merged.drop_duplicates(subset='Product_Name', keep='last', inplace=True)\n",
      "\n",
      "merged.to_csv(\"data/styrings_data/alle_produkter_ai_test.csv\", index=False)\n",
      "merged.to_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\", index=False)\n",
      "%history -f scraping.py\n",
      "%history\n"
     ]
    }
   ],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from openai._client import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=st.secrets[\"openai\"][\"api_key\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Add where you found the information for each block.\n",
    "\n",
    "                Write the following three content blocks:\n",
    "\n",
    "                \"General text\":{ (200 - 400 words):\n",
    "                Provide factual information blocks explaining the value that the products bring. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.}\n",
    "\n",
    "                \"Key Features\": { (40 - 100 words):\n",
    "                List the key features or benefits of the product. Include as many as needed. Use bullet points for each feature. Use the same bullet points as the pdf.}\n",
    "\n",
    "                \"Technical Specifications\": {(10 - 150 words):\n",
    "                Add technical specifications related to the product, following the expected structure: \"category, parameters, and parameter value.\" Include as many specifications as needed. Use bullet points for each specification.}\n",
    "\n",
    "\n",
    "\n",
    "                Return as Json with the same formats as always.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_json_blocks = \"\"\"\n",
    "                Read the whole file. Task: You are a content/format writer tasked with converting information from a PDF file into a a selected format. Only use the content found in the provided file. If there's insufficient information, insert \"I don't know\" in the respective block. Use the same sentences, same way of writing in the blocks if that is possible. Do not add any new information, and keep changes to a minimum, you are a formater, more than a content writer. Include a minimum of one and a maximum of three such blocks. If the product pdf offer more text, use three blocks. If the text has headings, use these headings. Have at elast 100 words in each block and add a suitable heading for each block.\n",
    "\n",
    "                Write the following three content blocks with (50 - 200 words) per block:\n",
    "\n",
    "\n",
    "                \"Block 1\":{ \n",
    "                Use the information provided of the product to write a block of text that describes the product. The product often have a suiteable heading, use this heading and the text after. \n",
    "                }\n",
    "\n",
    "                \"Block 2\":{\n",
    "                Use the information provided of the product to write a block of text that describes the product. The product often have a suiteable heading, use this heading and the text after. \n",
    "                }\n",
    "\n",
    "                \"Block 3\":{ \n",
    "                Use the information provided of the product to write a block of text that describes the product. The product often have a suiteable heading, use this heading and the text after. \n",
    "                }\n",
    "\n",
    "                \n",
    "             \n",
    "                Return as Json with the same formats as always.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blocks(text):\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt_json_blocks\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json.\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        # model=\"gpt-3.5-turbo-1106\",\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=messages,\n",
    "    )\n",
    "    output_text = response.choices[0].message.content.strip()\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"I have a text that I want to format into headings and text bulks. The text is:\\n\\n{text}\\n\\n Please format this text using the same words and languange as the text. Do not change the text to much, and do not add information that is not there. return as json with key and value. Do not add any list or dictionary.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=messages,\n",
    "    )\n",
    "    output_text = response.choices[0].message.content.strip()\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_general(prompt, text):\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Help the user to the best of your ability. If you don't know the answer, say 'I don't know'.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}. Use this text {text}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    output_text = response.choices[0].message.content.strip()\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_melted = pd.read_csv(\"data/file_melted.csv\")\n",
    "\n",
    "for index, row in file_melted.iterrows():\n",
    "    if row[\"Scraped Text\"] != None:\n",
    "        # if row[\"datasheet_name\"] is empty\n",
    "        if pd.isnull(row[\"General text\"]):\n",
    "            print(index,\"product_name:\", row[\"Product_Name\"])\n",
    "\n",
    "\n",
    "            input_text = row[\"Scraped Text\"]\n",
    "            MAX_TOKENS = 16385  # Maximum number of tokens\n",
    "\n",
    "\n",
    "            # Truncate the text if it's too long\n",
    "            if len(input_text.split()) > MAX_TOKENS:\n",
    "                input_text = ' '.join(input_text.split()[:MAX_TOKENS])\n",
    "\n",
    "\n",
    "            data_sheet_name = generate_text_general(\"From the text, return a single name that work as a product name and header. Onøy return this name.\",input_text)\n",
    "            file_melted.loc[index, 'datasheet_name'] = data_sheet_name\n",
    "            print(data_sheet_name, \"-----\",row[\"Data sheet\"])\n",
    "            output_text = generate_text(input_text)\n",
    "            print(output_text)\n",
    "            # Parse the JSON data\n",
    "            data = json.loads(output_text)\n",
    "            data = {k.lower(): v for k, v in data.items()}\n",
    "\n",
    "\n",
    "            block1 = data['general text']\n",
    "            block2 = data['key features']\n",
    "            block3 = data['technical specifications']\n",
    "            print(block1)\n",
    "            print(block2)\n",
    "            print(block3)\n",
    "            # Add the blocks to the DataFrame\n",
    "            file_melted.loc[index, 'General text'] = str(block1)\n",
    "            file_melted.loc[index, 'Features'] = str(block2)\n",
    "            file_melted.loc[index, 'Technical Specifications'] = str(block3)\n",
    "file_melted.to_csv(\"data/test.csv\", index=False)\n",
    "file_melted.to_excel(\"data/test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ih[10-40:] # code of the 10 most recently run cells (Even if those cells are deleted now)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE AND MAKE FILEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data/multiple_datasheets_v1.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# List of all the data sheet columns\n",
    "data_sheet_columns = [f'Data sheet {i}' for i in range(1, 17)]\n",
    "\n",
    "df_melted = df.melt(id_vars=['url', 'Product_Name'], value_vars=data_sheet_columns, var_name='DataSheet', value_name='Value')\n",
    "\n",
    "#remove NA\n",
    "df_melted = df_melted[df_melted['Value'].notna()]\n",
    "\n",
    "df_melted.to_excel('file_melted.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_melted = pd.read_excel(\"file_melted.xlsx\")\n",
    "# remove whitespace\n",
    "file_melted['Data sheet'] = file_melted['Data sheet'].str.strip()\n",
    "# remove duplicate data sheets\n",
    "file_melted = file_melted.drop_duplicates(subset=['Data sheet'])\n",
    "\n",
    "\n",
    "file_melted['Scraped Text'] = file_melted['Data sheet'].apply(scrape_pdf_text)\n",
    "file_melted.to_csv(\"data/file_melted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Product_Name', 'Data sheets', 'Features', 'General text',\n",
      "       'Technical Specifications', 'url', 'Product category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "all_products_categories = pd.read_excel(\"data/all_products_category.xlsx\", usecols=[\"Product_Name\", \"Product category\"])\n",
    "\n",
    "all_products_ai = pd.read_excel(\"data/styrings_data/alle_produkter_ai_test.xlsx\")\n",
    "\n",
    "# merge on product name and product category\n",
    "all_products_ai_category = pd.merge(all_products_ai, all_products_categories, on=[\"Product_Name\"], how=\"left\")\n",
    "\n",
    "print(all_products_ai_category.columns)\n",
    "all_products_ai_category.to_excel(\"data/all_products_ai_category.xlsx\", index=False)\n",
    "all_products_ai_category.to_csv(\"data/all_products_ai_category.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products_categories = pd.read_excel(\"data/all_products_category.xlsx\", usecols=[\"Product_Name\", \"Product category\"])\n",
    "multiple_datasheets_categories = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# merge on product name and product category\n",
    "multiple_datasheets_categories = pd.merge(multiple_datasheets_categories, all_products_categories, on=[\"Product_Name\"], how=\"left\")\n",
    "\n",
    "multiple_datasheets_categories.to_excel(\"data/multiple_datasheets_categories.xlsx\")\n",
    "multiple_datasheets_categories.to_csv(\"data/multiple_datasheets_categories.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_datasheets_categories = pd.read_csv(\"data/test.csv\")\n",
    "multiple_datasheets_categories.to_excel(\"data/multiple_datasheets_AI.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_datasheets_categories = pd.read_excel(\"data/multiple_datasheets_categories.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in multiple_datasheets_categories.iterrows():\n",
    "    match = re.search(r'\"(.*?)\"', row[\"datasheet_name\"])\n",
    "    if match:\n",
    "        value = match.group(1)\n",
    "        multiple_datasheets_categories.loc[i, \"datasheet_name\"] = value\n",
    "        print(value,\" from : \", row[\"datasheet_name\"])\n",
    "\n",
    "# remove duplicate data sheets\n",
    "multiple_datasheets_categories = multiple_datasheets_categories.drop_duplicates(subset=['Scraped Text'])\n",
    "multiple_datasheets_categories.to_excel(\"data/multiple_datasheets_categories_newname.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_datasheets_categories = pd.read_excel(\"data/multiple_datasheets_categories_newname.xlsx\")\n",
    "\n",
    "all_products = pd.read_excel(\"data/styrings_data/12-12-all_products.xlsx\")\n",
    "\n",
    "# merge on product name and product category\n",
    "all_products = pd.concat([all_products, multiple_datasheets_categories], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "all_products.to_excel(\"data/all_products.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "0                            AIS physical shore stations\n",
      "2                                    AIS space receivers\n",
      "5              Acoustic control system for BOP operation\n",
      "8                   Automatic Identification System, AIS\n",
      "9                   Autonomous Underwater Vehicle, HUGIN\n",
      "15        DGPS / DGNSS reference and monitoring stations\n",
      "19                     EA440 - shallow and medium waters\n",
      "25            EM 124 Multibeam echosounder, Max. 11000 m\n",
      "30                     EM 304 MKII Multibeam echosounder\n",
      "32             EM 712 Multibeam echosounder, Max. 3600 m\n",
      "33                        Echo sounder sweep system, MCU\n",
      "36                      GNSS compass and position sensor\n",
      "37                   GNSS-based Relative Position Sensor\n",
      "38                           GNSS/DGNSS Position Sensors\n",
      "40                   GPS transponder, Seatrack 220 / 320\n",
      "41                         GPS transponder, Seatrack 330\n",
      "44                            Helideck Monitoring System\n",
      "57                  Laser-based relative position sensor\n",
      "60          MW-based Long-range relative position sensor\n",
      "61                     MW-based Relative position sensor\n",
      "62                        Maritime Broadband Radio - MBR\n",
      "63     MicroPAP (¬µPAP) - Compact acoustic positionin...\n",
      "64                                 Motion Reference Unit\n",
      "79                SIS 5 - Multibeam echosounder software\n",
      "80                                           SIS remote \n",
      "86                         Side scan echo sounder, EA400\n",
      "158               Single beam bottom mapping transducers\n",
      "159        Single beam echo sounder, EA600 - To 11.000 m\n",
      "160             Single beam echo sounder, remote display\n",
      "165                           Sub-bottom profiler, Topas\n",
      "168    TTC 30 & TTC 10 - Transponders test and config...\n",
      "182                             cNODE Maxi - Transponder\n",
      "183                            cNODE Micro - Transponder\n",
      "184                             cNODE Midi - Transponder\n",
      "186                            cNODE MiniS - Transponder\n",
      "Name: Product_Name, dtype: object\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "multiple_datasheets_names = pd.read_excel(\"data/multiple_datasheets_v1.xlsx\", usecols=[\"Product_Name\"])\n",
    "\n",
    "print(len(multiple_datasheets_names.values))\n",
    "\n",
    "all_products_names = pd.read_excel(\"data/all_products.xlsx\")\n",
    "# remove the products that are in multiple datasheets\n",
    "\n",
    "\n",
    "# Store original Product_Name values\n",
    "original_product_names = all_products_names['Product_Name']\n",
    "\n",
    "# Perform the operation\n",
    "all_products_names = all_products_names[~all_products_names['Product_Name'].isin(multiple_datasheets_names['Product_Name'])]\n",
    "\n",
    "# Find and print the removed names\n",
    "removed_names = original_product_names[~original_product_names.isin(all_products_names['Product_Name'])]\n",
    "print(removed_names)\n",
    "print(len(removed_names.values))\n",
    "all_products_names.to_excel(\"data/merged_multiple_data_frames_without_names.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the URLs\n",
    "for url in urls:\n",
    "    # Check if the URL contains the text \"<span class=\"ProductPage__discontinuedBanner\"> No longer sold </span>\"\n",
    "    if '<span class=\"ProductPage__discontinuedBanner\"> No longer sold </span>' in url:\n",
    "        # Remove the URL from the list\n",
    "        urls.remove(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/em-304-multibeam-echosounder-max.-8000-m\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/echosounder-sweep-system/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/\n",
      "https://www.kongsberg.com/maritime/products/commercial-fisheries/fisherysonar/cs90/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/ocean-science/es_scientific/simrad-ek15/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/ocean-science/es_scientific/simrad-ek60/\n",
      "https://www.kongsberg.com/maritime/products/commercial-fisheries/td/simrad-120khz-transducers/simrad-es120-7dd/\n",
      "https://www.kongsberg.com/maritime/products/commercial-fisheries/td/simrad-38khz-transducers/es38_12/\n",
      "https://www.kongsberg.com/maritime/products/commercial-fisheries/td/simrad-38khz-transducers/es38b/\n",
      "https://www.kongsberg.com/maritime/products/commercial-fisheries/fisherysounder/es70/\n",
      "https://www.kongsberg.com/maritime/products/commercial-fisheries/simrad-trawl-sonars/fm90/\n",
      "https://www.kongsberg.com/maritime/products/commercial-fisheries/cm-sensors/pi-configurator/\n",
      "https://www.kongsberg.com/maritime/products/commercial-fisheries/fisherysonar/sh90/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA-600/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/RD-301/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/sub-bottom-profilers2/SBP-120-300/\n",
      "https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/transponders/cnode-transponders-for-hipap-and-uPAP/cNODE-IQAM/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA-600/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA-600/\n",
      "https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA-600/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to check if the URL contains the specified text\n",
    "def check_url(url):\n",
    "    if isinstance(url, str):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        result = soup.find('span', class_='ProductPage__discontinuedBanner')\n",
    "        if result and \"No longer sold\" in result.text.strip():\n",
    "            print(url)\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# Apply the function to the 'url' column and keep only the rows where the function returns True\n",
    "all_products = pd.read_excel(\"data/all_products.xlsx\")\n",
    "all_products = all_products[all_products['url'].apply(check_url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove this https://www.kongsberg.com/maritime/products/Acoustics-Positioning-and-Communication/transponders/cnode-transponders-for-hipap-and-uPAP/cNODE-IQAM/  :  cNODE IQAM - Intelligent data analysis and monitoring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA-600/  :  EA 400/600 Sidescan\n",
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA-600/  :  EA 600 Series\n",
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/multibeam-echo-sounders/em-304-multibeam-echosounder-max.-8000-m  :  EM 304 Multibeam echosounder, Max. 8000 m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA-600/  :  Remote display RD301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/  :  Side looking transducer 200 kHz\n",
      "remove this https://www.kongsberg.com/maritime/products/commercial-fisheries/fisherysonar/cs90/  :  Simrad CS90\n",
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/ocean-science/es_scientific/simrad-ek15/  :  Simrad EK15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove this https://www.kongsberg.com/maritime/products/commercial-fisheries/td/simrad-120khz-transducers/simrad-es120-7dd/  :  Simrad ES120-7DD\n",
      "remove this https://www.kongsberg.com/maritime/products/commercial-fisheries/td/simrad-38khz-transducers/es38_12/  :  Simrad ES38-12\n",
      "remove this https://www.kongsberg.com/maritime/products/commercial-fisheries/td/simrad-38khz-transducers/es38b/  :  Simrad ES38B\n",
      "remove this https://www.kongsberg.com/maritime/products/commercial-fisheries/fisherysounder/es70/  :  Simrad ES70\n",
      "remove this https://www.kongsberg.com/maritime/products/commercial-fisheries/simrad-trawl-sonars/fm90/  :  Simrad FM90\n",
      "remove this https://www.kongsberg.com/maritime/products/commercial-fisheries/cm-sensors/pi-configurator/  :  Simrad PI Configurator\n",
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/  :  EA400SP\n",
      "remove this https://www.kongsberg.com/maritime/products/commercial-fisheries/fisherysonar/sh90/  :  Simrad SH90\n",
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/  :  EA400\n",
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/es_bottommapping/EA400-600/  :  EA400 survey\n",
      "remove this https://www.kongsberg.com/maritime/products/ocean-science/mapping-systems/sub-bottom-profilers2/SBP-120-300/  :  Sub-bottom profiler, SBP 120/300\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to check if the URL contains the specified text\n",
    "def check_url(url):\n",
    "    if isinstance(url, str):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        result = soup.find('span', class_='ProductPage__discontinuedBanner')\n",
    "        if result and \"No longer sold\" in result.text.strip():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Read the DataFrame\n",
    "all_products = pd.read_excel(\"data/all_products_manually_done.xlsx\")\n",
    "\n",
    "# Create a new DataFrame to store the rows to keep\n",
    "products_to_keep = []\n",
    "# Define a list to store the indices of the rows to remove\n",
    "indices_to_remove = []\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in all_products.iterrows():\n",
    "    # If the URL does not contain the specified text\n",
    "    if check_url(row['url']):\n",
    "        # add one count for evry produt that is true\n",
    "        products_to_keep.append(str(row['url'])+\" :\"+str(row['Product_Name']))\n",
    "    else:\n",
    "        print(\"remove this\",row['url'], \" : \", row['Product_Name'])\n",
    "        indices_to_remove.append(index)\n",
    "\n",
    "        # Iterate over the rows of the DataFrame\n",
    "        for index2, row2 in all_products.iterrows():\n",
    "            if row[\"Product_Name\"] == row2[\"product_family_name\"]:\n",
    "                print(\"family grouped also removed: \",row2[\"url\"], \" : \", row2[\"Product_Name\"])\n",
    "                indices_to_remove.append(index2)\n",
    "\n",
    "# Remove the rows with the collected indices\n",
    "all_products = all_products.drop(indices_to_remove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products.to_excel(\"data/all_products_dropped_discontinuid.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product category</th>\n",
       "      <th>General text</th>\n",
       "      <th>Features</th>\n",
       "      <th>Technical Specifications</th>\n",
       "      <th>url</th>\n",
       "      <th>Data sheets</th>\n",
       "      <th>downloads</th>\n",
       "      <th>product_family_name</th>\n",
       "      <th>is_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seapath 380 - utilising GPS, GLONASS, Galileo...</td>\n",
       "      <td>Seafloor mapping</td>\n",
       "      <td>The Seapath 380 series uses a state-of-the-art...</td>\n",
       "      <td>• 0.007° to 0.02° roll and pitch accuracy depe...</td>\n",
       "      <td>PERFORMANCE POWER SPECIFICATIONS\\nHeave accura...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.kongsberg.com/contentassets/c22a59...</td>\n",
       "      <td>{'Data sheet': ['https://www.kongsberg.com/con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 kHz dual-beam transducer</td>\n",
       "      <td>Seafloor mapping, Geophysical survey</td>\n",
       "      <td>The Simrad 12-16/60 is a 12 kHz dual beam tran...</td>\n",
       "      <td>Resonant frequency: 12 kHz\\nDual beam transduc...</td>\n",
       "      <td>Resonant frequency: 12 kHz\\nMaximum transducer...</td>\n",
       "      <td>https://www.kongsberg.com/maritime/products/oc...</td>\n",
       "      <td>https://www.kongsberg.com/contentassets/d106c9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single beam bottom mapping transducers</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120 kHz single-beam transducer</td>\n",
       "      <td>Seafloor mapping, Geophysical survey</td>\n",
       "      <td>Introduction The Simrad 120-25 is a single-bea...</td>\n",
       "      <td>Resilient Design: Housed in a robust casing, t...</td>\n",
       "      <td>Resonant frequency: 120 kHz Maximum input powe...</td>\n",
       "      <td>https://www.kongsberg.com/maritime/products/oc...</td>\n",
       "      <td>https://www.kongsberg.com/contentassets/d106c9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single beam bottom mapping transducers</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18 kHz single-beam transducer</td>\n",
       "      <td>Seafloor mapping, Geophysical survey</td>\n",
       "      <td>The Simrad 18-11 is a large, single-beam trans...</td>\n",
       "      <td>Key Features:\\n- Resonant frequency: 18 kHz\\n-...</td>\n",
       "      <td>Technical Specifications:\\n- Resonant frequenc...</td>\n",
       "      <td>https://www.kongsberg.com/maritime/products/oc...</td>\n",
       "      <td>https://www.kongsberg.com/contentassets/d106c9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single beam bottom mapping transducers</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product_Name  \\\n",
       "0   Seapath 380 - utilising GPS, GLONASS, Galileo...   \n",
       "1                        12 kHz dual-beam transducer   \n",
       "2                     120 kHz single-beam transducer   \n",
       "3                      18 kHz single-beam transducer   \n",
       "\n",
       "                       Product category  \\\n",
       "0                      Seafloor mapping   \n",
       "1  Seafloor mapping, Geophysical survey   \n",
       "2  Seafloor mapping, Geophysical survey   \n",
       "3  Seafloor mapping, Geophysical survey   \n",
       "\n",
       "                                        General text  \\\n",
       "0  The Seapath 380 series uses a state-of-the-art...   \n",
       "1  The Simrad 12-16/60 is a 12 kHz dual beam tran...   \n",
       "2  Introduction The Simrad 120-25 is a single-bea...   \n",
       "3  The Simrad 18-11 is a large, single-beam trans...   \n",
       "\n",
       "                                            Features  \\\n",
       "0  • 0.007° to 0.02° roll and pitch accuracy depe...   \n",
       "1  Resonant frequency: 12 kHz\\nDual beam transduc...   \n",
       "2  Resilient Design: Housed in a robust casing, t...   \n",
       "3  Key Features:\\n- Resonant frequency: 18 kHz\\n-...   \n",
       "\n",
       "                            Technical Specifications  \\\n",
       "0  PERFORMANCE POWER SPECIFICATIONS\\nHeave accura...   \n",
       "1  Resonant frequency: 12 kHz\\nMaximum transducer...   \n",
       "2  Resonant frequency: 120 kHz Maximum input powe...   \n",
       "3  Technical Specifications:\\n- Resonant frequenc...   \n",
       "\n",
       "                                                 url  \\\n",
       "0                                                NaN   \n",
       "1  https://www.kongsberg.com/maritime/products/oc...   \n",
       "2  https://www.kongsberg.com/maritime/products/oc...   \n",
       "3  https://www.kongsberg.com/maritime/products/oc...   \n",
       "\n",
       "                                         Data sheets  \\\n",
       "0  https://www.kongsberg.com/contentassets/c22a59...   \n",
       "1  https://www.kongsberg.com/contentassets/d106c9...   \n",
       "2  https://www.kongsberg.com/contentassets/d106c9...   \n",
       "3  https://www.kongsberg.com/contentassets/d106c9...   \n",
       "\n",
       "                                           downloads  \\\n",
       "0  {'Data sheet': ['https://www.kongsberg.com/con...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "\n",
       "                      product_family_name  is_range  \n",
       "0                                     NaN       NaN  \n",
       "1  Single beam bottom mapping transducers       1.0  \n",
       "2  Single beam bottom mapping transducers       1.0  \n",
       "3  Single beam bottom mapping transducers       1.0  "
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tonality_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
